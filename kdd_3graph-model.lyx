#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
A graphical model for travel times
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kdd_graphmodel"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-kdd/gaussians.pdf
	width 9cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Picture-travel-times"

\end_inset


\begin_inset Argument
status open

\begin_layout Plain Layout
Picture representations about the travel times over a road link
\end_layout

\end_inset

Picture representations about the travel times over a road link: the travel
 time is a mixture of (typically) three Gaussian distributions: one mode
 for the travel times corresponding to a green light, one mode for the travel
 times corresponding to a red light, and one mode for complex random events
 (pedestrian, traffic occlusion, garbage truck, bus, etc.)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We develop a travel time model which exploits the compressed data returned
 by the Stop-Go model (number of stops and travel time experienced per link)
 and that takes into account our simplifying assumptions about travel times
 on a road network (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Picture-travel-times"

\end_inset

).
 The model captures the state transition probabilities: the probability
 of the number of stops on a link given the number of stops on the previous
 link(s) of the trajectory.
 It also models the correlations of travel times for neighboring links given
 their state (number of stops).
 The travel time model is a combination of two models parametrized by a
 set of values denoted by 
\begin_inset Formula $\theta$
\end_inset

:
\begin_inset Newline newline
\end_inset

 
\begin_inset Formula $\bullet$
\end_inset

 A directed Markov model of discrete state random variables gives the joint
 probability distribution of the link states 
\begin_inset Formula 
\[
\pi_{\theta}\left(\left\{ S_{l}\right\} _{l\in\mathcal{L}}\right)
\]

\end_inset

 
\begin_inset Formula $\bullet$
\end_inset

 A Gaussian Markov random field gives the joint distribution of the travel
 times 
\begin_inset Formula 
\[
\pi_{\theta}\left(\left\{ X_{l,s}\right\} _{l\in\mathcal{L},s\in\left\{ 1,\cdots m\right\} }\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-kdd/ttgraph_example.pdf
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:ttgraph_example"

\end_inset


\begin_inset Argument
status open

\begin_layout Plain Layout
Graphical model of the travel times on a stretch of three one-way road links
\end_layout

\end_inset

Graphical model of the travel times on a stretch of three one-way road links
 (going from top to bottom of the figure).
 The travel times of each link 
\begin_inset Formula $Z_{l}$
\end_inset

 depend on two conditionally independent set of variables: the discrete
 state variable (e.g.
 number of stops) 
\begin_inset Formula $S_{l}$
\end_inset

 and the conditional travel times 
\begin_inset Formula $X_{l,s}$
\end_inset

 where 
\begin_inset Formula $s\in\left\{ \text{Go, Red, Other}\right\} $
\end_inset

.
 The arrows between the variables 
\begin_inset Formula $S_{l}$
\end_inset

 represent the conditional dependencies between the random variables, after
 taking into account the Markov assumptions.
 The time variables 
\begin_inset Formula $X_{l,s}$
\end_inset

 make a Markov Random Field.
 The solid lines between the variables represent the conditional dependencies.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename docs-kdd/network_example.pdf
	width 10cm

\end_inset


\end_layout

\end_inset


\begin_inset Graphics
	filename docs-kdd/network_example.pdf
	width 10cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:ttgraph_sf"

\end_inset


\begin_inset Argument
status open

\begin_layout Plain Layout
Example of travel time graph for the San Francisco area
\end_layout

\end_inset

Example of travel time graph for the San Francisco area.
 The red box on the left has been expanded in the undirected Gaussian Markov
 Random Field (top right) and (directed) Discrete Markov Model (bottom right).
 In this example the number of discrete states has been fixed to 
\begin_inset Formula $m=3$
\end_inset

 for all nodes.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ttgraph_example"

\end_inset

 presents the graphical model that encodes the dependencies between the
 link travel time 
\begin_inset Formula $Z_{l}$
\end_inset

, the link states 
\begin_inset Formula $S_{l}$
\end_inset

 and the conditional travel times 
\begin_inset Formula $X_{l,s}$
\end_inset

.
 The total travel time experienced by a vehicle on link 
\begin_inset Formula $l$
\end_inset

 is 
\begin_inset Formula 
\[
Z_{l}=\sum_{s=0}^{m-1}X_{l,s}\mathbf{1}\left\{ S_{l}=s\right\} 
\]

\end_inset

 The left portion of the figure shows a subset of the GMRF of travel time
 variables, and the right portion shows a subset of the Markov chain of
 states.
\end_layout

\begin_layout Standard
The graphical model shows that conditioning on the states experienced along
 a path allows one to compute the path travel time by summing over the correspon
ding variables in the GMRF.
 Further, when one simultaneously conditions on the link travel times 
\begin_inset Formula $Z_{l}$
\end_inset

 and the link states 
\begin_inset Formula $S_{l}$
\end_inset

, then the two models become independent, which allows one to learn the
 models parameters separately.
 The rest of this section details the modeling and learning of the two models.
\end_layout

\begin_layout Subsection
Markov model for state transitions
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sub:kdd_markov_model"

\end_inset


\end_layout

\begin_layout Standard
We consider the link state variables 
\begin_inset Formula $\{S_{l}\}_{l,\in\mathcal{L}}$
\end_inset

.
 Each variable 
\begin_inset Formula $S_{l}$
\end_inset

 has support 
\begin_inset Formula $\left\{ 1\cdots m\right\} $
\end_inset

.
 In our experiment, vehicles travel along paths in the road network and
 cannot arbitrarily jump from one road to another unconnectecd road.
 We formalize this notion of valid path
\end_layout

\begin_layout Standard

\series bold
Valid path.
 
\series default
Given a directed graph 
\emph on

\begin_inset Formula $\mathcal{G}$
\end_inset

,
\emph default
 valid path is an ordered sequence 
\begin_inset Formula $p=\left(l_{1}\cdots l_{B}\right)\in\mathcal{L}^{B}$
\end_inset

 of distinct elements so that for all consecutive pairs 
\begin_inset Formula $\left(l,l^{'}\right)$
\end_inset

 from 
\begin_inset Formula $p$
\end_inset

, we have an edge 
\begin_inset Formula $l\rightarrow l^{'}$
\end_inset

 in the graph 
\begin_inset Formula $\mathcal{G}$
\end_inset

.
\end_layout

\begin_layout Standard
Consider a valid path 
\begin_inset Formula $p=\left(l_{1}\cdots l_{B}\right)\in\mathcal{L}^{B}$
\end_inset

 in the road network.
 For simplicity, all the 
\begin_inset Formula $l_{i}$
\end_inset

 are assumed to be distinct (handling this case is not an issue in practice).
 Given the path 
\begin_inset Formula $p$
\end_inset

 of a vehicle, the variables 
\begin_inset Formula $\left\{ s_{l_{i}}\right\} _{i\in1\cdots M}$
\end_inset

 have a Markov property: given the state 
\begin_inset Formula $s_{l_{i-1}}$
\end_inset

 of the 
\emph on
upstream link
\emph default
 
\begin_inset Formula $l_{i-1}$
\end_inset

, the conditional state 
\begin_inset Formula $S_{l_{i}}|S_{l_{i-1}}$
\end_inset

 is independent of the state of other upstream links:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbb{P}\left(S_{l_{i}}|S_{l_{i-1}},\dots,S_{l_{0}}\right)=\mathbb{P}\left(S_{l_{i}}|S_{l_{i-1}}\right)\label{eq:state_markov_prop}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This Markov model consists in discrete variables, with values in 
\begin_inset Formula $1\cdots m$
\end_inset

.
 For each link 
\begin_inset Formula $l$
\end_inset

, we parametrize the model using an initial probability vector 
\begin_inset Formula 
\begin{equation}
\pi_{l}\left(s\right)=\mathbb{P}(S_{l}=s)\label{eq:init_proba_vector}
\end{equation}

\end_inset

and a transition probability matrix: 
\begin_inset Formula 
\begin{equation}
\pi_{u\rightarrow l}\left(s_{u},s_{l}\right)=\mathbb{P}(S_{l}=s_{l}|S_{u}=s_{u})\label{eq:transition_matrix}
\end{equation}

\end_inset

here 
\begin_inset Formula $\pi_{u\rightarrow l}\left(s_{u},s_{l}\right)$
\end_inset

 is the probability that 
\begin_inset Formula $l$
\end_inset

 is in state 
\begin_inset Formula $s_{l}$
\end_inset

 given that the upstream link 
\begin_inset Formula $u$
\end_inset

 is in state 
\begin_inset Formula $s_{u}$
\end_inset

.
\end_layout

\begin_layout Standard
We learn the initial probability vector 
\begin_inset Formula $\pi_{l}$
\end_inset

 and the transition probability matrices 
\begin_inset Formula $\pi_{u\rightarrow l}$
\end_inset

 of the Markov Chain by maximizing the likelihood of observing 
\begin_inset Formula $(s^{(n)},y^{(n)})$
\end_inset

.
 The log-likelihood of all 
\begin_inset Formula $A$
\end_inset

 observations of states is given by 
\begin_inset Formula 
\[
ll=\sum_{a=1}^{A}\sum_{n=1}^{M^{\left(a\right)}}\left(\log\left(\pi_{l_{0}^{(n)}}\left(s_{0}^{(n)}\right)\right)+\sum_{u\rightarrow l}\log\left(\pi_{u\rightarrow l}\left(s_{u}^{(n)},s_{l}^{(n)}\right)\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The parameters that maximize the log-likelihood are:
\begin_inset Formula 
\begin{eqnarray*}
\pi_{u\rightarrow l}\left(s_{u},s_{l}\right) & \propto & \bar{T}_{s_{u},s_{l}}^{u\rightarrow l}\\
\pi_{l}\left(s\right) & \propto & \bar{\pi}_{s}^{l}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\bar{\pi}_{s}^{l}$
\end_inset

 are the empirical counts of initial states and 
\begin_inset Formula $\bar{T}_{s_{u},s_{l}}^{u\rightarrow l}$
\end_inset

 are the empirical transition counts.
 This solution corresponds to transitions and initial probabilities that
 are consistent with the empirical counts of initial states and transitions.
\end_layout

\begin_layout Standard

\series bold
Modelling note:
\series default
 While this Markov model is directed, it is 
\emph on
not
\emph default
 a Bayesian Graphical Model because it may contain cycles between the variables.
 However, it is 
\emph on
marginally
\emph default
 a graphical model with Markov properties: for any valid path 
\begin_inset Formula $p$
\end_inset

 in the network with no self-loop, the chain of variables 
\begin_inset Formula $S_{l_{1}}\cdots S_{l_{M}}$
\end_inset

 encodes a valid discrete Markov model distribution over these variables.
 The directed graph structure is an efficient way to compress all the informatio
n about the marginal distributions of the valid subsets of variables.
\end_layout

\begin_layout Subsection
GMRF model for travel time distributions
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kdd-gmrf-model"

\end_inset

 
\end_layout

\begin_layout Standard
We now present the model that describes the correlations between the travel
 time variables.
 We assume that the random variables 
\begin_inset Formula $\left(X_{l,s}\right)_{l\in\mathcal{L},s\in\left[1,m\right]}$
\end_inset

 are Gaussian
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
While Gaussian travel times can theoretically predict negative travel time,
 in practice, these probabilities are virtually zero, as validated in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:kdd_evaluation"

\end_inset

.
\end_layout

\end_inset

, and they can be stacked into one multivariate Gaussian variable 
\begin_inset Formula $X\sim\mathcal{N}\left(\mu,\Sigma\right)$
\end_inset

 of size 
\begin_inset Formula $r=m\left|\mathcal{L}\right|$
\end_inset

, with mean 
\begin_inset Formula $\mu$
\end_inset

 and covariance 
\begin_inset Formula $\Sigma$
\end_inset

.
 Recall that 
\begin_inset Formula $X_{l,s}$
\end_inset

 is the travel time on link 
\begin_inset Formula $l$
\end_inset

 conditioned on link state 
\begin_inset Formula $s$
\end_inset

, where 
\begin_inset Formula $s\in\left[1,m\right]$
\end_inset

.
 This travel time is a Gaussian random variable with mean 
\begin_inset Formula $\mu_{l,s}$
\end_inset

 and variance 
\begin_inset Formula $\sigma_{l,s}$
\end_inset

.
\end_layout

\begin_layout Standard
From the factorization property given by the Hammersley-Clifford Theorem
 (CITE), it is well known that the 
\emph on
precision matrix
\emph default
 
\begin_inset Formula $Q=\Sigma^{-1}$
\end_inset

 encodes the 
\emph on
conditional dependencies
\emph default
 between the variables.
 Since a link is assumed to be conditionally correlated only with its (geographi
cal) neighbors, the precision matrix is very sparse.
 Furthermore, this sparsity pattern is of particular interest: its pattern
 is that of a graph which is nearly planar.
 We take advantage of this structure to devise efficient algorithms that
 (1) estimate the precision matrix given some observations, and (2), infer
 the covariance between any pair of variables.
\end_layout

\begin_layout Standard
As mentioned earlier, we have a set of 
\begin_inset Formula $A$
\end_inset

 trajectories obtained from GPS data.
 After map-matching and trajectory reconstruction (Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:kdd_stopandgo"

\end_inset

), the set of observed trajectories 
\begin_inset Formula $\left\{ W_{a}\right\} _{a=1,\cdots,A}$
\end_inset

 are sequences of observed states and variables (travel time)
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
One could consider that the states and the variables are not directly observed,
 but guessed after a complicated process.
 The Expectation-Maximization algorithm could be run to estimate the value
 of the state and of the variable.
\end_layout

\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
W_{a} & = & \left(l_{1},s_{1}\right)\,\left(l_{2},s_{2}\right)\,\,\cdots\left(l_{M_{a}},s_{M_{a}}\right)\\
x^{a} & = & \left(x_{l,s}^{a}\right)_{(l,s)\in W_{a}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In our notations, 
\begin_inset Formula $x^{a}$
\end_inset

 is an observation of the random vector 
\begin_inset Formula $X_{a}=\left(X_{i}\right)_{i\in W_{a}}$
\end_inset

, which is a 
\begin_inset Formula $M_{a}$
\end_inset

-dimensional marginal (or subset) of the full distribution 
\begin_inset Formula $X$
\end_inset

.
 Hence 
\begin_inset Formula $X_{a}$
\end_inset

 is also a multivariate Gaussian with mean 
\begin_inset Formula $\mu_{(W_{a})}$
\end_inset

 and covariance 
\begin_inset Formula $\Sigma_{(W_{a})}$
\end_inset

 obtained by dropping the irrelevant variables (the variables that one wants
 to marginalize out) from the mean vector 
\begin_inset Formula $\mu$
\end_inset

 and the covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

.
 Its likelihood is thus the likelihood under the marginal distribution:
 
\begin_inset Formula 
\begin{equation}
\begin{array}{l}
\text{log }\pi\left(x^{a};\mu_{(W_{a})},\Sigma_{(W_{a})}\right)=\\
C-\frac{1}{2}\log\left|\Sigma_{(W_{a})}\right|-\frac{1}{2}\left(x^{a}-\mu_{(W_{a})}\right)^{T}\Sigma_{(W_{a})}^{-1}\left(x^{a}-\mu_{(W_{a})}\right),
\end{array}
\end{equation}

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 is a constant which does not depend on the parameters of the model.
\end_layout

\begin_layout Standard
The problem of estimating the parameters of the model 
\begin_inset Formula $\theta=(\mu,\Sigma)$
\end_inset

 from the i.i.d.
 set of observations 
\begin_inset Formula $\mathcal{D}=\left\{ W_{a}\right\} _{a=1,\cdots,A}$
\end_inset

 consists in finding the set of parameters 
\begin_inset Formula $\theta^{\star}=\left\{ \mu^{\star},\Sigma^{\star}\right\} $
\end_inset

 that maximize the sum of the likelihoods of each of the observations
\begin_inset Foot
status open

\begin_layout Plain Layout
The independent and identically distributed (i.i.d.) assumption is defined
 on the pair of marginal observation and masking subset.
\end_layout

\end_inset

: 
\begin_inset Formula 
\begin{equation}
l\left(\theta|\mathcal{D}\right)=\sum_{a=1}^{A}\text{log }\pi\left(x^{a};\mu_{(W_{a})},\Sigma_{(W_{a})}\right)\label{eq:gmrf-II}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Unfortunately, the problem of maximizing
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:gmrf-II"

\end_inset

 is in general not convex and may have multiple local minima since we have
 only partially observed variables 
\begin_inset Formula $x^{a}$
\end_inset

.
 A popular strategy in this case is to 
\emph on
complete
\emph default
 the vector (by computing the most likely completion given the observed
 variables).
 This algorithm is called the 
\shape italic
Expectation-Maximization
\shape default
 (EM) procedure.
 In our case, the EM procedure is not a good fit for two reasons:
\end_layout

\begin_layout Itemize
Since we observe only a small fraction of the values of each vector, the
 vast majority of the values we would use for learning would be sampled
 values, which would make the convergence rate dramatically slow.
 
\end_layout

\begin_layout Itemize
The data completion step would create a complete 
\begin_inset Formula $n-$
\end_inset

size sample for each of our observation, thus our complexity for the data
 completion step would be 
\begin_inset Formula $\mathcal{O}\left(Nn\right)$
\end_inset

, which is too large to be practical.
\end_layout

\begin_layout Standard
Instead, we solve a related problem by computing sufficient statistics from
 all the observations.
 Consider the simpler scenario in which all data has been observed, and
 denote the empirical covariance matrix by 
\begin_inset Formula $\tilde{\Sigma}$
\end_inset

.
 The maximum likelihood problem to find the most likely precision matrix
 is then equivalent to: 
\begin_inset Formula 
\begin{equation}
\underset{S}{\mathbf{minimize}}\,\,-\log\left|Q\right|+\text{Tr}\left(Q\tilde{\Sigma}\right)\label{eq:gmrf-maxll-emp}
\end{equation}

\end_inset

under the structured sparsity constraints 
\begin_inset Formula $Q_{uv}=0\,\,\forall\,\left(u,v\right)\notin\mathcal{E}$
\end_inset

.
 The objective is not defined when 
\begin_inset Formula $Q$
\end_inset

 is not positive definite, so the constraint that 
\begin_inset Formula $S$
\end_inset

 is positive definite is implicit.
 A key point to notice is that the objective only depends on a restricted
 subset of terms of the covariance matrix: 
\begin_inset Formula 
\[
\text{Tr}\left(Q\tilde{\Sigma}\right)=\sum_{\left(u,v\right)\in\mathcal{E}}Q_{uv}\tilde{\Sigma}_{uv}
\]

\end_inset


\end_layout

\begin_layout Standard
This observation motivates the following approach: instead of considering
 the individual likelihoods of each observation individually, we consider
 the covariance that would be produced if all the observations were aggregated
 into a single covariance matrix.
 This approach discards some information, for example the fact that some
 variables are more often seen than others.
 However, it lets us solve the full covariance Problem
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:gmrf-maxll-emp"

\end_inset

 using partial observations.
 Indeed, all we need to do is estimate the values of the coefficients 
\begin_inset Formula $\tilde{\Sigma}_{uv}$
\end_inset

 for 
\begin_inset Formula $\left(u,v\right)$
\end_inset

 a non-zero in the precision matrix.
 We present one way to estimate these coefficients.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $N_{i}$
\end_inset

 be the number of observations of the variable 
\begin_inset Formula $X_{i}$
\end_inset

: 
\begin_inset Formula $N_{i}=\text{card}\left(p:i\in W_{a}\right)$
\end_inset

.
 Combining all the observations that come across 
\begin_inset Formula $X_{i}$
\end_inset

, we can approximate the mean of any function 
\begin_inset Formula $f\left(X_{i}\right)$
\end_inset

 by some empirical mean, using the 
\begin_inset Formula $N_{i}$
\end_inset

 samples: 
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{i}\left[f\left(X_{i}\right)\right]=\frac{1}{N_{i}}\sum_{p:i\in W_{a}}f\left(x_{i}^{p}\right)\label{eq:emp-expectation-uni}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Similarly, defining the number of observations of both 
\begin_inset Formula $X_{i}$
\end_inset

 and 
\begin_inset Formula $X_{j}$
\end_inset

: 
\begin_inset Formula $N_{i\cap j}=\text{card}\left(\left\{ p\,:\, i\in W_{a}\text{ and }j\in W_{a}\right\} \right)$
\end_inset

, we can approximate the mean of any function 
\begin_inset Formula $f\left(X_{i},X_{j}\right)$
\end_inset

 of 
\begin_inset Formula $X_{i},X_{j}$
\end_inset

, using the set of observations that span both variables 
\begin_inset Formula $X_{i}$
\end_inset

 and 
\begin_inset Formula $X_{j}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbb{E}_{i\cap j}\left[f\left(X_{i},X_{j}\right)\right]=\frac{1}{N_{i\cap j}}\sum_{p\,:\, i,j\in W_{a}}f\left(x_{i}^{p},x_{j}^{p}\right)\label{eq:emp-expectation-bi}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Using this notation, the empirical mean is 
\begin_inset Formula $\hat{\mu}_{i}=\mathbb{E}_{i}\left[X_{i}\right]$
\end_inset

.
 Call 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 the 
\emph on
partial empirical covariance matrix
\emph default
 (PECM): 
\begin_inset Formula 
\[
\hat{\Sigma}_{ij}=\begin{cases}
\mathbb{E}_{i\cap j}\left[X_{i}X_{j}\right]-\mathbb{E}_{i}\left[X_{i}\right]\mathbb{E}_{j}\left[X_{j}\right] & \text{if }\left(i,j\right)\in\mathcal{E}\\
0 & \text{otherwise}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Using this PECM as a proxy for the real covariance matrix, one can then
 estimate the most likely GMRF by solving the following problem: 
\begin_inset Formula 
\begin{equation}
\underset{S}{\mathbf{minimize}}\,\,-\log\left|Q\right|+\left\langle Q,\hat{\Sigma}\right\rangle \label{eq:gmrf-maxll-gecm}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that this definition is asymptotically consistent: in the limit, when
 an infinite number of observations are gathered (
\begin_inset Formula $N_{ij}\rightarrow\infty$
\end_inset

), the PECM will converge towards the true covariance: indeed 
\begin_inset Formula $\hat{\Sigma}_{ij}\rightarrow\mathbb{E}\left[X_{i}X_{j}\right]-\mathbb{E}\left[X_{i}\right]\mathbb{E}\left[X_{j}\right]$
\end_inset

 and for all 
\begin_inset Formula $Q$
\end_inset

: 
\begin_inset Formula 
\[
\left\langle Q,\hat{\Sigma}\right\rangle \rightarrow\left\langle Q,\mathbb{E}\left[XX^{T}\right]-\mathbb{E}\left[X\right]\mathbb{E}\left[X\right]^{T}\right\rangle 
\]

\end_inset

.
\end_layout

\begin_layout Standard
Unfortunately, the problem is not convex because 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 is not necessarily positive semi-definite (although it is if the number
 of samples is large enough), since the variables are only partially observed.
 For instance, if we have a partially observed bivariate Gaussian variable
 
\begin_inset Formula $X$
\end_inset

: 
\begin_inset Formula $\left(10,10\right)$
\end_inset

, 
\begin_inset Formula $\left(-10,-10\right)$
\end_inset

, 
\begin_inset Formula $\left(1,\star\right)$
\end_inset

, 
\begin_inset Formula $\left(-1,\star\right)$
\end_inset

, 
\begin_inset Formula $\left(\star,1\right)$
\end_inset

, 
\begin_inset Formula $\left(\star,-1\right)$
\end_inset

, the empirical covariance matrix 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 has diagonal entries 
\begin_inset Formula $(51,51)$
\end_inset

 and off-diagonal entries 
\begin_inset Formula $(100,100)$
\end_inset

.
 Its eigenvalues are 
\begin_inset Formula $-49,151$
\end_inset

 hence it is not definite positive.
\end_layout

\begin_layout Standard
There is a number of ways to correct this.
 The simplest we found is to scale all the coefficients so that they have
 the same variance: 
\begin_inset Formula 
\[
\hat{\Sigma}_{ij}=\begin{cases}
\sqrt{\alpha_{ij}}\mathbb{E}_{i\cap j}\left[X_{i}X_{j}\right]-\frac{1}{\sqrt{\alpha_{ij}}}\mathbb{E}_{i}\left[X_{i}\right]\mathbb{E}_{j}\left[X_{j}\right] & \text{if }N_{ij}>0\\
0 & \text{otherwise}
\end{cases}
\]

\end_inset


\begin_inset Formula 
\[
\alpha_{ij}=\sqrt{\frac{\mathbb{E}_{i}\left[X_{i}\right]\mathbb{E}_{j}\left[X_{j}\right]}{\mathbb{E}_{i\cap j}\left[X_{i}\right]\mathbb{E}_{i\cap j}\left[X_{j}\right]}}
\]

\end_inset


\end_layout

\begin_layout Standard
This transformation has the advantage of being local and easy to compute.
 This is why it is completed by an increase of the diagonal coefficients
 by some factor of the identity matrix.
\end_layout

\begin_layout Standard
Another problem is due to the relative imbalance between the distributions
 of samples: cars travel much more on some roads than others.
 This means that some edges may be much better estimated than some others,
 but this confidence does not appear in the PECM.
 In practice, we found that 
\emph on
pruning
\emph default
 the edges with too few observations improved the results.
 
\begin_inset ERT
status open

\begin_layout Plain Layout

% easter
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Note: this approach is only valid if some assumptions are made about the
 generation mechanism.
 It is valid if the production mechanism is independent from the values
 being generated, and also if all the coefficients are uniformly covered
 (XXX to verify).
 In practice there is a sever imbalance in the number of observations per
 link (i.e.
 the distribution of 
\begin_inset Formula $N_{i}$
\end_inset

 is very skewed), as can be seen from Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:power_law"

\end_inset

.
 The algorithm we have described here cannot take that into account, but
 a more direct approach can do it (CITE cdc paper).
 Some more experiments may be necessary here.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
This page intentionally left blank for the furry bunnies to wander.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_body
\end_document
