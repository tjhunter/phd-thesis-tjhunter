#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Conclusion}
\end_layout

\begin_layout Plain Layout


\backslash
epigraph{He attacked everything in life
\end_layout

\begin_layout Plain Layout

with a mixture of extraordinary genius
\end_layout

\begin_layout Plain Layout

and naive incompetence and it was often
\end_layout

\begin_layout Plain Layout

difficult to tell which was which.}{Douglas Adams, 
\backslash
emph{in} The Hitchhiker's guide to the galaxy} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chapter:conclusion"

\end_inset


\end_layout

\begin_layout Standard
We now conclude this dissertation.
 This thesis presents a comprehensive study to the question of estimating
 travel times across a large road network.
 It studies the challenges of building a pipeline and estimation algorithms
 that operate at large scale from real-world datasets.
 In particular, it studies the limitations of available techniques when
 confronted to the diversity of data collected in the real world.
\end_layout

\begin_layout Standard
The industry has already deployed traffic information systems that serve
 millions of persons with great success.
 These systems collect very large amounts of data from users and in return
 give some information about travel times.
 However, this process does not give any confidence interval or statistical
 guarantees to the user.
 We propose in this dissertation several models that address this deficiency.
\end_layout

\begin_layout Section
Advantages and deficiencies of data-driven models
\end_layout

\begin_layout Standard
When describing a phenomenon, a model should have two goals: 
\emph on
explain 
\emph default
and 
\emph on
predict.
 
\emph default
A good model should reconcile some laws considered to hold within the context
 of the experiment with some observations.
 This is the explanation part.
 It should further be able to predict some aspects not observed yet.
 In all cases, there are some intrinsic variables within the model that
 need to be calibrated against the data and with respect to the physics
 behind the model.
 In physical models, these variables correspond to physical quantities that
 inform the scientist about the phenomenon.
 In statistical models, these variables are usually non-dimensional parameters
 that describe abstract correlations between random values.
 
\end_layout

\begin_layout Standard
In the case of traffic, we make the case that loosing the comprehension
 that goes along with physical models gives access to a much richer class
 of models that describe the observation better.
\end_layout

\begin_layout Standard
The second goal of prediction can be quantatively evaluated, although there
 will be always some argument about the proper metrics and protocols to
 evaluate the predictive power of a model.
 Purely statistical models, which use previous data to predict the distribution
 of future data, do not offer much insight about the system we are looking
 at.
 However, their calibration parameters can usually be refined using well-underst
ood optimization techniques when presented with data, and the quality of
 their inference is directly related to the calibration.
 In particular, they can usually be adjusted to prevent overfitting.
 Models in which calibration parameters are physical constants can be much
 harder to calibrate to the data at hand.
 In particular, they rely on some assumptions that may not be verified in
 practice.
 Furthermore, it may not be possible to diagnose if all the assumptions
 required by these models are actually met within the experiment (such as
 deciding that some quantities are negligible against others).
 Naively calibrating them with observed data may lead to a good fit but
 some unrealistic choice of parameters.
 It is common to add some Bayesian priors or smoothing/regularization terms,
 but these terms are seldom grounded with some theoretical justification.
 Furthermore, data-driven models are often driven by smooth parameters and
 degrade gracefully with the lack of data.
 Physical models have a fixed number of parameters that depend on the theoretica
l considerations.
 This number is usually very hard to change because it depends on the relation
 between different physical quantities, unlike sparsity or BIC criterions.
 This means that physical models have a lower bound in terms of observation
 if they want to retain some predictive power and not overfit the existing
 data.
 On the other hand, the number of parameters of statistical models can be
 adjusted in an automated fashion based on the amount of data at hand.
\end_layout

\begin_layout Standard
In both cases, some diagnostic tools exist such as the bootstratp.
 It would be advantageous for the transportation community to investigate
 the confidence intervals of some bounds on the calibration of physical
 parameters, to ensure that the bias of the experimenter is not coming into
 play.
\end_layout

\begin_layout Standard
Statistical models are not restricted to a specific structure, and thus
 can be structured in a way that is more amenable to computations, but cannot
 be justified on theoretical grounds.
 For example, it is clear that the traffic on one road link will have an
 impact on surrounding road links, but it is a common assumption to consider
 each link to be independent.
 A lot of transportation litterature has focused on adding some correlations,
 but the conservation laws behind it are complicated to model accurately.
 The simple model introduced in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

 can predict such correlations, but it does not give any insights about
 these correlations.
\end_layout

\begin_layout Standard
Use the physical insight to drive the overall structure of relationship
 between the variables at hand, and then let statistical machinery find
 the best estimator.
 Goal-driven approach.
 Complex models will not help in this respect.
\end_layout

\begin_layout Section
Summary of contributions
\end_layout

\begin_layout Standard
This thesis shows how a data-driven approach that relies on insights derived
 from the data, combined with a quantitative approach towards measurement
 goals, can lead to an estimation platform that is both robust and reliable,
 and can work at very large scales.
 Physical models are used to inform the choice of statistical models, and
 new techniques are presented to use these statistical models at scale.
 We present a framework that works for different tradeoffs of quality of
 data, timeliness and computing resources.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:pif"

\end_inset

 presents a principled approach for map-mathing GPS data onto the road network.
 This approach works on a wide range of conditions, and is shown to encompass
 a large range of existing techniques.
 We have presented a novel class of algorithms to track moving vehicles
 on a road network: the 
\emph on
Path Inference Filter
\emph default
.
 This algorithm first projects the raw points onto candidate projections
 on the road network and then builds candidate trajectories to link these
 candidate points.
 An observation model and a driver model are then combined in a Conditional
 Random Field to find the most probable trajectories.
\end_layout

\begin_layout Standard
The algorithm exhibits robustness to noise as well as to the peculiarities
 of driving in urban road networks.
 It is competitive over a wide range of sampling rates (1 seconds to 2 minutes)
 and greatly outperforms intuitive deterministic algorithms.
 Furthermore, given a set of ground truth data, the filter can be automatically
 tuned using a fast supervised learning procedure.
 Alternatively, using enough regular GPS data with no ground truth, it can
 be trained using unsupervised learning.
 Experimental results show that the unsupervised learning procedure compares
 favorably against learning from ground truth data.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:socc"

\end_inset

 presents how to use very large quantities of extremely noisy observations
 at scale to estimate travel times on a large network.
 As datasets grow in size, some new strategies are required to perform meaningfu
l computations in a short amount of time.
 We explored the implementation of a large-scale state estimation in near-real-t
ime using Discretized Streams, a recently proposed streaming technique.
 Our traffic algorithm is an Expectation-Maximization algorithm that computes
 travel time distributions of traffic by incremental online updates.
 This approach was validated with a large dataset of GPS traces.
 This algorithm seems to compare favorably with the state of the art and
 shows some attractive scalability features from an implementation perspective.
 When distributed on a cluster, this algorithm scales to very large road
 networks (half a million road links, tens of thousands of observations
 per second) and can update traffic state in a few seconds.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

 focuses on a travel time estimator that works with future GPS data and
 that takes into account the correlations of travel times.
 The state of the art for travel time estimation has focused on either precise
 and computationally intensive physical models, or large scale, data-driven
 approaches.
 We have presented a novel algorithm for travel time estimation that aims
 at combining the best of both worlds, by combining physical insights with
 some scalable algorithms.
 We model the variability of travel times due to stops at intersections
 using a Stop-Go model (to detect stops) and a Hidden Markov Model to learn
 the spatial dependencies between stop locations.
 We also take into account the spatio-temporal correlations of travel times
 due to driving behavior or congestion, using a Gaussian Markov Random Fields.
 In particular, we present a highly scalable algorithm to train and perform
 inference on Gaussian Markov Random Fields, when applied on geographs.
\end_layout

\begin_layout Standard
We analyze the accuracy of the model using probe vehicle data collected
 over the Bay Area of San Francisco, CA.
 The results underline the importance to take into account the multi-modality
 of travel times in arterial networks due to the presence of traffic signals.
 The quality of the results we obtain are competitive with the state of
 the state of the art in traffic, and also highlight the good scalability
 of our algorithm.
\end_layout

\end_body
\end_document
