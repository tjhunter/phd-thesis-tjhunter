#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Conclusion}
\end_layout

\begin_layout Plain Layout


\backslash
epigraph{He attacked everything in life
\end_layout

\begin_layout Plain Layout

with a mixture of extraordinary genius
\end_layout

\begin_layout Plain Layout

and naive incompetence and it was often
\end_layout

\begin_layout Plain Layout

difficult to tell which was which.}{Douglas Adams, 
\backslash
emph{in} The Hitchhiker's guide to the galaxy} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chapter:conclusion"

\end_inset


\end_layout

\begin_layout Standard
And now we conclude this dissertation.
\end_layout

\begin_layout Standard
This thesis presents a comprehensive study to the question of studying traffic.
 It studies the challenges of building a pipeline and an estimation algorithm
 that works at large scale from real-world datasets.
 In particular, it studies the limitations of available techniques when
 confronted to the diversity of data collected in the real world.
\end_layout

\begin_layout Subsection
Advantages and deficiencies of data-driven models
\end_layout

\begin_layout Standard
Data-driven models are often more efficient to tune and to have inference
 models than physics-based models.
\end_layout

\begin_layout Standard
Rather than discussing about data-driven model against physics-based models,
 we emphasize that a model should have two goals: 
\emph on
enlighten
\emph default
 the researcher about the phenomenons at hand, and 
\emph on
predict
\emph default
 some aspects currently being observed, or future aspets not observed yet.
 In any case, the first point is highly subjective and responds to the sensibili
ty of the scientist towards certain fashions.
 The second goal of prediction can be quantatively evaluated, although some
 will debate about the proper way to evaluate the predictive power of a
 model.
 Purely statistical models, which use previous data to predict the distribution
 of future data, do not offer much insight about the system we are looking
 at.
 However, their calibration parameters can usually be refined using well-underst
ood optimization techniques when presented with data, and the quality of
 their inference is directly related to the calibration.
 In particular, they can usually be adjusted to prevent overfitting.
 Models in which calibration parameters are physical constants can be much
 harder to calibrate to the data at hand.
 In particular, they rely on some assumptions that may not be verified in
 practice.
 Naively calibrating them with observed data may lead to some unrealistic
 choice of parameters.
 It is common to add some Bayesian priors or smoothing/regularization terms,
 but these terms are seldom grounded with some theoretical justification.
 Furthermore, data-driven models are often driven by smooth parameters and
 degrade gracefully with the lack of data.
 Physical models have a fixed number of parameters that depend on the theoretica
l considerations.
 This number is usually very hard to change because it depends on the relation
 between different physical quantities, unlike sparsity or BIC criterions.
 This means that physical models have a lower bound in terms of observation
 if they want to retain some predictive power and not overfit the existing
 data.
 On the other hand, the number of parameters of statistical models can be
 adjusted in an automated fashion.
\end_layout

\begin_layout Standard
In both cases, some diagnostic tools exist such as the bootstratp.
 It would be advantageous for the transportation community to investigate
 the confidence intervals of some bounds on the calibration of physical
 parameters, to ensure that the bias of the experimenter is not coming into
 play.
\end_layout

\begin_layout Standard
Data bias: we consider crowd-sourced datasets, in which we do not choose
 the subset of observations.
 The mechanism of observation is not uniform, and we are confronted with
 very different levels of confidence for different parts of the system.
 In practice, it is hard to consider global conservation laws to infer additiona
l information from regions with a high density of observation.
 This is why we need a model that degrades gracefully as the number of observati
ons tends towards zero.
 Ideally, a physical model could be run on some parts on the system, while
 more statistical model can be deployed over the rest.
 One should note in our case that if there are few observations on a part
 of the network, it translates into little traffic (assuming the distribution
 is not too biased).
 This is why it is more important to have high-quality models where a lot
 of observations are collected, but less important for most regions (hence
 we can use a simpler model for these regions).
\end_layout

\begin_layout Standard
Statistical models are not restricted to a specific structure, and thus
 can be structured in a way that is more amenable to computations, but cannot
 be justified on theoretical grounds.
 For example, it is clear that the traffic on one road link will have an
 impact on surrounding road links, but it is a common assumption to consider
 each link to be independent.
 A lot of transportation litterature has focused on adding some correlations,
 but the conservation laws behind it are complicated to model accurately.
 The simple model introduced in Chapter XXX can predict such correlations,
 but it does not give any insights about these correlations.
\end_layout

\begin_layout Standard
Use the physical insight to drive the overall structure of relationship
 between the variables at hand, and then let statistical machinery find
 the best estimator.
 Goal-driven approach.
 Complex models will not help in this respect.
\end_layout

\begin_layout Subsection
Summary of contributions
\end_layout

\begin_layout Standard
This thesis shows how a data-driven approach that relies on insights derived
 from the data, combined with a quantitative approach towards measurement
 goals, can lead to an estimation platform that is robust and reliable,
 and can work at very large scales.
 Physical models are used to inform the choice of statistical models, and
 new techniques are presented to use these statistical models at scale.
 We present a platform that works for different tradeoffs of quality of
 data, timeliness and computing resources.
\end_layout

\begin_layout Standard
Chapter XXX presents a principled approach for map-mathing GPS data onto
 the road network.
 This approach works on a wide range of conditions, and is shown to encompass
 a large range of existing techniques.We have presented a novel class of
 algorithms to track moving vehicles on a road network: the 
\emph on
path inference filter
\emph default
.
 This algorithm first projects the raw points onto candidate projections
 on the road network and then builds candidate trajectories to link these
 candidate points.
 An observation model and a driver model are then combined in a Conditional
 Random Field to find the most probable trajectories.
\end_layout

\begin_layout Standard
The algorithm exhibits robustness to noise as well as to the peculiarities
 of driving in urban road networks.
 It is competitive over a wide range of sampling rates (1 seconds to 2 minutes)
 and greatly outperforms intuitive deterministic algorithms.
 Furthermore, given a set of ground truth data, the filter can be automatically
 tuned using a fast supervised learning procedure.
 Alternatively, using enough regular GPS data with no ground truth, it can
 be trained using unsupervised learning.
 Experimental results show that the unsupervised learning procedure compares
 favorably against learning from ground truth data.
 One may conclude that given enough observations, there no need to collect
 expensive high-frequency data to train a model.
\end_layout

\begin_layout Standard
This algorithm supports a range of trade-offs between accuracy, timeliness
 and computing needs.
 In its most accurate settings, it extends the current state of the art
 
\begin_inset CommandInset citation
LatexCommand cite
key "zheng2011weight,yuan2010interactive"

\end_inset

.
 This result is supported by the theoretical foundations of Conditional
 Random Fields.
 Because no standardized benchmark exists, the authors have released an
 open-source implementation of the filter to foster comparison with other
 methodologies using other datasets 
\begin_inset CommandInset citation
LatexCommand cite
key "pythonimpl"

\end_inset

.
\end_layout

\begin_layout Standard
In conjunction with careful engineering, this program can achieve high map-match
ing throughput.
 The authors have written an industrial-strength version in the Scala programmin
g language, deployed in the 
\emph on
Mobile Millennium 
\emph default
system.
 This version maps GPS points at a rate of about 400 points per second on
 a single core for the San Francisco Bay area (several hundreds of thousands
 of road links), and has been scaled to multicore architecture to achieve
 an average throughput of several thousand points per second 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter2011SOCC"

\end_inset

.
 This implementation has been successfully deployed in Stockholm and Sacramento
 as well.
\end_layout

\begin_layout Standard
A number of extensions could be considered to the core framework.
 In particular, more detailed models of the driver behavior as well as algorithm
s for automatic feature selection should bring additional improvements in
 performance.
 Another line of research is the mapping of very sparse data (sampling intervals
 longer than two minutes).
 Although the filter already attempts to consider as few trajectories as
 possible, more aggressive pruning may be necessary in order to achieve
 good performance.
 Finally, the EM procedure presented for automatically tuning the algorithm
 requires large amounts of data to be effective, and could be tested on
 larger datasets that what we have presented here.
\end_layout

\begin_layout Standard
Chapter XXX presents how to use very large quantities of extremely noisy
 observations at scale.
 As datasets grow in size, some new strategies are required to perform meaningfu
l computations in a short amount of time.
 We explored the implementation of a large-scale state estimation in near-real-t
ime using D-Streams, a recently proposed streaming technique.
 Our traffic algorithm is an Expectation-Maximization algorithm that computes
 travel time distributions of traffic by incremental online updates.
 This approach was validated with a large dataset of GPS traces.
 This algorithm seems to compare favorably with the state of the art and
 shows some attractive features from an implementation perspective.
 When distributed on a cluster, this algorithm scales to very large road
 networks (half a million road links, tens of thousands of observations
 per second) and can update traffic state in a few seconds.
\end_layout

\begin_layout Standard
Chapter XXX focuses on a travel time estimator that works with future GPS
 data and that takes into account the correlations of travel times.
 The state of the art for travel time estimation has focused on either precise
 and computationally intensive physical models, or large scale, data-driven
 approaches.
 We have presented a novel algorithm for travel time estimation that aims
 at combining the best of both worlds, by combining physical insights with
 some scalable algorithms.
 We model the variability of travel times due to stops at intersections
 using a Stop-Go model (to detect stops) and a HMM to learn the spatial
 dependencies between stop locations.
 We also take into account the spatio-temporal correlations of travel times
 due to driving behavior or congestion, using a Gaussian Markov Random Fields.
 In particular, we present a highly scalable algorithm to train and perform
 inference on Gaussian Markov Random Fields, when applied on geographs.
\end_layout

\begin_layout Standard
We analyze the accuracy of the model using probe vehicle data collected
 over the Bay Area of San Francisco, CA.
 The results underline the importance to take into account the multi-modality
 of travel times in arterial networks due to the presence of traffic signals.
 The quality of the results we obtain are competitive with the state of
 the state of the art in traffic, and also highlight the good scalability
 of our algorithm.
\end_layout

\end_body
\end_document
