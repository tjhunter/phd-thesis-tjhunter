#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options false
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Fast inference in the travel time graph
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kdd_inference"

\end_inset


\end_layout

\begin_layout Standard
After the model parameters 
\begin_inset Formula $\theta$
\end_inset

 have been determined, we turn our attention to the computation of travel
 times across a path.
 The inference task takes as input a valid path 
\begin_inset Formula $p=\left(p_{1}\cdots p_{B}\right)\in\mathcal{L}^{B}$
\end_inset

 and returns the cumulative distribution of the path travel time 
\begin_inset Formula $Z^{\left(p\right)}$
\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
h\left(t\right):\,\mathbb{R}^{+} & \rightarrow & \left[0,1\right]\\
t\,\,\,\, & \rightarrow & \mathbb{P}\left(Z^{(p)}\le t\right)
\end{eqnarray*}

\end_inset

The path travel time (i.e.
\begin_inset space \space{}
\end_inset

total travel time along the path) is easily defined in a conditional way.
 Assuming that the states 
\begin_inset Formula $S_{l}$
\end_inset

 of the links have been fixed to the values 
\begin_inset Formula $s_{l}$
\end_inset

, the travel time is then an instantiation of the sum of the time variables
 that correspond to this state: 
\begin_inset Formula 
\[
Z^{\left(p\right)}|\left(s_{l}\right)_{l}=\sum_{i=1}^{B}X_{l_{i},s_{l_{i}}}
\]

\end_inset

Then, the total travel time is one instantiation across a possible state
 of the links:
\begin_inset Formula 
\[
Z^{\left(p\right)}=\sum_{s\in\left[1,m\right]^{\mathcal{L}}}\mathbf{1}\left\{ S=s\right\} Z^{\left(p\right)}|s
\]

\end_inset

Since the variable 
\begin_inset Formula $Z^{\left(p\right)}|s$
\end_inset

 is a sum of (dependant) Gaussian variables, the variable 
\begin_inset Formula $Z^{\left(p\right)}$
\end_inset

 is simply a mixture of one-dimensional Gaussian distribution.
 However, this mixture has as many elements as the number of state configuration
s along the path 
\begin_inset Formula $p$
\end_inset

, which is 
\begin_inset Formula $m^{B}$
\end_inset

.
 This number prevents the computation for any reasonably large path (
\begin_inset Formula $m=3,\, B=100$
\end_inset

).
 We are going to present some algorithms that can approximate this mixture:
 one algorithm based on sampling, one algorithm based of convolution for
 independent Gaussians, and one algorithm based on a junction tree approximation
 for non-independent variables.
\end_layout

\begin_layout Standard
Before we introduce these algorithms, we are going to formalize the problem
 and introduce additional notations.
 Note that we can marginalize out all the variables that do not pertain
 to path 
\begin_inset Formula $p$
\end_inset

, so we can reduce the problem to a linear chain of variables (see Figures
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-MM-GMRF"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ttgraph_example"

\end_inset

).
 
\end_layout

\begin_layout Subsection
Formalization
\end_layout

\begin_layout Standard
We consider the parametrization of a unidimensional distribution with a
 discrete Markov model coupled with a joint multivariate gaussian distribution
 as follows (see an example in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-MM-GMRF"

\end_inset

).
 Consider a trellis of random variables stacked in 
\begin_inset Formula $B$
\end_inset

 layers, each of the layers having width 
\begin_inset Formula $m\geq1$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
The case of different width is straightforward to derive from the following
 discussion
\end_layout

\end_inset

.
 There is thus a total of 
\begin_inset Formula $Bm$
\end_inset

 variables.
 For the sake of convenience, we are going to simply label each variable
 by a pair of indexes 
\begin_inset Formula $u,v$
\end_inset

 so that each variable is denoted 
\begin_inset Formula $Y_{u,v}$
\end_inset

, with 
\begin_inset Formula $u\in\left[1,B\right]$
\end_inset

 and 
\begin_inset Formula $v\in\left[1,m\right]$
\end_inset

.
 The mapping to a link and a state 
\begin_inset Formula $X_{l,s}$
\end_inset

 to a variable 
\begin_inset Formula $Y_{u,v}$
\end_inset

 is straightforward (see Figure XXX for the relation between the different
 variables).
\end_layout

\begin_layout Standard
A 
\emph on
state path
\emph default
 in the trellis is a sequence of integers that corresponds to a choice of
 a variable in each of the layers:
\begin_inset Formula 
\[
q=q_{1}\cdots q_{B}\in\left[1,m\right]^{B}
\]

\end_inset

When we consider state subpath from layer 
\begin_inset Formula $u$
\end_inset

 to layer 
\begin_inset Formula $v$
\end_inset

 (inclusive), we will note 
\begin_inset Formula $q_{u:v}$
\end_inset

the partial state path: 
\begin_inset Formula 
\[
q_{u:v}=q_{u}q_{u+1}\cdots q_{v}\in\left[1,m\right]^{v-u}
\]

\end_inset


\end_layout

\begin_layout Standard
We now introduce some statistical models over this structure.
 All the variables 
\begin_inset Formula $Y_{u,v}$
\end_inset

 are stacked together into a single vector 
\begin_inset Formula $Y\,:\, y\in\mathbb{R}^{Bm}$
\end_inset

.
 We consider that the variables 
\begin_inset Formula $Y_{u,v}$
\end_inset

 are jointly Gaussian:
\begin_inset Formula 
\begin{equation}
Y\sim\mathcal{N}\left(\hat{\mu},\hat{\Sigma}\right)\label{eq:y-def}
\end{equation}

\end_inset

 with 
\begin_inset Formula $\hat{\mu}\in\mathbb{R}^{Bm}$
\end_inset

 and 
\begin_inset Formula $\hat{\Sigma}\in\mathcal{S}_{+}^{Bm}$
\end_inset

.
\end_layout

\begin_layout Standard
We introduce a Markov Model over the state paths:
\begin_inset Formula 
\begin{eqnarray}
q & \sim & \text{MM}\left(p\right)\label{eq:mm-process}
\end{eqnarray}

\end_inset

which is simply defined as a Markov chain transition, starting from state
 
\begin_inset Formula $q_{0}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray}
\pi\left(q_{0}\right) & = & \pi_{0}\left(q_{0}\right)\nonumber \\
\pi\left(q_{u:v}\right) & = & \pi_{u}\left(q_{u},q_{u+1}\right)\pi\left(q_{u+1:v}\right)\label{eq:mm-process-trans}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In this definition, the 
\begin_inset Formula $\pi_{0}\left(\cdot\right)$
\end_inset

 defines the distribution over the initial state: 
\begin_inset Formula $\sum_{j=1}^{m}\pi_{0}\left(j\right)=1$
\end_inset

, and the distributions 
\begin_inset Formula $\pi_{u}\left(\cdot,\cdot\right)$
\end_inset

 are the transition probabilities:
\begin_inset Formula 
\[
\forall u\in\left[1,B-1\right],\,\forall i\in\left[1,m\right]\,:\,\,\sum_{j=1}^{m}\pi_{u}\left(i,j\right)=1
\]

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename docs-kdd/layers.pdf
	width 6cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-MM-GMRF-1"

\end_inset


\begin_inset Argument
status open

\begin_layout Plain Layout
Example of MM-GMRF process
\end_layout

\end_inset

Example of MM-GMRF process.
 The 
\begin_inset Formula $q_{u}$
\end_inset

 variables are the continuous variables selection process.
 They form a discrete Markov model: the dependencies between the variables
 are represented by the arrows).
 The 
\begin_inset Formula $Y_{u,v}$
\end_inset

 variables make a joint Gaussian distribution.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We need one additional piece of notation: given a state path 
\begin_inset Formula $q$
\end_inset

 (which can be a sub-path), consider the 
\emph on
activation vector
\emph default
 
\begin_inset Formula $a\left(q\right)\in\mathbb{R}^{Bm}$
\end_inset

:
\begin_inset Formula 
\[
a\left(q_{u:v}\right)=\sum_{k=u}^{v}\mathbf{e}_{\left(k,q_{k}\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
The distribution 
\begin_inset Formula $Z$
\end_inset

 over the real is defined as a mixture of Gaussians over the paths:
\begin_inset Formula 
\[
q\sim\text{MM}\left(p\right)
\]

\end_inset


\begin_inset Formula 
\[
Z|q=a\left(q\right)^{T}Y
\]

\end_inset


\end_layout

\begin_layout Standard
Call 
\begin_inset Formula $f\left(\alpha,\beta\right)$
\end_inset

 the function that defines the p.d.f.
 of a univariate Normal distribution with mean 
\begin_inset Formula $\alpha$
\end_inset

 and variance 
\begin_inset Formula $\beta$
\end_inset

:
\begin_inset Formula 
\[
\forall y\in\mathbb{R},\,\, f\left(\alpha,\beta\right)\left(y\right)=\frac{1}{\sqrt{2\pi\beta}}e^{-\frac{1}{2\beta}\left(y-\alpha\right)^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
This notation will prove convenient when we define convolution operations
 over the space of functions.
 Then the p.d.f.
 of 
\begin_inset Formula $Z$
\end_inset

 is a mixture of univariate Gaussian distributions:
\begin_inset Formula 
\begin{align*}
\pi_{h} & =\sum_{q}\pi\left(q\right)\pi\left(\cdot|q\right)\\
 & =\sum_{q}\pi\left(q\right)f\left(a\left(q\right)^{T}\hat{\mu},a\left(q\right)^{T}\hat{\Sigma}a\left(q\right)\right)
\end{align*}

\end_inset


\begin_inset Formula 
\begin{equation}
\sigma\left(q\right)^{2}=a\left(p\right)^{T}\hat{\Sigma}a\left(q\right)\label{eq:def-sigma}
\end{equation}

\end_inset

There are 
\begin_inset Formula $m^{B}$
\end_inset

 possible paths, each of which makes a different element in the mixture.
 This number is much too large for enumerating all the elements of the mixture.
 We present here an algorithm that computes a discretized approximation
 (a histogram) of the p.d.f.
 of 
\begin_inset Formula $Z$
\end_inset

 in linear time.
\end_layout

\begin_layout Subsection
Old explanations XXX
\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathcal{S}=\{1,\dots,m\}^{I}$
\end_inset

 is the set of possible path states, and 
\begin_inset Formula $Z_{|s}^{\left(p\right)}$
\end_inset

 is the path travel time given the path state 
\begin_inset Formula $s$
\end_inset

, and is given by 
\begin_inset Formula $Z_{|s}^{\left(p\right)}=\sum_{l\in p}Y_{l,s_{l}}=e(p,s)^{T}Y$
\end_inset

.
 Here, 
\begin_inset Formula $e(p,s)$
\end_inset

 is a binary vector that selects the appropriate entries in the vector of
 travel times 
\begin_inset Formula $Y$
\end_inset

 (corresponding to path 
\begin_inset Formula $p$
\end_inset

 with states 
\begin_inset Formula $s$
\end_inset

): 
\begin_inset Formula 
\[
e(p,s)_{(l,s'_{l})}=1\Longleftrightarrow l\in p\text{ and }s'_{l}=s_{l}
\]

\end_inset

This vector 
\begin_inset Formula $e(p,s)$
\end_inset

 is very sparse and has precisely 
\begin_inset Formula $I$
\end_inset

 non-zero entries.
 Using the law of total probability we have: 
\begin_inset Formula 
\begin{equation}
\mathbb{P}\left(Z^{\left(p\right)}\right)=\sum_{s\in s}\mathbb{P}(S=s)\mathbb{P}\left(Z_{|s}^{\left(p\right)}\right)\label{eqn:total-prob}
\end{equation}

\end_inset

The variable 
\begin_inset Formula $Z_{|s}^{\left(p\right)}=e(p,s)^{T}Y$
\end_inset

 is a linear combination of the multivariate variable 
\begin_inset Formula $Y$
\end_inset

, and so is also normally distributed: 
\begin_inset Formula 
\begin{equation}
Z_{|s}^{\left(p\right)}\sim\mathcal{N}\left(e(p,s)^{T}\mu,e(p,s)^{T}\Sigma e(p,s)\right)\label{eqn:cond-prob}
\end{equation}

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% 
\backslash
audecomment{Don't like "is a subset of the joint multivariate distribution
 $Y$".
 Plus $
\backslash
condPathTT{p}{s}$ is a variable, not a distribution, same for $Y$.
 Can we say something like "The variable $
\backslash
condPathTT{p}{s} = e(p, s)^TY$ is a linear combination of the multivariate
 variable $Y$, and so 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% is also normally distributed:}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The marginal distribution of travel times along a path is a mixture of univariat
e Gaussian distributions.
 There is however two problems for this algorithm to be practical.
 
\emph on
(i)
\emph default
 The mixture from Equation
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eqn:total-prob"

\end_inset

 contains a term for each possible combination of states, and has size 
\begin_inset Formula $m^{I}$
\end_inset

.
 Enumerating all the terms is impossible for moderately large lengths of
 paths.
 
\emph on
(ii)
\emph default
 In order to compute the variance of each distribution of the mixture, one
 needs to estimate the covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

.
 However storing (or computing) the complete covariance matrix is prohibitively
 expensive with millions of variables.
\end_layout

\begin_layout Standard
We find tractable solutions for 
\emph on
(i)
\emph default
 by using a sampling method on the Markov model to choose a tractable number
 of states, and 
\emph on
(ii)
\emph default
 by using a random projection method to construct a low-rank approximation
 of the covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

.
 Note that the mean of the complete distribution can be computed exactly
 in 
\begin_inset Formula $O(Im^{2})$
\end_inset

 time, using a dynamic programming algorithm.
 Thus, our model is also applicable to situations that do not require the
 full distribution.
 We do not present this algorithm further due to space limitations.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% 
\backslash
audecomment{Shall we add a reference here?}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% no space :-(
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gibbs sampling
\end_layout

\begin_layout Standard
One of the simplest way of building an approximation of the distribution
 
\begin_inset Formula $Z$
\end_inset

 is to sample state paths according to Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mm-process"

\end_inset

, and consider the final distribution as representative of all the state
 path.
 Consider 
\begin_inset Formula $C$
\end_inset

 state paths sampled from 
\begin_inset Formula $p$
\end_inset

:
\begin_inset Formula 
\[
\forall c\in\left[1,C\right],\,\,\hat{q}_{c}\in\left[1,m\right]^{B}:\,\hat{q}_{c}\sim\text{MM}\left(p\right)
\]

\end_inset

Building these state paths is straightforward from the distribution encoded
 in Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mm-process-trans"

\end_inset

.
 Then the approximate mixture is:
\begin_inset Formula 
\[
\hat{\pi}_{h}=\frac{1}{C}\sum_{c}f\left(a\left(\hat{q}_{c}\right)^{T}\hat{\mu},\sigma\left(\hat{q}\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
It is clear that this distribution tends towards 
\begin_inset Formula $h$
\end_inset

 as 
\begin_inset Formula $C$
\end_inset

 grows.
 XXX more justifications?
\end_layout

\begin_layout Standard
This procedure is fast and easy to implement, however it is hard to compute
 an error bound on the quality of the approximation.
 Indeed, if the transitions are all uniform, then all the modes have equal
 probability and should all be contained in the distribution.
 In practice, the transitions are generally skewed towards a few dominant
 modes, so it may be applicable.
 We present next a procedure for which the error can be quantified and bounded.
\end_layout

\begin_layout Subsection
The case of the independent variables
\end_layout

\begin_layout Standard
For this section only, consider the simpler case in which the variables
 
\begin_inset Formula $Y_{u,v}$
\end_inset

 are all independent Gaussian distributions.
 Then the covariance matrix 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 is diagonal:
\begin_inset Formula 
\[
\hat{\Sigma}_{ab}=\left\{ \begin{array}{cc}
\Delta_{a} & a=b\\
0 & a\neq b
\end{array}\right.
\]

\end_inset

The key observation that derives from this choice of covariance is that
 the covariance of individual variables is decomposable along a path:
\begin_inset Formula 
\[
\sigma\left(p_{u:v}\right)^{2}=\sum_{w=u}^{v}\Delta_{\left(w,p_{w}\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
We consider the problem of computing the distribution over a set of subpaths
 that start and end at the same variable.
 Given 
\begin_inset Formula $u<v$
\end_inset

, 
\begin_inset Formula $i\in\left[1,m\right]$
\end_inset

 and 
\begin_inset Formula $j\in\left[1,m\right]$
\end_inset

, our goal is to compute the partial sum of the mixture:
\begin_inset Formula 
\[
\widetilde{\pi}_{u:v}\left(i\rightarrow j\right)=\sum_{q=q_{u}\cdots q_{v}:q_{u}=i\,,\, q_{v}=j}\pi\left(q\right)f\left(a\left(q\right)^{T}\mu,\sigma\left(q\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
This is not a properly scaled probability distribution.
 However, when summing over all the possible outputs 
\begin_inset Formula $j$
\end_inset

, this is a valid probability distribution:
\begin_inset Formula 
\[
\int\sum_{k=1}^{n_{u}}\tilde{\pi}_{u:v}\left(i\rightarrow k\right)\left(x\right)\text{d}x=1
\]

\end_inset


\end_layout

\begin_layout Standard
We have the following recursive relation:
\end_layout

\begin_layout Proposition
Recursion for indepent variables:
\begin_inset Formula 
\[
\widetilde{\pi}_{u:v}\left(i\rightarrow j\right)=f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot\left[\sum_{k=1}^{n_{u+1}}\pi_{u}\left(i,k\right)\widetilde{\pi}_{u+1:v}\left(k\rightarrow j\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
in which 
\begin_inset Formula $\odot$
\end_inset

 is the convolution operator between two functions:
\begin_inset Formula 
\[
\left(g\odot h\right)\left(x\right)=\int g\left(x-t\right)h\left(t\right)\text{d}t
\]

\end_inset


\end_layout

\begin_layout Standard
The main insight is to decompose each of the modes of the distribution using
 a convolution operator, which is a consequence of using independent Gaussian
 distribution.
 Using our notation:
\begin_inset Formula 
\[
\forall\beta,\beta^{'}>0,\,\, f\left(\alpha+\alpha',\beta+\beta'\right)=f\left(\alpha',\beta'\right)\odot f\left(\alpha,\beta\right)
\]

\end_inset


\end_layout

\begin_layout Proof
Consider two inpedendent Gaussian random variables 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $V'$
\end_inset

 with respective means 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\alpha^{'}$
\end_inset

, and variances 
\begin_inset Formula $\beta$
\end_inset

 an 
\begin_inset Formula $\beta^{'}$
\end_inset

.
 The resulting distribution 
\begin_inset Formula $W$
\end_inset

 has mean 
\begin_inset Formula $\alpha+\alpha'$
\end_inset

 and variance 
\begin_inset Formula $\beta+\beta'$
\end_inset

.
\end_layout

\begin_layout Standard
Since the convolution operator is associative over the space of real functions,
 we can map the associativity of the parameters to the function space and
 factorize some common operations.
 Here is the proof of the main proposition.
\end_layout

\begin_layout Proof
From the lemma above, for any path:
\begin_inset Formula 
\[
f\left(a\left(q_{u:v}\right)^{T}\mu,\sigma\left(q_{u:v}\right)^{2}\right)=f\left(\mu_{\left(u,q_{u}\right)},\Delta_{\left(u,q_{u}\right)}\right)\odot f\left(a\left(q_{u+1:v}\right)^{T}\mu,\sigma\left(q_{u+1:v}\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
Using the definition of the partial mixture sum:
\begin_inset Formula 
\begin{eqnarray*}
\widetilde{\pi}_{u:v}\left(i\rightarrow j\right) & = & \sum_{q=q_{u}\cdots q_{v}:q_{u}=i\,,\, q_{v}=j}\pi\left(q\right)f\left(a\left(q\right)^{T}\mu,\sigma\left(q\right)^{2}\right)\\
 & = & \sum_{k=1}^{n_{u+1}}\sum_{q_{u+2:v}:q_{v}=j}\pi\left(i,\, k,\, q_{u+2:v}\right)f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu+\mu_{\left(u,i\right)},\sigma\left(k,\, q_{u+2:v}\right)^{2}+\Delta_{\left(u,i\right)}\right)\\
 & = & \sum_{k=1}^{n_{u+1}}\sum_{q_{u+2:v}:q_{v}=j}\pi_{u}\left(i,k\right)\pi\left(k,\, q_{u+2:v}\right)f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu+\mu_{\left(u,i\right)},\sigma\left(k,\, q_{u+2:v}\right)^{2}+\Delta_{\left(u,i\right)}\right)\\
 & = & \sum_{k=1}^{n_{u+1}}\pi_{u}\left(i,k\right)\sum_{q_{u+2:v}:q_{v}=j}\pi\left(k,\, q_{u+2:v}\right)f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu,\sigma\left(k,\, q_{u+2:v}\right)^{2}\right)\\
 & = & f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot\left[\sum_{k=1}^{n_{u+1}}\pi_{u}\left(i,k\right)\sum_{q_{u+2:v}:q_{v}=j}\pi\left(k,\, q_{u+2:v}\right)f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu,\sigma\left(k,\, q_{u+2:v}\right)^{2}\right)\right]\\
 & = & f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot\left[\sum_{k=1}^{n_{u+1}}\pi_{u}\left(i,k\right)\widetilde{\pi}_{u+1:v}\left(k\rightarrow j\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Once we can compute efficiently these partial mixture sums, they can be
 combined into complete distributions.
 Given a start probability 
\begin_inset Formula $\pi_{0}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\pi & = & \sum_{i=0}^{n_{1}}\sum_{j=0}^{n_{m}}\pi_{0}\left(i\right)\widetilde{\pi}_{1:m}\left(i\rightarrow j\right)\\
 & = & \sum_{i=0}^{n_{1}}\pi_{0}\left(i\right)\left[\sum_{j=0}^{n_{m}}\widetilde{\pi}_{1:m}\left(i\rightarrow j\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
Note
\series default
 The same algorithm could also work by considering the conditional distributions
\begin_inset Formula 
\[
\hat{\pi}_{u:v}\left(i\right)=\sum_{k=1}^{n_{v}}\widetilde{\pi}_{u:v}\left(i\rightarrow k\right)
\]

\end_inset

(which are well-defined, well-scaled probability distributions).
 However, as we will see later, using this explicit parameters leads to
 very elegant caching algorithms.
\end_layout

\begin_layout Subsection
Low rank covariance approximation
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% Preview source code from paragraph 31 to 35
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Compute the variances of each sequence of variables: 
\begin_inset Formula 
\[
e\left(s\right)^{T}\Sigma\, e\left(s\right)
\]

\end_inset

with 
\begin_inset Formula $s$
\end_inset

 being a valid sequence of variables in the graph of variables.
\end_layout

\begin_layout Standard
Using the full covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

 to estimate the covariance of each path 
\begin_inset Formula $\sigma\left(s\right)^{2}=e\left(p,s\right)^{T}\Sigma e\left(p,s\right)$
\end_inset

 is impractical for two reasons: as mentioned before, we cannot expect to
 compute and access the full covariance matrix, and also the sum 
\begin_inset Formula $e\left(p,s\right)^{T}\Sigma e\left(p,s\right)$
\end_inset

 sums 
\begin_inset Formula $I^{2}$
\end_inset

 elements from the covariance matrix.
 Since we do not need to know the variance terms with full precision, an
 approximation strategy using random projections is appropriate.
 More specifically, we use the following result from
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "achlioptas2001database"

\end_inset

:
\end_layout

\begin_layout Standard

\emph on
Given some fixed vectors 
\begin_inset Formula $v_{1}\cdots v_{n}\in\mathbb{R}^{d}$
\end_inset

 and 
\begin_inset Formula $\epsilon>0$
\end_inset

, let 
\begin_inset Formula $R\in\mathbb{R}^{k\times d}$
\end_inset

 be a random matrix with random Bernoulli entries 
\begin_inset Formula $\pm1/\sqrt{k}$
\end_inset

 and with 
\begin_inset Formula $k\geq24\epsilon^{-2}\log n$
\end_inset

.
 Then with probability at least 
\begin_inset Formula $1-1/n$
\end_inset

: 
\begin_inset Formula 
\[
\left(1-\epsilon\right)\left\Vert v_{i}\right\Vert ^{2}\leq\left\Vert Rv_{i}\right\Vert ^{2}\leq\left(1+\epsilon\right)\left\Vert v_{i}\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Call 
\begin_inset Formula $R$
\end_inset

 such a matrix.
 Consider the Cholesky decomposition of the precision matrix, 
\begin_inset Formula $S=LL^{T}$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray*}
\sigma\left(s\right)^{2} & = & e\left(p,s\right)^{T}\Sigma e\left(p,s\right)\\
 & = & e\left(p,s\right)^{T}L^{-T}L^{-1}e\left(p,s\right)\\
 & = & \left\Vert L^{-1}e\left(p,s\right)\right\Vert ^{2}
\end{eqnarray*}

\end_inset

From the following lemma, we can approximate this norm: 
\begin_inset Formula 
\begin{equation}
\hat{\sigma}^{2}\left(s\right)=\left\Vert Q^{T}e\left(p,s\right)\right\Vert ^{2}=\left\Vert \sum_{i=1\cdots I}Q_{\left(l_{i},s_{i}\right)}\right\Vert ^{2}
\end{equation}

\end_inset

with 
\begin_inset Formula $P=RL^{-1}$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 defined as obtained from the lemma above.
 Computing the approximate variance 
\begin_inset Formula $\hat{\sigma}^{2}$
\end_inset

 requires the addition of 
\begin_inset Formula $I$
\end_inset

 vectors of size 
\begin_inset Formula $k$
\end_inset

.
 In practice, this summing operation is vectorized and very fast.
\end_layout

\begin_layout Standard
This method assumes we can efficiently compute the Cholesky factorization,
 and that inversion operation 
\begin_inset Formula $L^{-1}x$
\end_inset

 is efficient as well.
 In our case, the graph of the GMRF is nearly planar.
 Some very efficient algorithms exist that compute the Cholesky factorization
 in near-linear time (
\begin_inset CommandInset citation
LatexCommand cite
key "chen2008algorithm,davis2009dynamic"

\end_inset

).
 In practice, computing the 
\begin_inset Formula $Q$
\end_inset

 matrix is very fast.
\end_layout

\end_body
\end_document
