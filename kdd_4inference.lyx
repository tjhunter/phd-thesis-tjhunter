#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options false
\begin_modules
theorems-std
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Fast inference in the travel time graph
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:kdd_inference"

\end_inset


\end_layout

\begin_layout Standard
Having seen how the model parameters 
\begin_inset Formula $\theta$
\end_inset

 have been determined, we turn our attention to the computation of travel
 times across a path.
 In the rest of the discussion, we consider that each link has the same
 number 
\begin_inset Formula $m$
\end_inset

 of discrete states.
 Generalizing to a different number of states is straightforward.
 The inference task takes as input a valid path 
\begin_inset Formula $p=\left(p_{1}\cdots p_{B}\right)\in\mathcal{L}^{B}$
\end_inset

 and returns the cumulative distribution of the path travel time 
\begin_inset Formula $Z^{\left(p\right)}$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
h\left(t\right):\,\mathbb{R}^{+} & \rightarrow & \left[0,1\right]\\
t\,\,\,\, & \rightarrow & \mathbb{P}\left(Z^{(p)}\le t\right)
\end{eqnarray*}

\end_inset

The path travel time (i.e.
\begin_inset space \space{}
\end_inset

total travel time along the path) is easily defined in a conditional way.
 Assuming that the states 
\begin_inset Formula $S_{l}$
\end_inset

 of the links have been fixed to the values 
\begin_inset Formula $s_{l}$
\end_inset

, the travel time is then an instantiation of the sum of the time variables
 that correspond to this state: 
\begin_inset Formula 
\[
Z^{\left(p\right)}|\left(s_{l}\right)_{l}=\sum_{i=1}^{B}X_{l_{i},s_{l_{i}}}
\]

\end_inset

Then, the total travel time is one instantiation across all possible state
 tuples of the links:
\begin_inset Formula 
\[
Z^{\left(p\right)}=\sum_{s\in\left[1,m\right]^{\mathcal{L}}}\mathbf{1}\left\{ S=s\right\} Z^{\left(p\right)}|s
\]

\end_inset

Since the conditional variable 
\begin_inset Formula $Z^{\left(p\right)}|s$
\end_inset

 is a sum of (dependent) Gaussian variables, the variable 
\begin_inset Formula $Z^{\left(p\right)}$
\end_inset

 is simply a mixture of one-dimensional Gaussian distribution.
 However, this mixture has as many elements as the number of state configuration
s along the path 
\begin_inset Formula $p$
\end_inset

, which is 
\begin_inset Formula $m^{B}$
\end_inset

.
 This number prevents the computation for any reasonably large path (
\begin_inset Formula $m=3,\, B=100$
\end_inset

).
 We are going to present some algorithms that can approximate the density
 function this mixture: one algorithm based on sampling, one algorithm based
 of convolution for independent Gaussians, and one algorithm based on a
 junction tree approximation for non-independent variables.
\end_layout

\begin_layout Standard
Before we introduce these algorithms, we are going to formalize the problem
 and introduce additional notations.
 Note that we can marginalize out all the variables that do not pertain
 to path 
\begin_inset Formula $p$
\end_inset

, so we can reduce the problem to a linear chain of variables (see Figures
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-MM-GMRF"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ttgraph_example"

\end_inset

).
 
\end_layout

\begin_layout Subsection
Formalization
\end_layout

\begin_layout Standard
We consider the parametrization of a unidimensional distribution with a
 discrete Markov model coupled with a joint multivariate gaussian distribution
 as follows (see an example in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-MM-GMRF"

\end_inset

).
 Consider a trellis of random variables stacked in 
\begin_inset Formula $B$
\end_inset

 layers, each of the layers having width 
\begin_inset Formula $m\geq1$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
The case of different width is straightforward to derive from the following
 discussion
\end_layout

\end_inset

.
 There is thus a total of 
\begin_inset Formula $Bm$
\end_inset

 normal variables and 
\begin_inset Formula $B$
\end_inset

 discrete variables, each with 
\begin_inset Formula $m$
\end_inset

 possible states.
 For the sake of convenience, we are going to simply label each variable
 by a pair of indexes 
\begin_inset Formula $u,v$
\end_inset

 so that each variable is denoted 
\begin_inset Formula $Y_{u,v}$
\end_inset

, with 
\begin_inset Formula $u\in\left[1,B\right]$
\end_inset

 and 
\begin_inset Formula $v\in\left[1,m\right]$
\end_inset

.
 The mapping to a link and a state 
\begin_inset Formula $X_{l,s}$
\end_inset

 to a variable 
\begin_inset Formula $Y_{u,v}$
\end_inset

 is straightforward
\begin_inset Note Note
status open

\begin_layout Plain Layout
(see Figure XXX for the relation between the different variables)
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
A 
\emph on
state path
\emph default
 in the trellis is a sequence of integers that corresponds to a choice of
 a variable in each of the layers:
\begin_inset Formula 
\[
q=q_{1}\cdots q_{B}\in\left[1,m\right]^{B}
\]

\end_inset

When we consider a 
\emph on
state subpath
\emph default
 from layer 
\begin_inset Formula $u$
\end_inset

 to layer 
\begin_inset Formula $v$
\end_inset

 (inclusive), we will note 
\begin_inset Formula $q_{u:v}$
\end_inset

the partial state path: 
\begin_inset Formula 
\[
q_{u:v}=q_{u}q_{u+1}\cdots q_{v}\in\left[1,m\right]^{v-u}
\]

\end_inset


\end_layout

\begin_layout Standard
We now introduce some statistical models over this structure.
 All the variables 
\begin_inset Formula $Y_{u,v}$
\end_inset

 are stacked together into a single vector 
\begin_inset Formula $Y\,:\, y\in\mathbb{R}^{Bm}$
\end_inset

.
 We consider that the variables 
\begin_inset Formula $Y_{u,v}$
\end_inset

 are jointly Gaussian:
\begin_inset Formula 
\begin{equation}
Y\sim\mathcal{N}\left(\hat{\mu},\hat{\Sigma}\right)\label{eq:y-def}
\end{equation}

\end_inset

 with 
\begin_inset Formula $\hat{\mu}\in\mathbb{R}^{Bm}$
\end_inset

 and 
\begin_inset Formula $\hat{\Sigma}\in\mathcal{S}_{+}^{Bm}$
\end_inset

.
\end_layout

\begin_layout Standard
We introduce a Markov Model over the state paths:
\begin_inset Formula 
\begin{eqnarray}
q & \sim & \text{MM}\left(p\right)\label{eq:mm-process}
\end{eqnarray}

\end_inset

which is simply defined as a Markov chain transition, starting from state
 
\begin_inset Formula $q_{0}\in\left[1,m\right]$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray}
\pi\left(q_{0}\right) & = & \pi_{0}\left(q_{0}\right)\nonumber \\
\pi\left(q_{u:v}\right) & = & \pi_{u}\left(q_{u},q_{u+1}\right)\pi\left(q_{u+1:v}\right)\label{eq:mm-process-trans}\\
 & = & \prod_{w=u}^{v-1}\pi_{w}\left(q_{w},q_{w+1}\right)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
In this definition, the 
\begin_inset Formula $\pi_{0}\left(\cdot\right)$
\end_inset

 defines the distribution over the initial state: 
\begin_inset Formula $\sum_{j=1}^{m}\pi_{0}\left(j\right)=1$
\end_inset

, and the distributions 
\begin_inset Formula $\pi_{u}\left(\cdot,\cdot\right)$
\end_inset

 are the transition probabilities:
\begin_inset Formula 
\[
\forall u\in\left[1,B-1\right],\,\forall i\in\left[1,m\right]\,:\,\,\sum_{j=1}^{m}\pi_{u}\left(i,j\right)=1
\]

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename docs-kdd/layers.pdf
	width 6cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-MM-GMRF"

\end_inset


\begin_inset Argument
status open

\begin_layout Plain Layout
Example of MM-GMRF process
\end_layout

\end_inset

Example of MM-GMRF process.
 The 
\begin_inset Formula $q_{u}$
\end_inset

 variables are the continuous variables selection process.
 They form a discrete Markov model: the dependencies between the variables
 are represented by the arrows).
 The 
\begin_inset Formula $Y_{u,v}$
\end_inset

 variables make a joint Gaussian distribution (their dependencies are represente
d by the box around all the variables).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We need one additional piece of notation: given a state path 
\begin_inset Formula $q$
\end_inset

 (which can be a sub-path), consider the 
\emph on
activation vector
\emph default
 
\begin_inset Formula $a\left(q\right)\in\mathbb{R}^{Bm}$
\end_inset

:
\begin_inset Formula 
\[
a\left(q_{u:v}\right)=\sum_{k=u}^{v}\mathbf{e}_{k,q_{k}}
\]

\end_inset


\end_layout

\begin_layout Standard
Using this notation, the description of the MM-GMRF process is much simplified.
 The distribution 
\begin_inset Formula $Z$
\end_inset

 over the real is defined as a mixture of Gaussians over the paths:
\begin_inset Formula 
\[
q\sim\text{MM}\left(p\right)
\]

\end_inset


\begin_inset Formula 
\[
Z|q=a\left(q\right)^{T}Y
\]

\end_inset


\end_layout

\begin_layout Standard
Call 
\begin_inset Formula $f\left(\alpha,\beta\right)$
\end_inset

 the function that defines the p.d.f.
 of a univariate Normal distribution with mean 
\begin_inset Formula $\alpha$
\end_inset

 and variance 
\begin_inset Formula $\beta$
\end_inset

:
\begin_inset Formula 
\[
\forall y\in\mathbb{R},\,\, f\left(\alpha,\beta\right)\left(y\right)=\frac{1}{\sqrt{2\pi\beta}}e^{-\frac{1}{2\beta}\left(y-\alpha\right)^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
This notation will prove convenient when we define convolution operations
 over the space of functions.
 Then the p.d.f.
 of 
\begin_inset Formula $Z$
\end_inset

 is a mixture of univariate Gaussian distributions:
\begin_inset Formula 
\begin{align*}
\pi_{h} & =\sum_{q}\pi\left(q\right)\pi\left(\cdot|q\right)\\
 & =\sum_{q}\pi\left(q\right)f\left(a\left(q\right)^{T}\hat{\mu},\sigma\left(q\right)^{2}\right)
\end{align*}

\end_inset

where the variance 
\begin_inset Formula $\sigma\left(q\right)^{2}$
\end_inset

 is introduced as a shortcut for the projection of the full covariance:
\begin_inset Formula 
\begin{equation}
\sigma\left(q\right)^{2}=a\left(p\right)^{T}\hat{\Sigma}a\left(q\right)\label{eq:def-sigma}
\end{equation}

\end_inset

There are 
\begin_inset Formula $m^{B}$
\end_inset

 possible paths, each of which makes a different element in the mixture.
 This number is much too large for enumerating all the elements of the mixture.
 We present here an algorithm that computes a discretized approximation
 (a histogram) of the p.d.f.
 of 
\begin_inset Formula $Z$
\end_inset

 in linear time.
\end_layout

\begin_layout Subsection
Gibbs sampling
\end_layout

\begin_layout Standard
One of the simplest way of building an approximation of the distribution
 
\begin_inset Formula $Z$
\end_inset

 is to sample state paths according to Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mm-process"

\end_inset

, and consider the final distribution as representative of all the state
 path.
 Consider 
\begin_inset Formula $C$
\end_inset

 state paths sampled from 
\begin_inset Formula $p$
\end_inset

:
\begin_inset Formula 
\[
\forall c\in\left[1,C\right],\,\,\hat{q}_{c}\in\left[1,m\right]^{B}:\,\hat{q}_{c}\sim\text{MM}\left(p\right)
\]

\end_inset

Building these state paths is straightforward from the distribution encoded
 in Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mm-process-trans"

\end_inset

.
 Then the approximate mixture is:
\begin_inset Formula 
\[
\hat{\pi}_{h}=\frac{1}{C}\sum_{c}f\left(a\left(\hat{q}_{c}\right)^{T}\hat{\mu},\sigma\left(\hat{q}\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
It is clear that this distribution tends towards 
\begin_inset Formula $h$
\end_inset

 as 
\begin_inset Formula $C$
\end_inset

 grows.
\end_layout

\begin_layout Standard
This procedure is fast and easy to implement, however it is hard to compute
 an error bound on the quality of the approximation.
 Indeed, if the transitions are all uniform, then all the modes have equal
 probability and should all be contained in the distribution.
 In practice, the transitions are generally skewed towards a few dominant
 modes, so it may be applicable.
 We present next a procedure for which the error can be quantified and bounded.
\end_layout

\begin_layout Subsection
The case of the independent variables
\end_layout

\begin_layout Standard
For this section only, consider the simpler case in which the variables
 
\begin_inset Formula $Y_{u,v}$
\end_inset

 are all independent Gaussian distributions.
 Then the covariance matrix 
\begin_inset Formula $\hat{\Sigma}$
\end_inset

 is diagonal:
\begin_inset Formula 
\[
\hat{\Sigma}_{ab}=\left\{ \begin{array}{cc}
\Delta_{a} & a=b\\
0 & a\neq b
\end{array}\right.
\]

\end_inset

The key observation that derives from this choice of covariance is that
 the covariance of individual variables is decomposable along a path:
\begin_inset Formula 
\[
\sigma\left(p_{u:v}\right)^{2}=\sum_{w=u}^{v}\Delta_{w,p_{w}}
\]

\end_inset


\end_layout

\begin_layout Standard
We consider the problem of computing the distribution over a set of subpaths
 that start and end at the same variable.
 Given 
\begin_inset Formula $u<v$
\end_inset

, 
\begin_inset Formula $i\in\left[1,m\right]$
\end_inset

 and 
\begin_inset Formula $j\in\left[1,m\right]$
\end_inset

, our goal is to compute the partial sum of the mixture:
\begin_inset Formula 
\[
\widetilde{\pi}_{u:v}\left(i\rightarrow j\right)=\sum_{q=q_{u}\cdots q_{v}:q_{u}=i\,,\, q_{v}=j}\pi\left(q\right)f\left(a\left(q\right)^{T}\mu,\sigma\left(q\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
This is not a properly scaled probability distribution.
 However, when summing over all the possible outputs 
\begin_inset Formula $j$
\end_inset

, this is a valid probability distribution:
\begin_inset Formula 
\[
\int\sum_{k=1}^{m}\tilde{\pi}_{u:v}\left(i\rightarrow k\right)\left(x\right)\text{d}x=1
\]

\end_inset


\end_layout

\begin_layout Standard
We have the following recursive relation:
\end_layout

\begin_layout Proposition
Recursion for independent variables:
\begin_inset Formula 
\[
\widetilde{\pi}_{u:v}\left(i\rightarrow j\right)=f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot\left[\sum_{k=1}^{m}\pi_{u}\left(i,k\right)\widetilde{\pi}_{u+1:v}\left(k\rightarrow j\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
in which 
\begin_inset Formula $\odot$
\end_inset

 is the convolution operator between two functions:
\begin_inset Formula 
\[
\left(g\odot h\right)\left(x\right)=\int g\left(x-t\right)h\left(t\right)\text{d}t
\]

\end_inset


\end_layout

\begin_layout Standard
The main insight is to decompose each of the modes of the distribution using
 a convolution operator, which is a consequence of using independent Gaussian
 distributions.
 Using our notation:
\begin_inset Formula 
\begin{equation}
\forall\beta,\beta^{'}>0,\,\, f\left(\alpha+\alpha',\beta+\beta'\right)=f\left(\alpha',\beta'\right)\odot f\left(\alpha,\beta\right)\label{eq:gaussian-convolution}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
Consider two independent Gaussian random variables 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $V'$
\end_inset

 with respective means 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\alpha^{'}$
\end_inset

, and variances 
\begin_inset Formula $\beta$
\end_inset

 an 
\begin_inset Formula $\beta^{'}$
\end_inset

.
 The resulting distribution 
\begin_inset Formula $W$
\end_inset

 has mean 
\begin_inset Formula $\alpha+\alpha'$
\end_inset

 and variance 
\begin_inset Formula $\beta+\beta'$
\end_inset

.
\end_layout

\begin_layout Standard
Since the convolution operator is associative over the space of real functions,
 we can map the associativity of the parameters to the function space and
 factorize some common operations.
 Here is the proof of the main proposition.
\end_layout

\begin_layout Proof
From the lemma above, for any path:
\begin_inset Formula 
\[
f\left(a\left(q_{u:v}\right)^{T}\mu,\sigma\left(q_{u:v}\right)^{2}\right)=f\left(\mu_{\left(u,q_{u}\right)},\Delta_{\left(u,q_{u}\right)}\right)\odot f\left(a\left(q_{u+1:v}\right)^{T}\mu,\sigma\left(q_{u+1:v}\right)^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
Using the definition of the partial mixture sum:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{eqnarray*}
\widetilde{\pi}_{u:v}\left(i\rightarrow j\right) & = & \sum_{q=q_{u}\cdots q_{v}:q_{u}=i\,,\, q_{v}=j}\pi\left(q\right)f\left(a\left(q\right)^{T}\mu,\sigma\left(q\right)^{2}\right)\\
 & = & \sum_{k=1}^{m}\sum_{q_{u+2:v}:q_{v}=j}\pi\left(i,\, k,\, q_{u+2:v}\right)\\
 &  & \times f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu+\mu_{\left(u,i\right)},\sigma\left(k,\, q_{u+2:v}\right)^{2}+\Delta_{\left(u,i\right)}\right)\\
 & = & \sum_{k=1}^{m}\sum_{q_{u+2:v}:q_{v}=j}\pi_{u}\left(i,k\right)\pi\left(k,\, q_{u+2:v}\right)\\
 &  & \times f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu+\mu_{\left(u,i\right)},\sigma\left(k,\, q_{u+2:v}\right)^{2}+\Delta_{\left(u,i\right)}\right)\\
 & = & \sum_{k=1}^{m}\pi_{u}\left(i,k\right)\sum_{q_{u+2:v}:q_{v}=j}\pi\left(k,\, q_{u+2:v}\right)\\
 &  & \times f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu,\sigma\left(k,\, q_{u+2:v}\right)^{2}\right)\\
 & = & f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot\\
 &  & \times\left[\sum_{k=1}^{m}\pi_{u}\left(i,k\right)\sum_{q_{u+2:v}:q_{v}=j}\pi\left(k,\, q_{u+2:v}\right)f\left(a\left(k,\, q_{u+2:v}\right)^{T}\mu,\sigma\left(k,\, q_{u+2:v}\right)^{2}\right)\right]\\
 & = & f\left(\mu_{\left(u,i\right)},\Delta_{\left(u,i\right)}\right)\odot\left[\sum_{k=1}^{m}\pi_{u}\left(i,k\right)\widetilde{\pi}_{u+1:v}\left(k\rightarrow j\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Once we can compute efficiently these partial mixture sums, they can be
 combined into complete distributions.
 Given a start probability 
\begin_inset Formula $\pi_{0}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\pi & = & \sum_{i=0}^{m}\sum_{j=0}^{m}\pi_{0}\left(i\right)\widetilde{\pi}_{1:m}\left(i\rightarrow j\right)\\
 & = & \sum_{i=0}^{n_{1}}\pi_{0}\left(i\right)\left[\sum_{j=0}^{n_{m}}\widetilde{\pi}_{1:m}\left(i\rightarrow j\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
Note
\series default
 The same algorithm could also work by considering the conditional distributions
\begin_inset Formula 
\[
\hat{\pi}_{u:v}\left(i\right)=\sum_{k=1}^{n_{v}}\widetilde{\pi}_{u:v}\left(i\rightarrow k\right)
\]

\end_inset

(which are well-defined, well-scaled probability distributions).
 However, as we will see later, using this explicit parameters leads to
 very elegant caching algorithms.
\begin_inset Note Comment
status open

\begin_layout Subsection
A linear approximation for full covariance
\end_layout

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
This is interesting but I do not have the time to write it down, let alone
 implement it.
 For someone else.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Local influence approximation of the covariance.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
The decomposability property of Gaussian distribution (Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gaussian-convolution"

\end_inset

), which is a fundamental property for the fast algorithm presented above,
 does not hold anymore for the case of full covariance matrices.
 However, one can approximate this property as follows.
 The full covariance matrix corresponds to marginalizing out the other variables
 of the GMRF.
 In effect, this leads to a complete graph for the variables.
 Instead of full marginalization, one can consider 
\begin_inset Quotes eld
\end_inset

taking out
\begin_inset Quotes erd
\end_inset

 the subgraph that corresponds to the path, effectively approximating the
 propagation of conditional information across neighboring nodes as being
 negligible.
 Then, this graph has additional conditional independence properties for
 each layers:
\begin_inset Formula 
\[
\forall w\in\left[2,B-1\right],\,\forall u<w,\forall u^{'}>w,\forall v,v^{'}\in\left[1,m\right]:\,\, Y_{u,v}\parallel Y_{u^{'},v^{'}}|\left(Y_{w,j}\right)_{j\in\left[1,m\right]}
\]

\end_inset


\end_layout

\begin_layout Plain Layout
This conditional independence property is represented in Figure XXX.
 This leads to an algorithm to compute approximately the marginal distribution
 
\begin_inset Formula $\pi_{h}$
\end_inset

.
 Given 
\begin_inset Formula $u<w<v$
\end_inset

, and fixing the values 
\begin_inset Formula $\left(Y_{w,j}\right)_{j\in\left[1,m\right]}$
\end_inset

 by 
\begin_inset Formula $\left(y_{w,j}\right)_{j}$
\end_inset

: XXX must add the start and end values 
\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
\widetilde{\pi}_{u:v}\left(i\rightarrow j\right)=\int_{y_{w,1}}\cdots\int_{y_{w,m}}\widetilde{\pi}_{u:v|w}\left(i\rightarrow j\left|\left(y_{w,j}\right)\right.\right)\text{d}y_{w,1}\cdots\text{d}y_{w,m}
\]

\end_inset


\begin_inset Formula 
\[
\widetilde{\pi}_{u:v|w}\left(i\rightarrow j\left|\left(y_{w,j}\right)\right.\right)=\sum_{k=1}^{m}\sum_{l=1}^{m}\hat{\eta}_{w}\left(k,l\left|y_{w,1}\cdots y_{w,m}\right.\right)\widetilde{\pi}_{u:w-1}\left(i\rightarrow k\right)\odot\widetilde{\pi}_{w+1:v}\left(l\rightarrow j\right)
\]

\end_inset

with 
\begin_inset Formula $\hat{w}_{u,v}\left(z_{1}\cdots z_{m}\right)$
\end_inset

 a constant that depend on the parameters of the level 
\begin_inset Formula $u$
\end_inset

 of the trellis:
\begin_inset Formula 
\[
\hat{\eta}_{w}\left(i,j\left|z_{1}\cdots z_{m}\right.\right)=\sum_{k^{'}=1}^{m}\pi_{w}\left(i,k^{'}\right)\pi_{w+1}\left(k^{'},j\right)f\left(\mu_{w+1,k^{'}},\Delta_{w+1,k^{'}}\right)\left(z_{k^{'}}\right)
\]

\end_inset


\end_layout

\begin_layout Plain Layout
From this algorithm, it should be clear how to compute the distribution
 following a divide-and-conquer strategy: given some indexes 
\begin_inset Formula $u,v$
\end_inset

 compute recursively all the 
\begin_inset Formula $\widetilde{\pi}_{u:w-1}\left(i\rightarrow k\right)$
\end_inset

 and 
\begin_inset Formula $\widetilde{\pi}_{w+1:v}\left(l\rightarrow j\right)$
\end_inset

 with 
\begin_inset Formula $w=\left\lceil \frac{u+v}{2}\right\rceil $
\end_inset

.
 Then make a grid of 
\begin_inset Formula $\left(y_{w,j}\right)_{j}$
\end_inset

 and compute an approximate value for the integral.
\end_layout

\begin_layout Plain Layout
Obviously, this scheme is only practical for small values of 
\begin_inset Formula $m$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

, but it is interesting to explore in practice, especially as most of these
 computations can be cached and efficiently computed on GPUs.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Low rank covariance approximation
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% Preview source code from paragraph 31 to 35
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Compute the variances of each sequence of variables: 
\begin_inset Formula 
\[
e\left(s\right)^{T}\Sigma\, e\left(s\right)
\]

\end_inset

with 
\begin_inset Formula $s$
\end_inset

 being a valid sequence of variables in the graph of variables.
\end_layout

\begin_layout Standard
Using the full covariance matrix 
\begin_inset Formula $\Sigma$
\end_inset

 to estimate the covariance of each path 
\begin_inset Formula $\sigma\left(s\right)^{2}=e\left(p,s\right)^{T}\Sigma e\left(p,s\right)$
\end_inset

 is impractical for two reasons: as mentioned before, we cannot expect to
 compute and access the full covariance matrix, and also the sum 
\begin_inset Formula $e\left(p,s\right)^{T}\Sigma e\left(p,s\right)$
\end_inset

 sums 
\begin_inset Formula $I^{2}$
\end_inset

 elements from the covariance matrix.
 Since we do not need to know the variance terms with full precision, an
 approximation strategy using random projections is appropriate.
 More specifically, we use the following result from
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "achlioptas2001database"

\end_inset

:
\end_layout

\begin_layout Standard

\emph on
Given some fixed vectors 
\begin_inset Formula $v_{1}\cdots v_{n}\in\mathbb{R}^{d}$
\end_inset

 and 
\begin_inset Formula $\epsilon>0$
\end_inset

, let 
\begin_inset Formula $R\in\mathbb{R}^{k\times d}$
\end_inset

 be a random matrix with random Bernoulli entries 
\begin_inset Formula $\pm1/\sqrt{k}$
\end_inset

 and with 
\begin_inset Formula $k\geq24\epsilon^{-2}\log n$
\end_inset

.
 Then with probability at least 
\begin_inset Formula $1-1/n$
\end_inset

: 
\begin_inset Formula 
\[
\left(1-\epsilon\right)\left\Vert v_{i}\right\Vert ^{2}\leq\left\Vert Rv_{i}\right\Vert ^{2}\leq\left(1+\epsilon\right)\left\Vert v_{i}\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Call 
\begin_inset Formula $R$
\end_inset

 such a matrix.
 Consider the Cholesky decomposition of the precision matrix, 
\begin_inset Formula $S=LL^{T}$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray*}
\sigma\left(s\right)^{2} & = & e\left(p,s\right)^{T}\Sigma e\left(p,s\right)\\
 & = & e\left(p,s\right)^{T}L^{-T}L^{-1}e\left(p,s\right)\\
 & = & \left\Vert L^{-1}e\left(p,s\right)\right\Vert ^{2}
\end{eqnarray*}

\end_inset

From the following lemma, we can approximate this norm: 
\begin_inset Formula 
\begin{equation}
\hat{\sigma}^{2}\left(s\right)=\left\Vert Q^{T}e\left(p,s\right)\right\Vert ^{2}=\left\Vert \sum_{i=1\cdots I}Q_{\left(l_{i},s_{i}\right)}\right\Vert ^{2}
\end{equation}

\end_inset

with 
\begin_inset Formula $P=RL^{-1}$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 defined as obtained from the lemma above.
 Computing the approximate variance 
\begin_inset Formula $\hat{\sigma}^{2}$
\end_inset

 requires the addition of 
\begin_inset Formula $I$
\end_inset

 vectors of size 
\begin_inset Formula $k$
\end_inset

.
 In practice, this summing operation is vectorized and very fast.
\end_layout

\begin_layout Standard
This method assumes we can efficiently compute the Cholesky factorization,
 and that inversion operation 
\begin_inset Formula $L^{-1}x$
\end_inset

 is efficient as well.
 In our case, the graph of the GMRF is nearly planar.
 Some very efficient algorithms exist that compute the Cholesky factorization
 in near-linear time (
\begin_inset CommandInset citation
LatexCommand cite
key "chen2008algorithm,davis2009dynamic"

\end_inset

).
 In practice, computing the 
\begin_inset Formula $Q$
\end_inset

 matrix is very fast.
\end_layout

\end_body
\end_document
