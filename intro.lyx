#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass book
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Introduction}
\end_layout

\begin_layout Plain Layout


\backslash
epigraph{From the midst of this 
\end_layout

\begin_layout Plain Layout

darkness a sudden light broke in upon
\end_layout

\begin_layout Plain Layout

me--a light so brilliant and wondrous, 
\end_layout

\begin_layout Plain Layout

yet so simple, that while I became dizzy 
\end_layout

\begin_layout Plain Layout

with the immensity of the prospect which it 
\end_layout

\begin_layout Plain Layout

illustrated, I was surprised that among so
\end_layout

\begin_layout Plain Layout

many men of genius who had directed their
\end_layout

\begin_layout Plain Layout

inquiries towards the same science, that 
\end_layout

\begin_layout Plain Layout

I alone should be reserved to discover so 
\end_layout

\begin_layout Plain Layout

astonishing a secret.}{Mary Shelley, 
\backslash
emph{in} Frankenstein}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chapter:introduction"

\end_inset


\end_layout

\begin_layout Section
Car traffic as a cyberphysical system
\end_layout

\begin_layout Standard

\series bold
Sensor fusion.
 
\series default
The integration of sensors and data, the increase in data volume, the decrease
 in costs is transforming most physical systems.
 The digital world is pervasively invading all aspects of manufacturing
 and sensing.
 Nowadays, every significant system collects enormous amounts of informations
 about its inner state.
 This information however is usually collected on a ad-hoc basis and does
 not fullfill the requirements that one would like in terms of coverage,
 signal-to-noise ratio.
 In other terms, its relevance to the monitoring task is indirect, unlike
 a gauge that measures the level of gas in a tank.
 For these cyberphysical systems, the reconstruction of the inner state
 has to be done by merging massive streams of data.
 We present here an application of these in the realm of civil engineering:
 the case of estimating traffic in a city.
\end_layout

\begin_layout Standard

\series bold
Arterial traffic.
 
\end_layout

\begin_layout Standard
Talk about real-time state estimation at scale.
 Here is an example in traffic.
\end_layout

\begin_layout Standard

\series bold
Data driven inference.

\series default
 Consider that we want to build some model of the traffic phenomenon.
 The ultimate goal is to use the measurements currently at hand to build
 a function that given new (unseen) data, provides some estimate of the
 state of the system, and some validation of this state.
 This function often have a fixed structure and some parameters that depend
 on this particular structure that can be tuned given some data.
 This includes some physical model where the physical constants have to
 be calibrated, some statistical model in which the statistical parameters
 have to be adjusted to best match the observations, etc.
 Another approach is to assume that all observations are points in some
 arbitratry space, and that their distribution can be described in terms
 of these data points only.
 This is the non-parametric approach, which we will not consider here.
 There are two ways to approach the problem of building a model, which relate
 into how much the existing data at hand play a role in elaborating a structure:
 a statistical approach and a first principle approach.
 These two ways are complementary.
 In the statistical approach, the data is used to prescribe a structure,
 which is then tuned using this data.
 In the first principle approach, the scientist has some additional knowledge
 about the general laws that govern the phenomenon, such as the law of physics,
 some particular assumptions about speed, people's behaviour, etc.
 Based on these laws, some general patterns can describe the phenomenon.
 For example, if cars on a highway were represented as a continuous fluid,
 then the treatment of highways through fluid dynamics will predict shockwaves,
 traffic jams around intersections, etc.
 without having seen data.
 One could argue that with enough information and computing power, one could
 in principle select a set of optimal statistical model that could predict
 (but not explain) the most minute details of the phenomenon.
 In the other hand, with the perfect knowledge of the conditions and a rigorous
 derivation that takes into account all the experimental flaws, the first
 principle approach could full predict the data collected.
 In practice, due to our ignorance of some important parts of the experiments,
 we need to make some simplifying assumptions in a model and compensate
 by inferring some constants from the data.
 Also, given a data-driven approach, our prior knowledge of the phenomenon
 will influence our choice of variable (representation problem), and give
 us some insight about the structure of dependencies between these variables.
\end_layout

\begin_layout Standard
We describe here a big problem that is driven by data.
 The classical approach is to look first for first principles and then deduce
 some facts based these principles, and then pick a problem that one can
 solve based on these facts.
\end_layout

\begin_layout Standard
Our approach will focus on extracting structure from data.
 In the case of traffic, there is a wealth of research focused on explaining
 observations based on first principles (more on that later).
 Our data exploration will be motivated by the general physical principals
 behind the phenomenon.
 However, it will not try to 
\emph on
explain
\emph default
 as much as 
\emph on
predict
\emph default
 the future or the unobserved data.
 In particural, some very strong simplifying assumptions will be made, and
 justified only by the power of the prediction they let us make.
\end_layout

\begin_layout Standard
However, we present first some exploration of the dataset that will motivate
 the subsequent models.
 The facts obtained from this analysis show that a simple approach can lead
 already to some substantial results.
 This is the first chapter.
\end_layout

\begin_layout Standard
There are different trade-offs possible to build an estimation system.
 We explore different trade offs, and we propose to solve the problem of
 traffic.
 
\end_layout

\begin_layout Standard
Such systems are only interesting in the sense that they can lead to computable
 models that perform inference at fast enough a rate.
 We present some new paradigms to perform these computations in the third
 chapter.
\end_layout

\begin_layout Standard

\series bold
Pipeline.

\series default
 In this data-intensive task, the final output looks like a complete pipeline
 separated in multiple stages, each of the stages being responsible for
 a separate task.
 Our traffic pipeline is decomposed in the steps shown in figure XXXX.
 We will focus on the step in BLUE in the pipeline.
\end_layout

\begin_layout Standard

\series bold
Diversity in GPS data.

\series default
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-intro/tt_function.pdf
	width 6cm

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:tt_function"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Schema of travel time function.
\end_layout

\end_inset

The travel time function that we desire to compute: given a start time and
 a path across a road network, we wish to provide a statistical distribution
 of arrival times.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The rest of this discussion will be decomposed as the following.
 In a first chapter, we will study how to recover travel time informations
 from GPS data.
 In today's world, there are different characteristics in the data.
 This is why we are going to explore two possible scenarios:
\end_layout

\begin_layout Itemize
A first scenario in which we have a lot of sparse GPS data
\end_layout

\begin_layout Itemize
A second scenario in which we have more high-frequency data
\end_layout

\begin_layout Standard
In both cases, we want to work at the scale of the city, and we will highlight
 the contributions towards scaling the problem to very large metropolis.
\end_layout

\begin_layout Standard
The final model will be presented in the last chapter.
 
\end_layout

\begin_layout Section
Data-driven insights
\end_layout

\begin_layout Standard
Our final objective is to give good information about travel times to user.
 We aim at a model of travel times that can give useful bounds on the travel
 times (in a statistical sense).
 This goal informs the complexity of the model that we want to build.
 In particular, we should aim at a model that can capture significant correlatio
ns to the travel times between different links, since the unit of inference
 that we choose is the link on a road network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename figures-intro/sampleplot_trajs.png
	width 5cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Temporal distribution of trajectories from a typical source of data
\end_layout

\end_inset

Temporal distribution of trajectories from a typical source of data.
 All trajectories are observed in small chunks.
 The vertical axis corresponds to the overall duration of the chunk, and
 the horizontal axis corresponds to the sampling rate of the chunk.
 Most data is programmatically generated at fixed intervals (10, 60, 90,
 120 seconds), and most of it is generated at large intervals (> 10 seconds).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Nowadays, the GPS data is collected by a number of organizations and commercial
 providers XXX.
 The most striking feature of this data is that it is very heterogeneous,
 both in a spatial sense and in a temporal sense (see Figure XXX and Figure
 XXX).
 Some small portions of the road network (the highways and the main arterials)
 concentrate most of the data.
 It is not always clear if this also means that these portions also concentrate
 most of the traffic.
 Furthermore, this GPS data is collected from various sources that were
 not designed for traffic analysis; hence they may record the location of
 a vehicle at intervals up to three minutes.
 These very low frequency collection schemes make the bulk of the data collected
 these days, and using them for accurate traffic estimation remains a challenge.
 As a conclusion, we have access to large amount of a low-frequency data,
 or to a small amount of high-frequency data.
 This is why we will present two models for traffic that correspond to these
 two scenarios.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename docs-intro/power_law.pdf
	width 6cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:power_law"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Power law distribution of GPS data.
\end_layout

\end_inset

The number of observations (GPS points projected to the road network) by
 link, in decreasing order (normalized by the length of the link), for a
 full day, from a commercial data provider.
 The first 100 elements correspond to highways, and contain 30% of all observati
ons, and have a very high observation density.
 In these situations, a fine-grained physical model is effective.
 For 90% of the links (index greater than XXX), we barely observe one vehicle
 per day (spread-out suburbs), so a simple historical models will be the
 best we can hope for.
 For the middle range, we have enough observations for a richer statistical
 model, but not enough for a finer physical model.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We studied some high-frequency data collected during an experiment in San
 Francisco (see Section XXX for more details).
 This experiment provided detailled trajectory information along stretches
 of the main arterial network in San Francisco.
 After mapping the GPS points to the road, and reconstructing the trajectories,
 we had a very precise account of the patterns of traffic on the ground.
 In particular, we reached the following conclusions for the travel times
 on each link:
\end_layout

\begin_layout Itemize
discrete number of modes.
 The travel times follow a multi-modal distribution across each block.
 There are generally two main modes that correspond to the light signal
 at the end of the intersection (see Figure XXX).
 This distribution can be predicted by simple physical models that take
 into account the dynamics of cars (CITE aude).
\end_layout

\begin_layout Itemize
lights correlation.
 The traffic lights are synchronized, the synchronization scheme being managed
 at the level of the district.
 However, this synchronization pattern is usually fairly complex: it combines
 traffic sensing equipment with written rules and random pedestrian events,
 and each district has a different set of rules.
 From the perspective of a travelling vehicle, we consider the synchronization
 of the lights a 
\emph on
discrete stochastic process
\emph default
, the parameters of which can be inferred from observing enough vehicles.
 We will see in section XXX how we can accurately model this process as
 Markov graph.
\end_layout

\begin_layout Itemize
travel time correlation inside each mode.
 It is well known that vehicles at lights tend to travel together at the
 same speed.
 Furnthermore, it is natural to consider that if a vehicle travels at constant
 speed across several green lights: there is no reason to believe that the
 driver would randomly adjust her speed as she crosses a light.
 This is why, if we were to fix a pattern of red and green lights across
 the whole network, we could assume that all the travel times are still
 correllated.
 This is referred to as inner mode correlation.
\end_layout

\begin_layout Standard
We will see how it works together.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename figures-intro/example_franklin.png
	lyxscale 10
	width 5cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename figures-intro/example_vanness.png
	lyxscale 10
	width 5cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Space time plots for selected streets in San Francisco
\end_layout

\end_inset

Sets of trajectories collected around two main arterial in San Francisco:
 northbound Franklin street on the left and northbound Van Ness avenue on
 the right.
 The distance is normalized for each othe road link by the link length.
\begin_inset Newline newline
\end_inset

Franklin street exhibits much more variability in the travel times due to
 weaker correlations between the traffic lights.
 Conversely, vehicles on Van Ness avenue are severely constrained and cannot
 drive too fast or too slow due to the scheduling of the traffic lights.
 We wish to capture this variability in travel times in our model.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:twostreets_comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Litterature review
\end_layout

\begin_layout Standard
XXX put all the litterature review here.
\end_layout

\begin_layout Section
Organization of the thesis and contributions
\end_layout

\begin_layout Standard
This thesis is organized as follows.
\end_layout

\begin_layout Standard
The first chapter introduces the problem of large-scale estimation of traffic
 and gives an overview of the data currently available for this task.
 It presents the different modelling trade-offs required for this task.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:introduction"

\end_inset

 reviews existing work on large-scale estimation of traffic.
 In particular, it presents the motivation behind the choice of the different
 models based on a data-centered approach.
 XXX to do
\end_layout

\begin_layout Standard
The first task that needs to happen is the map-matching of GPS data.
 This is a challenging issue due to the long intervals between each observation.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:pif"

\end_inset

 presents an algorithm to map-match the GPS data at a very wide range of
 latencies and for a variety of computation/accuracy trade-offs.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter formalizes the problem of reconstructing trajectories from
 low-frequency observations, and presents an algorithm, called the Path
 Inference Filter, to do that.
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter12wafr,hunter12pathinference"

\end_inset


\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:socc"

\end_inset

 considers the case of large amounts of low-latency data, which is what
 is generally available nowadays.
 In this case, the estimation problem is mostly bound by computation times,
 as we will see.
 If some further modelling assumptions are made about the independence of
 travel time distributions, we can build a model that can scale linearly
 to hundreds of machines.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel times that can scale linearly to
 massive inputs of sparse observations and very large road networks.
 It uses the Spark programming framework 
\begin_inset CommandInset citation
LatexCommand cite
key "spark"

\end_inset

 to distribute computations on a large cluster of computers.
 Our implementation can update traffic estimates from hundreds of thousands
 of observations in a few seconds.
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter2011SOCC,hunterlarge"

\end_inset

.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

 makes opposite assumptions and explores the scenario in which a small amount
 of high-frequency observations are available to the researcher.
 In this case, it is hopeless to achieve real-time traffic estimation.
 However, one can hope for a good baseline/historical model of travel times
 that takes into account the correlation of traffic between different parts
 of the road network.
 We will first motivate this model from a data analysis perspective from
 the angle of a data analysis perspective.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel time that uses privacy-aware GPS
 data, and that provides distributions of travel times for any path across
 the network.
 We also introduce some new modelization techniques to represent transitory
 phenomenon on a graph, and we present a fast inference algorithm based
 on Fast Fourrier Transform to compute these travel times.
 Our implementation can work with long paths on a large road network with
 more than half a million road links.
\end_layout

\begin_layout Standard
Finally, we will conclude in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:conclusion"

\end_inset

.
\end_layout

\end_body
\end_document
