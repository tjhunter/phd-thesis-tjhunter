#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass book
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Introduction}
\end_layout

\begin_layout Plain Layout


\backslash
epigraph{From the midst of this 
\end_layout

\begin_layout Plain Layout

darkness a sudden light broke in upon
\end_layout

\begin_layout Plain Layout

me--a light so brilliant and wondrous, 
\end_layout

\begin_layout Plain Layout

yet so simple, that while I became dizzy 
\end_layout

\begin_layout Plain Layout

with the immensity of the prospect which it 
\end_layout

\begin_layout Plain Layout

illustrated, I was surprised that among so
\end_layout

\begin_layout Plain Layout

many men of genius who had directed their
\end_layout

\begin_layout Plain Layout

inquiries towards the same science, that 
\end_layout

\begin_layout Plain Layout

I alone should be reserved to discover so 
\end_layout

\begin_layout Plain Layout

astonishing a secret.}{Mary Shelley, 
\backslash
emph{in} Frankenstein}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chapter:introduction"

\end_inset


\end_layout

\begin_layout Standard
Our society is increasingly dependent on very large infrastructure systems
 that provide vital services such as water, electricity, information, energy.
 Proeminent examples are telecommunication systems, power grids, water distribut
ion systems or roads.
 These systems are continuously monitored by a very vast array of sensors,
 which gather large streams of digital information.
 This network of sensors gives information about the aggregate state of
 the system, or about local variables such as chemical control, power output,
 temperatures, etc.
 These variables are then used to take corrective action.
 In order to process this stream of information, it will be more and more
 common in the future to use massive amounts of computing power:
\end_layout

\begin_layout Itemize
the sensing network is increasingly complex: it usually offers indirect
 measurements of the state (the sensors are either not located in the area
 of interest or measure other variables correlated with the variables of
 interest).
\end_layout

\begin_layout Itemize
due to coupling effects, estimating the state of a large system is usually
 much more difficult than estimating the states of the subsystems
\end_layout

\begin_layout Standard
Furthermore, as monitoring networks grow in complexity and versatility,
 they are usually tasked with answering more complex questions such as predictin
g the evolution of the system, or performing diagnostics on some vital sections
 (and provide some statistical guarantees about the quality of these diagnostics
).
 These tasks often require much more computing power than providing aggregated
 measurement, and yet become an increasingly important part of the monitoring
 (think of assessing the spread of toxic chemicals in a water system or
 radioactive surges in a nuclear core).
 These tasks grow in complexity and will require more and more the use of
 advanced statistical and computing techniques in order to be completed
 in a timely manner.
 Some disciplines such as weather forecasting have already harnessed the
 potential offered by cheap computing platforms.
 We discuss in this thesis the well-known challenge of vehicular traffic
 estimation, from the perspective of a large cyberphysical system.
\end_layout

\begin_layout Section
Car traffic as a cyberphysical system
\end_layout

\begin_layout Standard

\series bold
Traffic.

\series default
 Traffic congestion affects nearly everyone in the world due to the environmenta
l damage and transportation delays it causes.
 The
\begin_inset space ~
\end_inset


\begin_inset Formula $2007$
\end_inset

 Urban Mobility Report
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "tti"

\end_inset

 states that traffic congestion causes
\begin_inset space ~
\end_inset


\begin_inset Formula $4.2$
\end_inset

 billion hours of extra travel in the United States every year, which accounts
 for 2.9 billion extra gallons of fuel and an additional cost of
\begin_inset space ~
\end_inset

$78 billion.
 Amongst the modern man-made plagues, traffic congestion is a universally
 recognized challenge 
\begin_inset CommandInset citation
LatexCommand cite
key "downs2004still"

\end_inset

.
 Building reliable and cost-effective traffic monitoring systems is a prerequisi
te to addressing this phenomenon.
 
\end_layout

\begin_layout Standard
One way to alleviate the effects of traffic congestion is to provide accurate
 and timely informations to drivers.
 Providing drivers with accurate traffic information reduces the stress
 associated with congestion and allows drivers to make informed decisions,
 which generally increases the efficiency of the entire road network
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "ban_optimal_2008"

\end_inset

.
 Researchers on Traffic Information Systems (TIS) broadly agree that accurate
 information is critical to increase their usage 
\begin_inset CommandInset citation
LatexCommand cite
key "chorus2006use"

\end_inset

.
 So far however, it seems only a small fraction of the drivers uses TIS
 mostly because the value of information is percieved to be too low compared
 to the experience fo the driver in this setting 
\begin_inset CommandInset citation
LatexCommand cite
key "gao2011cognitive"

\end_inset

.
 One of the main challenges of TIS is to provide reliable and accurate informati
on to drivers, especially in terms of travel times.
 
\end_layout

\begin_layout Standard
This question is well understood in the case of highways.
 Modeling highway traffic conditions has been well-studied by the transportation
 community with work dating back to the pioneering work of Lighthill, Whitham
 and Richards
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lighthill_kinematic_1955"

\end_inset

.
 Recently, researchers demonstrated that estimating highway traffic conditions
 can be done using only GPS probe vehicle data
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "work2010traffic"

\end_inset

.
 Traffic estimation is still an open question in the case of roads within
 a city, called arterial roads.
 
\emph on
Arterial roads
\emph default
 are the major urban city streets that connect population centers within
 and between cities.
 In this setting, it is too costly to measure flows of vehicles using static
 sensors, and the most promising source of data is sensors embedded in vehicles.
 Historically, the estimation of traffic congestion has been limited to
 highways, and has relied mostly on a static, dedicated sensing infrastructure
 such as loop detectors or cameras 
\begin_inset CommandInset citation
LatexCommand cite
key "Work08"

\end_inset

.
 The estimation problem is more challenging in the case of the secondary
 road network, also called the 
\emph on
arterial network
\emph default
, due to the cost of deploying a wide network of sensors in large metropolitan
 areas.
 The most promising source of data is the GPS receiver in personal smartphones
 and commercial fleet vehicles 
\begin_inset CommandInset citation
LatexCommand cite
key "6214607"

\end_inset

.
 According to some studies 
\begin_inset CommandInset citation
LatexCommand cite
key "schrank2009"

\end_inset

, devices with a data connection and a GPS will represent 80% of the cellphone
 market by 2015.
 GPS observations in cities are noisy 
\begin_inset CommandInset citation
LatexCommand cite
key "cui2003autonomous"

\end_inset

, and are usually provided at low sampling rates (on the order of one minute)
 
\begin_inset CommandInset citation
LatexCommand cite
key "cabspotting"

\end_inset

.
 
\end_layout

\begin_layout Standard
Recent studies focusing on estimating real-time arterial traffic conditions
 have investigated traffic flow reconstruction for single intersections
 using dedicated traffic sensors.
 Dedicated traffic sensors are expensive to install, maintain and operate,
 which limits the number of sensors that governmental agencies can deploy
 on the road network.
 The lack of sensor coverage across the arterial network thus motivates
 the use of GPS probe vehicle data for estimating traffic conditions.
\end_layout

\begin_layout Standard

\series bold
Goal.

\series default
 The specific problem we address in this use case is how to extract travel
 time distributions from 
\emph on
sparse, noisy
\emph default
 GPS measurements collected 
\emph on
in real-time
\emph default
 from vehicles, and over a 
\emph on
very large network
\emph default
.
 Given any route in the road network, we want to build a function that returns
 a statistical distribution of travel times over this route (see Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tt_function"

\end_inset

).
 Commercial providers have so far focused on the easier task of attributing
 a single number for the travel time for travel, which is usually described
 as an average travel time.
 This number is usually not enough information for the end user.
 For example, consider the trip of someone to the airport.
 The user would like to know when to leave in order to be at the airport
 with some reasonable confidence, and the mean does not convey this information,
 or even worse, may be misleading the user into taking a route that has
 a smaller mean but a larger variance (the trips are usually shorter but
 they are longer, the delays are significant).
 Seasoned taxi drivers and drivers usually take their past experience into
 account, but cannot quantify their reasoning.
 Computing the full statistical distribution of travel times lets the user
 decide how much risk he or she is willing to take
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Of course, leaving an infinity of time before the event is the only strategy
 to arrive with probability one.
 We assume the driver's boyfriend would object to such a long trip to the
 airport.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-intro/tt_function.pdf
	width 8cm

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:tt_function"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Schema of travel time function.
\end_layout

\end_inset

The travel time function that we desire to compute: given a start time and
 a path across a road network, we wish to provide a statistical distribution
 of arrival times.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Source of data.
 
\series default
The integration of sensors and data, the increase in data volume, the decrease
 in costs is transforming most physical systems.
 The digital world is pervasively invading all aspects of manufacturing
 and sensing.
 Nowadays, every significant system collects enormous amounts of informations
 about its inner state.
 This information however is usually collected on a ad-hoc basis and does
 not fullfill the requirements that one would like in terms of coverage,
 signal-to-noise ratio.
 In other terms, its relevance to the monitoring task is indirect, unlike
 a gauge that measures the level of gas in a tank.
 For these cyberphysical systems, the reconstruction of the inner state
 has to be done by merging massive streams of data.
 We present here an application of these in the realm of civil engineering:
 the case of estimating traffic in a city.
\end_layout

\begin_layout Standard

\emph on
Static sensors.
 
\end_layout

\begin_layout Standard

\emph on
Probe data
\emph default
 is information collected from moving vehicles, and typically includes the
 location and speed of the vehicle.
 This type of data have been revolutionized by the introduction of the Global
 Positioning System (GPS) in the 1980s.
 The GPS can measure the location of a beacon across the the globe with
 relatively high accuracy.
 However, the last decade has brought the GPS to the masses by making it
 a standard feature in smartphones.
 GPS data collected for traffic estimation purposes comes from two different
 sources:
\end_layout

\begin_layout Itemize
Fleet data.
 A number of businesses (taxi companies, FedEx, UPS, trucks) operate large
 fleets of vehicles and optimize their services by tracking the locations
 of the vehicles.
 This source of GPS information is usually of high volume (tens of thousands
 of vehicles across the continent), low frequency (every minute or so),
 and with limited privacy concerns.
 Also, their coverage of the road network can be limited to certain areas.
\end_layout

\begin_layout Itemize
Smartphone data.
 Some global companies collect vehicular GPS data by using software installed
 in the device, like mapping software (Garmin.
 Google, INRIX, Noka, Waze, etc.) This data is believed to be of high frequency
 and to take some privacy concerns into account as part of the collection
 process.
\end_layout

\begin_layout Standard
Probe data presents some compelling advantages over static sensors: it is
 easier to upgrade (for example by pushing software upgrades to devices),
 it is cheaper to operate (the energy and transmission costs are shouldered
 by the end users) and its coverage is usually very extensive (see Figure
 XXX).
 It also presents some drawbacks: achieving reasonable privacy levels is
 an open question, the coverage is dependent on the user base and may be
 poor in some areas, and the market fragmentation means that agreements
 must be negotiated with each company.
\end_layout

\begin_layout Standard

\series bold
Pipeline.

\series default
 
\emph on
Mobile Millennium
\emph default
 infers real-time traffic conditions using GPS measurements from drivers
 running cell phone applications, taxicabs, and other mobile and static
 data sources.
 This system was initially deployed in the San Francisco Bay area and later
 expanded to other locations such as Sacramento, Stockholm, and Porto.
 Arterial roads, which , provide additional challenges for traffic estimation.
 
\emph on
Mobile Millennium
\emph default
 is intended to work at the scale of large metropolitan areas: the road
 network considered in this work is a real road network (a large portion
 of the greater Bay Area, comprising 506,685 road links) and the data for
 this work is collected from thousands of vehicles that generate millions
 of observations per day.
 As a consequence of these specifications and requirements, we employ highly
 scalable traffic algorithms.
 Furthermore, 
\emph on
Mobile Millennium 
\emph default
is a research platform and can be used with various models of travel times.
 The fundamental unit of estimation is the probability distribution of travel
 times over a single link of the road network.
 We have built upon 
\emph on
Mobile Millennium
\emph default
 to provide a statistical estimation engine.
 In this data-intensive task, the final output looks like a complete pipeline
 separated in multiple stages, each of the stages being responsible for
 a separate task.
 Our traffic pipeline is decomposed in the steps shown in figure XXXX.
 We will focus on the step in BLUE in the pipeline.
\end_layout

\begin_layout Standard

\series bold
Equilibrium.

\series default
 In this scenario, we can consider that revealing the true state of the
 traffic to the participants will not change the dynamics of the overall
 phenomenon: the informed drivers will optimize their routes or schedule
 without changing (much) the global equilibrium of the other road users
\end_layout

\begin_layout Standard
However, we present first some exploration of the dataset that will motivate
 the subsequent models.
 The facts obtained from this analysis show that a simple approach can lead
 already to some substantial results.
 This is the first chapter.
\end_layout

\begin_layout Standard
There are different trade-offs possible to build an estimation system.
 We explore different trade offs, and we propose to solve the problem of
 traffic.
 
\end_layout

\begin_layout Standard
Such systems are only interesting in the sense that they can lead to computable
 models that perform inference at fast enough a rate.
 We present some new paradigms to perform these computations in the third
 chapter.
\end_layout

\begin_layout Standard

\series bold
Diversity in GPS data.

\series default
 
\end_layout

\begin_layout Standard
The rest of this discussion will be decomposed as the following.
 In a first chapter, we will study how to recover travel time informations
 from GPS data.
 In today's world, there are different characteristics in the data.
 This is why we are going to explore two possible scenarios:
\end_layout

\begin_layout Itemize
A first scenario in which we have a lot of sparse GPS data
\end_layout

\begin_layout Itemize
A second scenario in which we have more high-frequency data
\end_layout

\begin_layout Standard
In both cases, we want to work at the scale of the city, and we will highlight
 the contributions towards scaling the problem to very large metropolis.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:mm_cloud_point"

\end_inset

 graphically presents a subset of probe data collected by 
\emph on
Mobile Millennium
\emph default
.
 In addition to these geolocalization attributes, data points contain other
 attributes such as heading, spot speed, etc.
 We will show how this additional information can be integrated in the rest
 of the framework presented in this article.
\end_layout

\begin_layout Section
Complexity of modelling traffic from GPS observations
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures-pif/example_cloud_point_sf.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
An example of dataset available to 
\emph on
Mobile Millennium
\emph default
 and processed by the path inference filter
\end_layout

\end_inset

An example of dataset available to 
\emph on
Mobile Millennium
\emph default
 and processed by the path inference filter: taxicabs in San Francisco from
 the Cabspotting program 
\begin_inset CommandInset citation
LatexCommand cite
key "cabspotting"

\end_inset

.
 Large circles in red show the position of the taxis at a given time and
 small dots (in black) show past positions (during the last five hours)
 of the fleet.
 The position of each vehicle is observed every minute.
\begin_inset CommandInset label
LatexCommand label
name "fig:mm_cloud_point"

\end_inset

 XXX put in introduction
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our final objective is to give good information about travel times to user.
 We aim at a model of travel times that can give useful bounds on the travel
 times (in a statistical sense).
 This goal informs the complexity of the model that we want to build.
 In particular, we should aim at a model that can capture significant correlatio
ns to the travel times between different links, since the unit of inference
 that we choose is the link on a road network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename figures-intro/sampleplot_trajs.png
	width 8cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Temporal distribution of trajectories from a typical source of data
\end_layout

\end_inset

Temporal distribution of trajectories from a typical source of data.
 All trajectories are observed in small chunks.
 The vertical axis corresponds to the overall duration of the chunk, and
 the horizontal axis corresponds to the sampling rate of the chunk.
 Most data is programmatically generated at fixed intervals (10, 60, 90,
 120 seconds), and most of it is generated at large intervals (> 10 seconds).
 Scale is ommitted for confidentiality reasons.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Nowadays, the GPS data is collected by a number of organizations and commercial
 providers XXX.
 The most striking feature of this data is that it is very heterogeneous,
 both in a spatial sense and in a temporal sense (see Figure XXX and Figure
 XXX).
 Some small portions of the road network (the highways and the main arterials)
 concentrate most of the data.
 It is not always clear if this also means that these portions also concentrate
 most of the traffic.
 Furthermore, this GPS data is collected from various sources that were
 not designed for traffic analysis; hence they may record the location of
 a vehicle at intervals up to three minutes.
 These very low frequency collection schemes make the bulk of the data collected
 these days, and using them for accurate traffic estimation remains a challenge.
 As a conclusion, we have access to large amount of a low-frequency data,
 or to a small amount of high-frequency data.
 This is why we will present two models for traffic that correspond to these
 two scenarios.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename docs-intro/power_law.pdf
	width 9cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:power_law"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Power law distribution of GPS data.
\end_layout

\end_inset

The number of observations (GPS points projected to the road network) by
 link, in decreasing order (normalized by the length of the link), for a
 full day, from a commercial data provider.
 The first 100 elements correspond to highways, and contain 30% of all observati
ons, and have a very high observation density.
 In these situations, a fine-grained physical model is effective.
 For 90% of the links (index greater than XXX), we barely observe one vehicle
 per day (spread-out suburbs), so a simple historical models will be the
 best we can hope for.
 For the middle range, we have enough observations for a richer statistical
 model, but not enough for a finer physical model.
 The exact scale is ommitted for confidentiality reasons.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Data driven inference.

\series default
 Consider that we want to build some model of the traffic phenomenon.
 The ultimate goal is to use the measurements currently at hand to build
 a function that given new (unseen) data, provides some estimate of the
 state of the system, and some validation of this state.
 This function often have a fixed structure and some parameters that depend
 on this particular structure that can be tuned given some data.
 This includes some physical model where the physical constants have to
 be calibrated, some statistical model in which the statistical parameters
 have to be adjusted to best match the observations, etc.
 Another approach is to assume that all observations are points in some
 arbitratry space, and that their distribution can be described in terms
 of these data points only.
 This is the non-parametric approach, which we will not consider here.
 There are two ways to approach the problem of building a model, which relate
 into how much the existing data at hand play a role in elaborating a structure:
 a statistical approach and a first principle approach.
 These two ways are complementary.
 In the statistical approach, the data is used to prescribe a structure,
 which is then tuned using this data.
 In the first principle approach, the scientist has some additional knowledge
 about the general laws that govern the phenomenon, such as the law of physics,
 some particular assumptions about speed, people's behaviour, etc.
 Based on these laws, some general patterns can describe the phenomenon.
 For example, if cars on a highway were represented as a continuous fluid,
 then the treatment of highways through fluid dynamics will predict shockwaves,
 traffic jams around intersections, etc.
 without having seen data.
 One could argue that with enough information and computing power, one could
 in principle select a set of optimal statistical model that could predict
 (but not explain) the most minute details of the phenomenon.
 In the other hand, with the perfect knowledge of the conditions and a rigorous
 derivation that takes into account all the experimental flaws, the first
 principle approach could full predict the data collected.
 In practice, due to our ignorance of some important parts of the experiments,
 we need to make some simplifying assumptions in a model and compensate
 by inferring some constants from the data.
 Also, given a data-driven approach, our prior knowledge of the phenomenon
 will influence our choice of variable (representation problem), and give
 us some insight about the structure of dependencies between these variables.
\end_layout

\begin_layout Standard
We describe here a big problem that is driven by data.
 The classical approach is to look first for first principles and then deduce
 some facts based these principles, and then pick a problem that one can
 solve based on these facts.
\end_layout

\begin_layout Standard
Our approach will focus on extracting structure from data.
 In the case of traffic, there is a wealth of research focused on explaining
 observations based on first principles (more on that later).
 Our data exploration will be motivated by the general physical principals
 behind the phenomenon.
 However, it will not try to 
\emph on
explain
\emph default
 as much as 
\emph on
predict
\emph default
 the future or the unobserved data.
 In particural, some very strong simplifying assumptions will be made, and
 justified only by the power of the prediction they let us make.
\end_layout

\begin_layout Section
Organization of the thesis and contributions
\end_layout

\begin_layout Standard
This thesis is organized as follows.
\end_layout

\begin_layout Standard
The first chapter introduces the problem of large-scale estimation of traffic
 and gives an overview of the data currently available for this task.
 It presents the different modelling trade-offs required for this task.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:introduction"

\end_inset

 reviews existing work on large-scale estimation of traffic.
 In particular, it presents the motivation behind the choice of the different
 models based on a data-centered approach.
 XXX to do
\end_layout

\begin_layout Standard
The first task that needs to happen is the map-matching of GPS data.
 This is a challenging issue due to the long intervals between each observation.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:pif"

\end_inset

 presents an algorithm to map-match the GPS data at a very wide range of
 latencies and for a variety of computation/accuracy trade-offs.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter formalizes the problem of reconstructing trajectories from
 low-frequency observations, and presents an algorithm, called the Path
 Inference Filter, to do that.
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter12wafr,hunter12pathinference"

\end_inset


\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:socc"

\end_inset

 considers the case of large amounts of low-latency data, which is what
 is generally available nowadays.
 In this case, the estimation problem is mostly bound by computation times,
 as we will see.
 If some further modelling assumptions are made about the independence of
 travel time distributions, we can build a model that can scale linearly
 to hundreds of machines.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel times that can scale linearly to
 massive inputs of sparse observations and very large road networks.
 It uses the Spark programming framework 
\begin_inset CommandInset citation
LatexCommand cite
key "spark"

\end_inset

 to distribute computations on a large cluster of computers.
 Our implementation can update traffic estimates from hundreds of thousands
 of observations in a few seconds.
 This EM algorithm is the core of an estimation pipeline deployed inside
 the 
\emph on
Mobile Millennium
\emph default
 traffic information system 
\begin_inset CommandInset citation
LatexCommand cite
key "mm,mm-socc"

\end_inset

.
 This engine gathers GPS observations from participating vehicles and produces
 estimates of the travel times on the road network.As we will see, our framework
 can accommodate 
\emph on
any 
\emph default
distribution of travel times that provided they expose a few functionalities
 (sampling, parameter estimation from observations).
 This should be of interest to the traffic researchers and practitioners
 since our framework solves all the issues of using raw GPS samples to build
 traffic estimates at a very large scale with low latency.
 A probabilistic model of travel times on the arterial network is presented
 along with an online 
\emph on
Expectation Maximization
\emph default
 (EM) algorithm for learning the parameters of this model (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:model"

\end_inset

).
 The algorithm is expensive due to the large dimension of the network and
 the complexity inherent to the evolution of traffic.
 Furthermore, our EM algorithm has no closed-form expression and requires
 sampling and non-linear optimization techniques.
 This is why the use of a distributed system is appropriate.
 
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter2011SOCC,hunterlarge"

\end_inset

.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

 makes opposite assumptions and explores the scenario in which a small amount
 of high-frequency observations are available to the researcher.
 In this case, it is hopeless to achieve real-time traffic estimation.
 However, one can hope for a good baseline/historical model of travel times
 that takes into account the correlation of traffic between different parts
 of the road network.
 We will first motivate this model from a data analysis perspective from
 the angle of a data analysis perspective.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel time that uses privacy-aware GPS
 data, and that provides distributions of travel times for any path across
 the network.
 We also introduce some new modelization techniques to represent transitory
 phenomenon on a graph, and we present a fast inference algorithm based
 on Fast Fourrier Transform to compute these travel times.
 Our implementation can work with long paths on a large road network with
 more than half a million road links.
\end_layout

\begin_layout Standard
Finally, we will conclude in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:conclusion"

\end_inset

.
\end_layout

\end_body
\end_document
