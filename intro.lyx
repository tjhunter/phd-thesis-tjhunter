#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass book
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Introduction}
\end_layout

\begin_layout Plain Layout


\backslash
epigraph{From the midst of this 
\end_layout

\begin_layout Plain Layout

darkness a sudden light broke in upon
\end_layout

\begin_layout Plain Layout

me--a light so brilliant and wondrous, 
\end_layout

\begin_layout Plain Layout

yet so simple, that while I became dizzy 
\end_layout

\begin_layout Plain Layout

with the immensity of the prospect which it 
\end_layout

\begin_layout Plain Layout

illustrated, I was surprised that among so
\end_layout

\begin_layout Plain Layout

many men of genius who had directed their
\end_layout

\begin_layout Plain Layout

inquiries towards the same science, that 
\end_layout

\begin_layout Plain Layout

I alone should be reserved to discover so 
\end_layout

\begin_layout Plain Layout

astonishing a secret.}{Mary Shelley, 
\backslash
emph{Frankenstein}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chapter:introduction"

\end_inset


\end_layout

\begin_layout Standard
Our society is increasingly dependent on very large infrastructure systems
 that provide vital services such as water, electricity, information, or
 energy.
 Prominent examples are telecommunication systems, power grids, water distributi
on or transportation systems.
 These systems are continuously monitored by a vast array of sensors, which
 gather large streams of digital information.
 This network of sensors gives information about the aggregate state of
 the system, or about local variables such as chemical concentrations, power
 output, temperatures, etc.
 This step is commonly referred to as state estimation or filtering.
 These variables are then used to provide corrective feedback to the system
 (control).
 In order to process this stream of information, it will be more and more
 common in the future to use massive amounts of computing power for the
 following reasons:
\end_layout

\begin_layout Itemize
the sensing network is increasingly complex: it usually offers indirect
 measurements of the state (the sensors are either not located in the area
 of interest or measure other variables correlated with the variables of
 interest).
\end_layout

\begin_layout Itemize
due to coupling effects, estimating the state of a large system is usually
 much more difficult than estimating the states of the subsystems
\end_layout

\begin_layout Standard
Furthermore, as monitoring networks grow in complexity and versatility,
 they are usually tasked with answering more complex questions such as predictin
g the evolution of the system or performing diagnostics on critical sections
 (and also providing some statistical guarantees about the quality of these
 diagnostics).
 These tasks often require much more computing power than aggregating measuremen
ts, and yet become an increasingly important part of the monitoring.
 Examples include assessing the spread of toxic chemicals in a water system
 or detecting radioactive surges in a nuclear core.
 These tasks grow in complexity and will require more and more the use of
 advanced statistical and computing techniques in order to be completed
 in a timely manner.
 Some disciplines such as weather forecasting have already harnessed the
 potential offered by cost-effective computing platforms.
 We discuss in this thesis the well-known challenge of vehicular traffic
 estimation, from the perspective of a large cyberphysical system.
\end_layout

\begin_layout Section
Car traffic as a cyberphysical system
\end_layout

\begin_layout Standard

\series bold
Traffic.

\series default
 Traffic congestion affects nearly everyone in the world due to the environmenta
l damage and transportation delays it causes.
 The
\begin_inset space ~
\end_inset


\begin_inset Formula $2012$
\end_inset

 Urban Mobility Report
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "schrank2012tti"

\end_inset

 states that traffic congestion causes
\begin_inset space ~
\end_inset


\begin_inset Formula $5.5$
\end_inset

 billion hours of extra travel in the United States in 2011, which accounted
 for 2.9 billion extra gallons of fuel and an additional cost of
\begin_inset space ~
\end_inset

$121 billion.
 Amongst the modern man-made plagues, traffic congestion is a universally
 recognized challenge 
\begin_inset CommandInset citation
LatexCommand cite
key "downs2004still"

\end_inset

.
 Building reliable and cost-effective traffic monitoring systems is a prerequisi
te to addressing this phenomenon.
 
\end_layout

\begin_layout Standard
One way to alleviate the effects of traffic congestion is to provide accurate
 and timely information to drivers.
 Providing drivers with accurate traffic information reduces the stress
 associated with congestion and allows drivers to make informed decisions,
 which generally increases the efficiency of the entire road network
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "ban_optimal_2008"

\end_inset

.
 Researchers on Traffic Information Systems (TIS) broadly agree that accurate
 information is critical to increase their usage 
\begin_inset CommandInset citation
LatexCommand cite
key "chorus2006use"

\end_inset

.
 One of the main challenges of TIS is to provide reliable, accurate and
 timely travel information to drivers, and especially travel time information.
\end_layout

\begin_layout Standard
The question of estimating traffic and providing accurate travel information
 is well understood in the case of highways.
 Modeling highway traffic conditions has been well-studied by the transportation
 community, with work dating back to the pioneering work of Lighthill, Whitham
 and Richards
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lighthill_kinematic_1955"

\end_inset

.
 Historically, the estimation of traffic congestion has been limited to
 highways, and has relied mostly on a static, dedicated sensing infrastructure
 such as loop detectors or cameras 
\begin_inset CommandInset citation
LatexCommand cite
key "Work08"

\end_inset

.
 Recently, researchers demonstrated that estimating highway traffic conditions
 can be done using only GPS probe vehicle data
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "work2010traffic"

\end_inset

.
 Traffic estimation is still an open question in the case of roads within
 a city, called 
\emph on
arterial roads
\emph default
.
 Arterial roads
\emph on
 
\emph default
are the major urban city streets that connect population centers within
 and between cities.
 The estimation problem is more challenging in the case of arterial roads,
 due to the higher complexity of city traffic, and to the cost of deploying
 a wide network of sensors in large metropolitan areas.
 Dedicated traffic sensors are expensive to install, maintain and operate,
 which limits the number of sensors that governmental agencies can deploy
 on the road network.
 The lack of sensor coverage across the arterial network thus motivates
 the use of GPS probe vehicle data for estimating traffic conditions.
\end_layout

\begin_layout Standard

\series bold
Goal.

\series default
 The specific problem we address in this study is how to extract travel
 time distributions from 
\emph on
sparse, noisy
\emph default
 GPS measurements collected 
\emph on
in real-time
\emph default
 from vehicles, and over a 
\emph on
very large network
\emph default
.
 We wish to provide a function that, given any route in the road network,
 returns statistical information of travel times over this route (see Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tt_function"

\end_inset

).
 Commercial providers 
\begin_inset CommandInset citation
LatexCommand cite
key "navteq,telenav,inrix"

\end_inset

 have so far focused on the easier task of assigning a single number for
 the travel time on a road, which is usually understood as an average travel
 time
\begin_inset Foot
status open

\begin_layout Plain Layout
For various legal reasons, these products purposefully omit any labeling
 that could be interpreted as a statistical guarantee on the information
 provided to drivers
\end_layout

\end_inset

.
 This number is usually not enough information for the end user.
 For example, consider a trip to a nearby airport.
 A passenger would like to know when to leave her
\begin_inset Foot
status open

\begin_layout Plain Layout
We follow the economics convention to address the casual driver with a female
 gender.
 See 
\begin_inset CommandInset citation
LatexCommand cite
key "sen2004rationality"

\end_inset

 for a more complete discussion.
\end_layout

\end_inset

 current location in order to be at the airport with some reasonable confidence,
 and the mean does not convey this information, or even worse, may be misleading
 the user into taking a route that has a smaller mean but a larger variance
 (i.e., the trips are usually shorter but they are longer, the delays are
 significant).
 Seasoned taxi drivers take their past experience into account to develop
 their routing strategies, but this knowledge is hard to quantify and transfer
 to casual drivers.
 Furthermore, computing the full statistical distribution of travel times
 lets the user decide how much risk he or she is willing to take
\begin_inset Foot
status open

\begin_layout Plain Layout
Of course, leaving an infinity of time before the event is the only strategy
 to arrive with probability one.
 We assume drivers would object to such a long trip to the airport.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-intro/tt_function.pdf
	width 12cm

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:tt_function"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Schema of travel time function.
\end_layout

\end_inset

The travel time function that we desire to compute: given a start time and
 a path across a road network, we wish to provide a statistical distribution
 of arrival times (represented here as a cumulative distribution).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Source of data.
 
\series default
\emph on
Probe data
\emph default
 is information collected from moving vehicles, and typically includes the
 location and speed of the vehicle.
 This type of data have been revolutionized by the introduction of the Global
 Positioning System (GPS) in the 1980s.
 The most promising source of data is the GPS receiver in personal smartphones
 and commercial fleet vehicles 
\begin_inset CommandInset citation
LatexCommand cite
key "6214607"

\end_inset

.
 The GPS can measure the location of a receiver across the the globe with
 relatively high accuracy.
 The last decade has brought the GPS to the masses by making it a standard
 feature in smartphones.
 According to some studies 
\begin_inset CommandInset citation
LatexCommand cite
key "schrank2009"

\end_inset

, devices with a data connection and a GPS will represent 80% of the cellphone
 market by 2015.
 GPS data collected for traffic estimation purposes comes from two different
 sources:
\end_layout

\begin_layout Itemize
Fleet data.
 A number of businesses (taxi companies, FedEx, UPS, trucks) operate large
 fleets of vehicles and optimize their services by tracking the locations
 of the vehicles.
 This source of GPS information is usually of high volume (tens of thousands
 of vehicles across the continent), low frequency (every minute or so),
 and with limited privacy concerns.
 Also, their coverage of the road network can be limited to certain areas.
\end_layout

\begin_layout Itemize
Smartphone data.
 Some global companies collect vehicular GPS data by using software installed
 in the device, like mapping software (Garmin.
 Google, INRIX, Nokia, Waze, etc.) This data is believed to be of high frequency
 and to be more or less scrubbed from personal information during the collection
 process
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
For example, as of February 2014, the privacy policy of Google maps states:
 
\begin_inset Quotes eld
\end_inset

We you use [Google Maps], we may collect and process information about your
 actual location, like GPS signal sent by a mobile device.
 We may also use various technologies to determine location, such as [WiFi
 and cell tower information]
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

.
 However, this data is seldom accessible to researchers due to privacy and
 competitivity concerns.
\end_layout

\begin_layout Standard
Probe data presents some compelling advantages over static sensors: it is
 easier to upgrade (for example by pushing software upgrades to devices),
 it is cheaper to operate (the energy and transmission costs are shouldered
 by the end users) and its coverage is usually very extensive (see Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:mm_cloud_point"

\end_inset

 for an example).
 It also presents some drawbacks: achieving reasonable levels of privacy
 is an open question, the coverage is dependent on the user base and may
 be poor in some areas, and the market fragmentation means that agreements
 must be negotiated with each company that collects GPS data (often gathered
 from different users).
\end_layout

\begin_layout Standard

\series bold
Pipeline.

\series default
 All the algorithms and techniques we are about to present have been implemented
 and deployed within the 
\emph on
Mobile Millennium
\emph default
 traffic platform.
 
\emph on
Mobile Millennium
\emph default
 infers real-time traffic conditions using GPS measurements from drivers
 running cell phone applications, taxicabs, and other mobile and static
 data sources.
 This system was initially deployed in the San Francisco Bay area and later
 expanded to other locations such as Sacramento, Stockholm, and Porto.
 
\emph on
Mobile Millennium
\emph default
 is intended to work at the scale of large metropolitan areas: the road
 network considered in this work is a real road network (a large portion
 of the greater Bay Area, comprising 506,685 road links) and the data for
 this work is collected from thousands of vehicles that generate millions
 of observations per day.
 Furthermore, 
\emph on
Mobile Millennium 
\emph default
is a research platform and can be used with various models of travel times.
 We have built upon 
\emph on
Mobile Millennium
\emph default
 to provide a statistical estimation engine for travel time, and expanding
 it was a significant contribution of our work.
 In this data-intensive task, the final output looks like a complete pipeline
 separated in multiple stages, each of the stages being responsible for
 a separate task.
 Our traffic pipeline is decomposed in the following steps 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Maybe add the picture here XXX
\end_layout

\begin_layout Plain Layout
shown in Figure XXXX
\end_layout

\end_inset

:
\end_layout

\begin_layout Itemize
An ingestion module that takes all the feeds and stores them into a database,
\end_layout

\begin_layout Itemize
A path inference module that reconstructs trajectories from GPS points,
\end_layout

\begin_layout Itemize
Various travel time modules that compute travel time estimates on the road
 network,
\end_layout

\begin_layout Itemize
A validation module that implements different metrics of travel times.
\end_layout

\begin_layout Standard
Most of this code is available under an open source license, and the output
 has been deployed as a web service
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://traffic.berkeley.edu/navigatesf
\end_layout

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Scaling/Spark.

\series default
 While a lot of research has focused on a few road intersections, we have
 decided to work from the outset at the scale of large city.
 This adds multiple scalability challenges: (1) the data to process is substanti
ally larger, and (2) estimation algorithms need to scale to millions of
 variables, which is a hard challenge in Machine Learning in general.
 As a consequence, we investigate the use of innovating cloud computing
 frameworks that let scientific users deploy code across hundreds of computers
 in a transparent way.
 We have implemented some of our algorithms on top of the Spark computing
 framework 
\begin_inset CommandInset citation
LatexCommand cite
key "spark"

\end_inset

.
 After some modifications to Spark, we were able to achieve massive throughput
 with low-latency (near real-time), an elusive yet necessary goal for complex
 cyberphysical systems.
 
\end_layout

\begin_layout Standard

\series bold
Equilibrium.

\series default
 In this scenario, we can consider that revealing the true state of the
 traffic to the participants will not change the dynamics of the overall
 phenomenon: the informed drivers will optimize their routes or schedule
 without changing (much) the global equilibrium of the other road users.
 
\end_layout

\begin_layout Standard

\series bold
Existing systems.

\series default
 Up to now, few researchers have attempted to tackle the full problem and
 to consider at the same time the problem of probe data (real data), scalability
 and accuracy.
 This approach has been mostly developed in the industry, which has developed
 solutions that scale to the whole world and can ingest thousands of observation
s per second.
 However, there has been no published attempt to quantify the quality of
 the solutions offered by the industry.
 The terms of use generally limit scientific investigation of the output
 provided to the public.
 The engineering challenges involved in building a reliable system for mass
 collection of GPS data as well as the difficulty in enlisting enough users
 to reach a critical mass pose formidable challenges to researchers.
 Furthermore, companies that collect such data are loathe to share it with
 researchers due to obvious legal and privacy concerns.
 The only successful examples of full platforms developed in academia that
 we are aware of are Cartel 
\begin_inset CommandInset citation
LatexCommand cite
key "hull2006cartel"

\end_inset

 and 
\emph on
Mobile Millennium
\emph default
.
 We hope that this dissertation will provide some insight as to the problems
 that are relevant to researchers in the field.
\end_layout

\begin_layout Section
Complexity of modeling traffic from GPS observations
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures-pif/example_cloud_point_sf.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
An example of dataset available to 
\emph on
Mobile Millennium
\emph default
 and processed by the path inference filter
\end_layout

\end_inset

An example of dataset available to 
\emph on
Mobile Millennium
\emph default
 and processed by the path inference filter: taxicabs in San Francisco from
 the Cabspotting program 
\begin_inset CommandInset citation
LatexCommand cite
key "cabspotting"

\end_inset

.
 Large circles in red show the position of the taxis at a given time and
 small dots (in black) show past positions (during the last five hours)
 of the taxi fleet.
 The position of each vehicle is observed every minute.
\begin_inset CommandInset label
LatexCommand label
name "fig:mm_cloud_point"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
What sort of GPS information can be processed by the 
\emph on
Mobile Millennium
\emph default
 system? A datum of GPS information contains a time stamp, a unique identifier
 for the driver or the trip, a location encoded by latitude and longitude,
 and often some other elements such as heading, speed, status (empty, en
 route, etc.).
 These additional attributes were usually found to be too noisy to be used
 in a systematic fashion, and were not used.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:mm_cloud_point"

\end_inset

 graphically presents a subset of this probe data collected by 
\emph on
Mobile Millennium
\emph default
.
\end_layout

\begin_layout Standard
Our final objective is to give good information about travel times to user.
 We aim at a model of travel times that can give useful bounds on the travel
 times (in a statistical sense).
 This goal informs the complexity of the model that we want to build.
 In particular, we should aim at a model that can capture significant correlatio
ns to the travel times between different links, since the unit of inference
 that we choose is the link on a road network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename figures-intro/sampleplot_trajs.png
	width 8cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Temporal distribution of trajectories from a typical source of data
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Temporal-distribution-trajectories"

\end_inset

Temporal distribution of trajectories from a typical source of data.
 All trajectories are observed in small chunks.
 The vertical axis corresponds to the overall duration of the chunk, and
 the horizontal axis corresponds to the sampling rate of the chunk.
 Most data is programmatically generated at fixed intervals (10, 60, 90,
 120 seconds), and most of it is generated at large intervals (> 10 seconds).
 Scale is omitted for confidentiality reasons.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Temporal disparity.

\series default
 Existing GPS data is very heterogeneous, both in a spatial sense and in
 a temporal sense (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Temporal-distribution-trajectories"

\end_inset

 and Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:power_law"

\end_inset

).
 Some small portions of the road network (the highways and the main arterials)
 concentrate most of the data.
 It is not always clear if this also means that these portions also concentrate
 most of the traffic.
 This GPS data is collected from various sources that were not designed
 for traffic analysis; hence they may record the location of a vehicle at
 intervals up to three minutes.
 These very low frequency collection schemes make the bulk of the data collected
 these days, and using them for accurate traffic estimation remains a challenge.
 Furthermore, in the case of individual drivers, in order to limit privacy
 intrusion and to save battery life, it is common to only record the location
 of the vehicle during short intervals (a few minutes to an hour).
 There is no control over when the GPS unit will record the trip.
 As a conclusion, we have access to large amount of a low-frequency data,
 or to a small amount of high-frequency data.
 This is why we will present two models for traffic that correspond to these
 two typical scenarios.
 
\end_layout

\begin_layout Itemize
A first scenario in which we have a lot of sparse GPS data
\end_layout

\begin_layout Itemize
A second scenario in which we have more high-frequency data
\end_layout

\begin_layout Standard
In both cases, we want to work at the scale of the city, and we will highlight
 the contributions towards scaling the problem to very large metropolis.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-intro/power_law.pdf
	width 9cm

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:power_law"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Power law distribution of GPS data.
\end_layout

\end_inset

The number of observations (GPS points projected to the road network) by
 link, in decreasing order (normalized by the length of the link), for a
 full day, from a commercial data provider.
 The exact scale is shifted for confidentiality reasons.
 The first 100 elements correspond to highways.
 They contain 30% of all observations and have a very high observation density.
 In these situations, a fine-grained physical model is effective.
 For 90% of the links (index greater than 
\begin_inset Formula $10^{3}$
\end_inset

), we barely observe a few vehicles per day on each road (spread-out suburbs),
 so a simple historical models will be the best we can hope for.
 For the middle range, we have enough observations for a richer statistical
 model, but not enough for a finer physical model.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Spatial disparity.

\series default
 It is clear to any driver that all the roads do not bear the same traffic.
 As one can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:power_law"

\end_inset

, this difference is very large (by more than four orders of magnitude).
 Highways bear the brunt of heavy traffic and are used almost continuously
 day and night.
 Most of the roads however are part of the tertiary network in the suburban
 areas and are used a few times each day or less.
 These roads seldom present any congestion, but may have some busy intersections
 (such as around schools) that are hard to predict 
\emph on
a priori\SpecialChar \@.
 
\emph default
Between these two extreme lies a rich variety of road configurations with
 one, two or three lanes, stop signs, signaled lights, bus and bicycle lanes,
 etc.
 Any model that aims at a reasonable level of accuracy must (1) be robust
 to a scarcity of data and/or noisy data, and (2) not depend too much on
 prior knowledge of the road network as it is always shifting or inaccurate.
 Bayesian statistical models provide a compelling solution to address these
 two issues.
\end_layout

\begin_layout Standard

\series bold
Data driven inference.

\series default
 The ultimate goal is to use available measurements to build a function
 that given new (unseen) data, provides some estimate of the state of the
 system, and some validation of this state.
 This function will be a estimator.
 It often a fixed structure and some parameters that depend on this particular
 structure that can be tuned given available data.
 For example, this function could be a physical model where the physical
 constants have to be calibrated, or a statistical model in which the statistica
l parameters have to be adjusted to best match the observations, etc.
 Another approach is to assume that all observations are points in some
 arbitrary space, and that their distribution can be described in terms
 of these data points only.
 This is the non-parametric approach, which we will not consider here.
 There are two ways to approach the problem of building a model, which relate
 into how much the existing data at hand play a role in elaborating a structure:
 a statistical approach and an approach based on first principles.
 These two ways are complementary.
 In the statistical approach, the data is used to prescribe a structure,
 which is then tuned using this data.
 In the first principle approach, the scientist has some additional knowledge
 about the general laws that govern the phenomenon, such as the law of physics,
 some particular assumptions about speed, people's behavior, etc.
 Based on these laws, some general patterns can describe the phenomenon.
 For example, if cars on a highway were represented as a continuous fluid,
 then the treatment of highways through fluid dynamics will predict shock
 waves, traffic jams around intersections, etc.
 without having seen data.
 One could argue that with enough information and computing power, one could
 in principle select a set of optimal statistical model that could predict
 (but not explain) the most minute details of the phenomenon.
 In the other hand, with the perfect knowledge of the conditions and a rigorous
 derivation that takes into account all the experimental biases, the first
 principle approach could full predict the data collected.
 In practice, due to our ignorance of some important parts of the experiment,
 we need to make some simplifying assumptions in a model and compensate
 by inferring some constants from collected observations.
 Also, given a data-driven approach, our prior knowledge of the phenomenon
 will influence our choice of variables (representation problem), and give
 us some insight about the structure of dependencies between these variables.
\end_layout

\begin_layout Standard
Our approach will focus on extracting structure from data.
 In the case of traffic, there is a wealth of research focused on explaining
 observations based on first principles (more on that later).
 Our data exploration will be motivated by the general physical principals
 behind the phenomenon.
 However, it will not try to 
\emph on
explain
\emph default
 as much as 
\emph on
predict
\emph default
 the future or the unobserved data.
 In particular, some very strong simplifying assumptions will be made, and
 justified only by the accuracy of the prediction they let us make.
 These assumptions are justified only in the light that they let us make
 reasonable 
\emph on
predictions
\emph default
 using limited computing resources.
\end_layout

\begin_layout Section
Organization of the thesis and contributions
\end_layout

\begin_layout Standard
This thesis is organized as follows.
 This first chapter introduced the problem of estimating traffic at scale
 and gave an overview of the data currently available for this task.
 The following chapters present a framework for extracting travel information
 from GPS data.
\end_layout

\begin_layout Standard
As we saw, the GPS data is very noisy and needs to be filtered and map-matched
 onto the map before any use.
 This is a challenging issue due to the long intervals between each observation.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:pif"

\end_inset

 presents an algorithm to map-match the GPS data at a very wide range of
 latencies and for a variety of computation/accuracy trade-offs.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter formalizes the problem of reconstructing trajectories from
 high to low-frequency observations, and presents an algorithm, called the
 Path Inference Filter, to do that.
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter12wafr,hunter12pathinference"

\end_inset


\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:socc"

\end_inset

 considers the case of large amounts of low-latency data, which is what
 is generally available nowadays.
 In this case, the estimation problem is mostly bound by computation times.
 If some further independence assumptions are made about travel time distributio
ns, we can build a model that can scale to hundreds of machines.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel times that can scale linearly to
 massive inputs of sparse observations and very large road networks.
 It uses the Spark programming framework 
\begin_inset CommandInset citation
LatexCommand cite
key "spark"

\end_inset

 to distribute computations on a large cluster of computers.
 Our implementation can update traffic estimates from hundreds of thousands
 of observations in a few seconds.
 This algorithm is the core of an estimation pipeline deployed inside 
\emph on
Mobile Millennium
\emph default
.
 This engine gathers GPS observations from participating vehicles and produces
 estimates of the travel times on the road network.
 As we will see, our framework can accommodate 
\emph on
any 
\emph default
distribution of travel times that provided they expose a few functionalities
 (sampling, parameter estimation from observations).
 This should be of interest to the traffic researchers and practitioners
 since our framework solves all the issues of using raw GPS samples to build
 traffic estimates at a very large scale with low latency.
 A probabilistic model of travel times on the arterial network is presented
 along with an online 
\emph on
Expectation Maximization
\emph default
 (EM) algorithm for learning the parameters of this model (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:model"

\end_inset

).
 
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter2011SOCC,hunterlarge"

\end_inset

.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

 makes different assumptions and explores the scenario in which a small
 amount of high-frequency observations are available to the researcher.
 In this case, it is hopeless to achieve real-time traffic estimation.
 However, one can hope for a good baseline/historical model of travel times
 that takes into account the correlation of traffic between different parts
 of the road network.
 We will first motivate this model by a careful look at the available data.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel time that uses privacy-aware GPS
 data, and that provides distributions of travel times for any path across
 the network.
 We also introduce some new modelization techniques to represent transitory
 phenomenon on a graph, and we present a fast inference algorithm based
 on Fast Fourier Transform to compute these travel times.
 Our implementation can work with long paths on a large road network with
 more than half a million road links.
\end_layout

\begin_layout Standard
All the models presented so far have a reasonable size (less than a million
 variables), for which inference and learning algorithms can be derived
 by adapting traditional techniques.
 As the size of the problems at hand increases to truly enormous dimensions,
 some radically different techniques may be required.
 Learning the Gaussian model presented in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

 requires evaluating the determinant of a very large matrix, which is a
 significant problem in linear algebra for very large matrices.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:detsdd"

\end_inset

 presents a new algorithm that can compute the determinant of a class of
 matrices (diagonally dominant matrices) in 
\emph on
expected near-linear time
\emph default
, which is a significant progress compared to the traditional intuition
 that the computation time for the determinant is cubic with respect to
 the dimension.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a number of algorithm for approximating the logarithm
 of the determinant of real, symmetric, diagonally dominant matrices.
 More specifically, given a matrix 
\begin_inset Formula $A$
\end_inset

 of size 
\begin_inset Formula $n\times n$
\end_inset

 with 
\begin_inset Formula $m$
\end_inset

 non-zero coefficients, an 
\begin_inset Formula $\epsilon-$
\end_inset

approximation of 
\begin_inset Formula $n^{-1}\log\det A$
\end_inset

 can be computed with high probability in expected time 
\begin_inset Newline linebreak
\end_inset


\begin_inset Formula $O\left(m\epsilon^{-2}\log^{3}n\log\left(\frac{\kappa_{A}}{\epsilon}\right)\log\log^{8}n\right)$
\end_inset

, where 
\begin_inset Formula $\kappa_{A}$
\end_inset

 is the condition number of 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard

\bar under
Publication:
\bar default
 This part is under submission at the SIAM Journal of Matrix Analysis and
 Applications.
\end_layout

\begin_layout Standard
Finally, we will conclude in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:conclusion"

\end_inset

.
\end_layout

\end_body
\end_document
