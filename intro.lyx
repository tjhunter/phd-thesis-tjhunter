#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass book
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Introduction}
\end_layout

\begin_layout Plain Layout


\backslash
epigraph{From the midst of this 
\end_layout

\begin_layout Plain Layout

darkness a sudden light broke in upon
\end_layout

\begin_layout Plain Layout

me--a light so brilliant and wondrous, 
\end_layout

\begin_layout Plain Layout

yet so simple, that while I became dizzy 
\end_layout

\begin_layout Plain Layout

with the immensity of the prospect which it 
\end_layout

\begin_layout Plain Layout

illustrated, I was surprised that among so
\end_layout

\begin_layout Plain Layout

many men of genius who had directed their
\end_layout

\begin_layout Plain Layout

inquiries towards the same science, that 
\end_layout

\begin_layout Plain Layout

I alone should be reserved to discover so 
\end_layout

\begin_layout Plain Layout

astonishing a secret.}{Mary Shelley, 
\backslash
emph{in} Frankenstein}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chapter:introduction"

\end_inset


\end_layout

\begin_layout Standard
Our society is increasingly dependent on very large infrastructure systems
 that provide vital services such as water, electricity, information, energy.
 Proeminent examples are telecommunication systems, power grids, water distribut
ion systems or roads.
 These systems are continuously monitored by a very vast array of sensors,
 which gather large streams of digital information.
 This network of sensors gives information about the aggregate state of
 the system, or about local variables such as chemical control, power output,
 temperatures, etc.
 These variables are then used to take corrective action.
 In order to process this stream of information, it will be more and more
 common in the future to use massive amounts of computing power:
\end_layout

\begin_layout Itemize
the sensing network is increasingly complex: it usually offers indirect
 measurements of the state (the sensors are either not located in the area
 of interest or measure other variables correlated with the variables of
 interest).
\end_layout

\begin_layout Itemize
due to coupling effects, estimating the state of a large system is usually
 much more difficult than estimating the states of the subsystems
\end_layout

\begin_layout Standard
Furthermore, as monitoring networks grow in complexity and versatility,
 they are usually tasked with answering more complex questions such as predictin
g the evolution of the system, or performing diagnostics on some vital sections
 (and provide some statistical guarantees about the quality of these diagnostics
).
 These tasks often require much more computing power than providing aggregated
 measurement, and yet become an increasingly important part of the monitoring
 (think of assessing the spread of toxic chemicals in a water system or
 radioactive surges in a nuclear core).
 These tasks grow in complexity and will require more and more the use of
 advanced statistical and computing techniques in order to be completed
 in a timely manner.
 Some disciplines such as weather forecasting have already harnessed the
 potential offered by cheap computing platforms.
 We discuss in this thesis the well-known challenge of vehicular traffic
 estimation, from the perspective of a large cyberphysical system.
\end_layout

\begin_layout Section
Car traffic as a cyberphysical system
\end_layout

\begin_layout Standard

\series bold
Traffic.

\series default
 Traffic congestion affects nearly everyone in the world due to the environmenta
l damage and transportation delays it causes.
 The
\begin_inset space ~
\end_inset


\begin_inset Formula $2007$
\end_inset

 Urban Mobility Report
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "tti"

\end_inset

 states that traffic congestion causes
\begin_inset space ~
\end_inset


\begin_inset Formula $4.2$
\end_inset

 billion hours of extra travel in the United States every year, which accounts
 for 2.9 billion extra gallons of fuel and an additional cost of
\begin_inset space ~
\end_inset

$78 billion.
 Amongst the modern man-made plagues, traffic congestion is a universally
 recognized challenge 
\begin_inset CommandInset citation
LatexCommand cite
key "downs2004still"

\end_inset

.
 Building reliable and cost-effective traffic monitoring systems is a prerequisi
te to addressing this phenomenon.
 
\end_layout

\begin_layout Standard
One way to alleviate the effects of traffic congestion is to provide accurate
 and timely informations to drivers.
 Providing drivers with accurate traffic information reduces the stress
 associated with congestion and allows drivers to make informed decisions,
 which generally increases the efficiency of the entire road network
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "ban_optimal_2008"

\end_inset

.
 Researchers on Traffic Information Systems (TIS) broadly agree that accurate
 information is critical to increase their usage 
\begin_inset CommandInset citation
LatexCommand cite
key "chorus2006use"

\end_inset

.
 So far however, it seems only a small fraction of the drivers uses TIS
 mostly because the value of additional information is percieved as low
 compared to the experience fo the driver 
\begin_inset CommandInset citation
LatexCommand cite
key "gao2011cognitive"

\end_inset

.
 One of the main challenges of TIS is to provide reliable, accurate and
 timely travel information to drivers, especially travel time information.
\end_layout

\begin_layout Standard
The question of estimating traffic and providing accurate travel information
 is well understood in the case of highways.
 Modeling highway traffic conditions has been well-studied by the transportation
 community with work dating back to the pioneering work of Lighthill, Whitham
 and Richards
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "lighthill_kinematic_1955"

\end_inset

.
 Historically, the estimation of traffic congestion has been limited to
 highways, and has relied mostly on a static, dedicated sensing infrastructure
 such as loop detectors or cameras 
\begin_inset CommandInset citation
LatexCommand cite
key "Work08"

\end_inset

.
 Recently, researchers demonstrated that estimating highway traffic conditions
 can be done using only GPS probe vehicle data
\begin_inset space ~
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "work2010traffic"

\end_inset

.
 Traffic estimation is still an open question in the case of roads within
 a city, called 
\emph on
arterial roads
\emph default
.
 Arterial roads
\emph on
 
\emph default
are the major urban city streets that connect population centers within
 and between cities.
 The estimation problem is more challenging in the case of arterial roads
 due to the cost of deploying a wide network of sensors in large metropolitan
 areas.
 Recent studies focusing on estimating real-time arterial traffic conditions
 have investigated traffic flow reconstruction for single intersections
 using dedicated traffic sensors.
 Dedicated traffic sensors are expensive to install, maintain and operate,
 which limits the number of sensors that governmental agencies can deploy
 on the road network.
 The lack of sensor coverage across the arterial network thus motivates
 the use of GPS probe vehicle data for estimating traffic conditions.
\end_layout

\begin_layout Standard

\series bold
Goal.

\series default
 The specific problem we address in this use case is how to extract travel
 time distributions from 
\emph on
sparse, noisy
\emph default
 GPS measurements collected 
\emph on
in real-time
\emph default
 from vehicles, and over a 
\emph on
very large network
\emph default
.
 We wish to provide a function that, given any route in the road network,
 returns statistical information of travel times over this route (see Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:tt_function"

\end_inset

).
 Commercial providers (CITE google, navteq, telenav, inrix) have so far
 focused on the easier task of assigning a single number for the travel
 time on a road, which is usually understood as an average travel time
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
All these products carefully omit any labelling that could be interpreted
 as a statistical guarantee on the information provided to drivers
\end_layout

\end_inset

.
 This number is usually not enough information for the end user.
 For example, consider a trip to the airport.
 A passenger would like to know when to leave her
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We follow the economics convention to address the casual driver with a female
 gender.
\end_layout

\end_inset

 current location in order to be at the airport with some reasonable confidence,
 and the mean does not convey this information, or even worse, may be misleading
 the user into taking a route that has a smaller mean but a larger variance
 (the trips are usually shorter but they are longer, the delays are significant).
 Seasoned taxi drivers usually take their past experience into account to
 develop their routing strategies, but this intuitive behavior is hard to
 quantify and extrapolate to another driver.
 Furthermore, computing the full statistical distribution of travel times
 lets the user decide how much risk he or she is willing to take
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Of course, leaving an infinity of time before the event is the only strategy
 to arrive with probability one.
 We assume the driver's boyfriend would object to such a long trip to the
 airport.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-intro/tt_function.pdf
	width 8cm

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:tt_function"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Schema of travel time function.
\end_layout

\end_inset

The travel time function that we desire to compute: given a start time and
 a path across a road network, we wish to provide a statistical distribution
 of arrival times (represented here as a cumulative distribution).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Source of data.
 
\series default
\emph on
Probe data
\emph default
 is information collected from moving vehicles, and typically includes the
 location and speed of the vehicle.
 This type of data have been revolutionized by the introduction of the Global
 Positioning System (GPS) in the 1980s.
 The most promising source of data is the GPS receiver in personal smartphones
 and commercial fleet vehicles 
\begin_inset CommandInset citation
LatexCommand cite
key "6214607"

\end_inset

.
 According to some studies 
\begin_inset CommandInset citation
LatexCommand cite
key "schrank2009"

\end_inset

, devices with a data connection and a GPS will represent 80% of the cellphone
 market by 2015.
 GPS observations in cities are noisy 
\begin_inset CommandInset citation
LatexCommand cite
key "cui2003autonomous"

\end_inset

, and are usually provided at low sampling rates (on the order of one minute)
 
\begin_inset CommandInset citation
LatexCommand cite
key "cabspotting"

\end_inset

.
 The GPS can measure the location of a beacon across the the globe with
 relatively high accuracy.
 However, the last decade has brought the GPS to the masses by making it
 a standard feature in smartphones.
 GPS data collected for traffic estimation purposes comes from two different
 sources:
\end_layout

\begin_layout Itemize
Fleet data.
 A number of businesses (taxi companies, FedEx, UPS, trucks) operate large
 fleets of vehicles and optimize their services by tracking the locations
 of the vehicles.
 This source of GPS information is usually of high volume (tens of thousands
 of vehicles across the continent), low frequency (every minute or so),
 and with limited privacy concerns.
 Also, their coverage of the road network can be limited to certain areas.
\end_layout

\begin_layout Itemize
Smartphone data.
 Some global companies collect vehicular GPS data by using software installed
 in the device, like mapping software (Garmin.
 Google, INRIX, Noka, Waze, etc.) This data is believed to be of high frequency
 and to be more or less scrubbed from personal information during the collection
 process
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
For example, as of February 2014, the privacy policy of Google maps states:
 
\begin_inset Quotes eld
\end_inset

We you use [Google Maps], we may collect and process information about your
 actual location, like GPS signal sent by a mobile device.
 We may also use various technologies to determine location, such as [WiFi
 and cell tower information]
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Probe data presents some compelling advantages over static sensors: it is
 easier to upgrade (for example by pushing software upgrades to devices),
 it is cheaper to operate (the energy and transmission costs are shouldered
 by the end users) and its coverage is usually very extensive (see Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:mm_cloud_point"

\end_inset

).
 It also presents some drawbacks: achieving reasonable levels of privacy
 is an open question, the coverage is dependent on the user base and may
 be poor in some areas, and the market fragmentation means that agreements
 must be negotiated with each company that collects GPS data (often from
 different users).
\end_layout

\begin_layout Standard

\series bold
Pipeline.

\series default
 All the algorithms and techniques we are about to discuss have been implemented
 and deployed within the 
\emph on
Mobile Millennium
\emph default
 traffic platform.
 
\emph on
Mobile Millennium
\emph default
 infers real-time traffic conditions using GPS measurements from drivers
 running cell phone applications, taxicabs, and other mobile and static
 data sources.
 This system was initially deployed in the San Francisco Bay area and later
 expanded to other locations such as Sacramento, Stockholm, and Porto.
 
\emph on
Mobile Millennium
\emph default
 is intended to work at the scale of large metropolitan areas: the road
 network considered in this work is a real road network (a large portion
 of the greater Bay Area, comprising 506,685 road links) and the data for
 this work is collected from thousands of vehicles that generate millions
 of observations per day.
 Furthermore, 
\emph on
Mobile Millennium 
\emph default
is a research platform and can be used with various models of travel times.
 We have built upon 
\emph on
Mobile Millennium
\emph default
 to provide a statistical estimation engine for travel time, and expanding
 it was a significant contribution of our work.
 In this data-intensive task, the final output looks like a complete pipeline
 separated in multiple stages, each of the stages being responsible for
 a separate task.
 Our traffic pipeline is decomposed in the following steps 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Maybe add the picture here XXX
\end_layout

\begin_layout Plain Layout
shown in Figure XXXX
\end_layout

\end_inset

:
\end_layout

\begin_layout Itemize
An ingestion module that takes all the feeds and stores them into a DB
\end_layout

\begin_layout Itemize
A path inference module that reconstructs trajectories from GPS points
\end_layout

\begin_layout Itemize
Various travel time modules that compute travel time estimates on the road
 network
\end_layout

\begin_layout Itemize
A validation module that implements different metrics of travel times
\end_layout

\begin_layout Standard
Most of this code is available under an open source licence, and the output
 has been deployed as a web service
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://traffic.berkeley.edu/navigatesf
\end_layout

\end_inset


\end_layout

\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Scaling/Spark.

\series default
 While a lot of research has focused on a few road intersection, we have
 decided to work from the outset at the scale of large city.
 This adds multiple scalability challenges (1) the data to process is substantia
lly larger, and (2) estimation algorithms need to scale gracefully to millions
 of variables, which is a hard challenge in Machine Learning in general.
 As a consequence, we investigate the use of innovating cloud computing
 frameworks that let scientific users deploy code across hundreds of computers
 in a transparent way.
 We have implemented some of our algorithms on top of the Spark computing
 framework
\begin_inset CommandInset citation
LatexCommand cite
key "spark"

\end_inset

.
 After some modifications to Spark, we were able to achieve massive throughput
 with low-latency (near real-time), an elusive yet necessary goal for complex
 cyberphysical systems.
 
\end_layout

\begin_layout Standard

\series bold
Equilibrium.

\series default
 In this scenario, we can consider that revealing the true state of the
 traffic to the participants will not change the dynamics of the overall
 phenomenon: the informed drivers will optimize their routes or schedule
 without changing (much) the global equilibrium of the other road users.
 
\end_layout

\begin_layout Standard

\series bold
Existing systems.

\series default
 Up to now, few researchers have attempted to tackle the full problem and
 to consider at the same time the problem of probe data (real data), scalability
 and accuracy.
 This approach has been mostly developped in the industry, which has developped
 solutions that scale to the whole world and can ingest thousands of observation
s per second.
 However, there has been no published attempt to quantify the quality of
 the solutions offered by the industry.
 The terms of use generally limit scientific investigation of the output
 provided to the public.
 The engineering challenges involved in building a reliable system for mass
 collection of GPS data as well as the difficulty in enlisting enough users
 to reach a critical mass pose formidable challenges to researchers.
 Furthermore, companies that collect such data are loathe to share it with
 researchers for obvious legal and privacy reasons.
 The only successful examples of full platforms devellopped in academia
 that we are aware of are Cartel and 
\emph on
Mobile Millennium
\emph default
.
 We hope that this dissertation will provide some insight as to the problems
 that are relevant to researchers in the field.
\end_layout

\begin_layout Section
Complexity of modelling traffic from GPS observations
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures-pif/example_cloud_point_sf.png
	lyxscale 30
	width 70col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
An example of dataset available to 
\emph on
Mobile Millennium
\emph default
 and processed by the path inference filter
\end_layout

\end_inset

An example of dataset available to 
\emph on
Mobile Millennium
\emph default
 and processed by the path inference filter: taxicabs in San Francisco from
 the Cabspotting program 
\begin_inset CommandInset citation
LatexCommand cite
key "cabspotting"

\end_inset

.
 Large circles in red show the position of the taxis at a given time and
 small dots (in black) show past positions (during the last five hours)
 of the taxi fleet.
 The position of each vehicle is observed every minute.
\begin_inset CommandInset label
LatexCommand label
name "fig:mm_cloud_point"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
What sort of GPS information can be processed by the 
\emph on
Mobile Millennium
\emph default
 system? A datum of GPS information contains a time stamp, a unique identifier
 for the driver or the trip, a location encoded by latitude and longitude,
 and often some other elements such as heading, speed, status (empty, en
 route, etc.).
 These additional informations were usually found to be too noisy to be
 used in a systematic fashion, and were discarded.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:mm_cloud_point"

\end_inset

 graphically presents a subset of this probe data collected by 
\emph on
Mobile Millennium
\emph default
.
\end_layout

\begin_layout Standard
Our final objective is to give good information about travel times to user.
 We aim at a model of travel times that can give useful bounds on the travel
 times (in a statistical sense).
 This goal informs the complexity of the model that we want to build.
 In particular, we should aim at a model that can capture significant correlatio
ns to the travel times between different links, since the unit of inference
 that we choose is the link on a road network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename figures-intro/sampleplot_trajs.png
	width 8cm

\end_inset


\begin_inset space \hfill{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Temporal distribution of trajectories from a typical source of data
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Temporal-distribution-trajectories"

\end_inset

Temporal distribution of trajectories from a typical source of data.
 All trajectories are observed in small chunks.
 The vertical axis corresponds to the overall duration of the chunk, and
 the horizontal axis corresponds to the sampling rate of the chunk.
 Most data is programmatically generated at fixed intervals (10, 60, 90,
 120 seconds), and most of it is generated at large intervals (> 10 seconds).
 Scale is ommitted for confidentiality reasons.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Temporal disparity.

\series default
 The most striking feature of this data is that it is very heterogeneous,
 both in a spatial sense and in a temporal sense (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Temporal-distribution-trajectories"

\end_inset

 and Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:power_law"

\end_inset

).
 Some small portions of the road network (the highways and the main arterials)
 concentrate most of the data.
 It is not always clear if this also means that these portions also concentrate
 most of the traffic.
 Furthermore, this GPS data is collected from various sources that were
 not designed for traffic analysis; hence they may record the location of
 a vehicle at intervals up to three minutes.
 These very low frequency collection schemes make the bulk of the data collected
 these days, and using them for accurate traffic estimation remains a challenge.
 Furthermore, in the case of individual drivers, in order to limit privacy
 intrusion and to save battery life, it is common to only record the location
 of the vehicle during short intervals (a few minutes to an hour).
 There is no control over when the GPS unit will record the trip.
 As a conclusion, we have access to large amount of a low-frequency data,
 or to a small amount of high-frequency data.
 This is why we will present two models for traffic that correspond to these
 two typical scenarios.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename docs-intro/power_law.pdf
	width 9cm

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:power_law"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Argument
status open

\begin_layout Plain Layout
Power law distribution of GPS data.
\end_layout

\end_inset

The number of observations (GPS points projected to the road network) by
 link, in decreasing order (normalized by the length of the link), for a
 full day, from a commercial data provider.
 The exact scale is shifted for confidentiality reasons.
 The first 100 elements correspond to highways.
 They contain 30% of all observations and have a very high observation density.
 In these situations, a fine-grained physical model is effective.
 For 90% of the links (index greater than 
\begin_inset Formula $10^{3}$
\end_inset

), we barely observe a few vehicles per day on each road (spread-out suburbs),
 so a simple historical models will be the best we can hope for.
 For the middle range, we have enough observations for a richer statistical
 model, but not enough for a finer physical model.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The rest of this discussion will be decomposed as the following.
 In a first chapter, we will study how to recover travel time informations
 from GPS data.
 In today's world, there are different characteristics in the data.
 This is why we are going to explore two possible scenarios:
\end_layout

\begin_layout Itemize
A first scenario in which we have a lot of sparse GPS data
\end_layout

\begin_layout Itemize
A second scenario in which we have more high-frequency data
\end_layout

\begin_layout Standard
In both cases, we want to work at the scale of the city, and we will highlight
 the contributions towards scaling the problem to very large metropolis.
 
\end_layout

\begin_layout Standard

\series bold
Spatial disparity.

\series default
 It is clear to any driver that all the roads do not bear the same traffic.
 As one can see on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:power_law"

\end_inset

, this diffence is very large (by more than four orders of magnitude).
 Highways bear the brunt of heavy traffic and are used almost continuously
 throughout the day.
 Most of the roads however are part of the tertiary network in the suburban
 areas and are used a few times each day or less.
 These roads seldom present any congestion, but may have some busy intersections
 (such as around schools) that are hard to predict 
\emph on
a priori\SpecialChar \@.
 
\emph default
Between these two extreme lies a rich variety of road configurations with
 one, two or three lanes, stop signs, signalled lights, bus and bicycle
 lanes, etc.
 Any model that aims at a reasonable level of accurarcy must (1) be robust
 to a scarcity of data and/or noisy data, and (2) not depend too much on
 prior knowledge of the road network as it is always shifting and inaccurate.
 Bayesion statistical models provide a compelling solution to address these
 two issues.
\end_layout

\begin_layout Standard

\series bold
Data driven inference.

\series default
 The ultimate goal is to use available measurements to build a function
 that given new (unseen) data, provides some estimate of the state of the
 system, and some validation of this state.
 This function will be a estimator.
 It often a fixed structure and some parameters that depend on this particular
 structure that can be tuned given available data.
 For example, this function could be a physical model where the physical
 constants have to be calibrated, or a statistical model in which the statistica
l parameters have to be adjusted to best match the observations, etc.
 Another approach is to assume that all observations are points in some
 arbitratry space, and that their distribution can be described in terms
 of these data points only.
 This is the non-parametric approach, which we will not consider here.
 There are two ways to approach the problem of building a model, which relate
 into how much the existing data at hand play a role in elaborating a structure:
 a statistical approach and an approach based on first principles.
 These two ways are complementary.
 In the statistical approach, the data is used to prescribe a structure,
 which is then tuned using this data.
 In the first principle approach, the scientist has some additional knowledge
 about the general laws that govern the phenomenon, such as the law of physics,
 some particular assumptions about speed, people's behaviour, etc.
 Based on these laws, some general patterns can describe the phenomenon.
 For example, if cars on a highway were represented as a continuous fluid,
 then the treatment of highways through fluid dynamics will predict shockwaves,
 traffic jams around intersections, etc.
 without having seen data.
 One could argue that with enough information and computing power, one could
 in principle select a set of optimal statistical model that could predict
 (but not explain) the most minute details of the phenomenon.
 In the other hand, with the perfect knowledge of the conditions and a rigorous
 derivation that takes into account all the experimental biases, the first
 principle approach could full predict the data collected.
 In practice, due to our ignorance of some important parts of the experiment,
 we need to make some simplifying assumptions in a model and compensate
 by inferring some constants from collected observations.
 Also, given a data-driven approach, our prior knowledge of the phenomenon
 will influence our choice of variables (representation problem), and give
 us some insight about the structure of dependencies between these variables.
\end_layout

\begin_layout Standard
Our approach will focus on extracting structure from data.
 In the case of traffic, there is a wealth of research focused on explaining
 observations based on first principles (more on that later).
 Our data exploration will be motivated by the general physical principals
 behind the phenomenon.
 However, it will not try to 
\emph on
explain
\emph default
 as much as 
\emph on
predict
\emph default
 the future or the unobserved data.
 In particural, some very strong simplifying assumptions will be made, and
 justified only by the power of the prediction they let us make.
 These assumptions are justified only in the light that they let us make
 reasonable 
\emph on
predictions
\emph default
 using limited computing resources.
\end_layout

\begin_layout Section
Organization of the thesis and contributions
\end_layout

\begin_layout Standard
This thesis is organized as follows.
 This first chapter introduced the problem estimation of traffic at scale
 and gave an overview of the data currently available for this task.
 The following chapters present a framework for extracting travel information
 from GPS data.
\end_layout

\begin_layout Standard
As we saw, the GPS data is very noisy and needs to be filtered and map-matched
 onto the map before any use.
 This is a challenging issue due to the long intervals between each observation.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:pif"

\end_inset

 presents an algorithm to map-match the GPS data at a very wide range of
 latencies and for a variety of computation/accuracy trade-offs.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter formalizes the problem of reconstructing trajectories from
 high- to low-frequency observations, and presents an algorithm, called
 the Path Inference Filter, to do that.
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter12wafr,hunter12pathinference"

\end_inset


\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:socc"

\end_inset

 considers the case of large amounts of low-latency data, which is what
 is generally available nowadays.
 In this case, the estimation problem is mostly bound by computation times.
 If some further modelling assumptions are made about the independence of
 travel time distributions, we can build a model that can scale to hundreds
 of machines.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel times that can scale linearly to
 massive inputs of sparse observations and very large road networks.
 It uses the Spark programming framework 
\begin_inset CommandInset citation
LatexCommand cite
key "spark"

\end_inset

 to distribute computations on a large cluster of computers.
 Our implementation can update traffic estimates from hundreds of thousands
 of observations in a few seconds.
 This EM algorithm is the core of an estimation pipeline deployed inside
 the 
\emph on
Mobile Millennium
\emph default
 traffic information system 
\begin_inset CommandInset citation
LatexCommand cite
key "mm,mm-socc"

\end_inset

.
 This engine gathers GPS observations from participating vehicles and produces
 estimates of the travel times on the road network.As we will see, our framework
 can accommodate 
\emph on
any 
\emph default
distribution of travel times that provided they expose a few functionalities
 (sampling, parameter estimation from observations).
 This should be of interest to the traffic researchers and practitioners
 since our framework solves all the issues of using raw GPS samples to build
 traffic estimates at a very large scale with low latency.
 A probabilistic model of travel times on the arterial network is presented
 along with an online 
\emph on
Expectation Maximization
\emph default
 (EM) algorithm for learning the parameters of this model (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:model"

\end_inset

).
 The algorithm is expensive due to the large dimension of the network and
 the complexity inherent to the evolution of traffic.
 Furthermore, our EM algorithm has no closed-form expression and requires
 sampling and non-linear optimization techniques.
 This is why the use of a distributed system is appropriate.
 
\end_layout

\begin_layout Standard

\bar under
Publications:
\bar default
 
\begin_inset CommandInset citation
LatexCommand cite
key "hunter2011SOCC,hunterlarge"

\end_inset

.
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

 makes opposite assumptions and explores the scenario in which a small amount
 of high-frequency observations are available to the researcher.
 In this case, it is hopeless to achieve real-time traffic estimation.
 However, one can hope for a good baseline/historical model of travel times
 that takes into account the correlation of traffic between different parts
 of the road network.
 We will first motivate this model from a data analysis perspective from
 the angle of a data analysis perspective.
\end_layout

\begin_layout Standard

\bar under
Contribution:
\bar default
 The chapter presents a model for travel time that uses privacy-aware GPS
 data, and that provides distributions of travel times for any path across
 the network.
 We also introduce some new modelization techniques to represent transitory
 phenomenon on a graph, and we present a fast inference algorithm based
 on Fast Fourrier Transform to compute these travel times.
 Our implementation can work with long paths on a large road network with
 more than half a million road links.
\end_layout

\begin_layout Standard
Finally, we will conclude in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:conclusion"

\end_inset

.
\end_layout

\end_body
\end_document
