#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
chapter{Computing determinants at very large scale}
\end_layout

\begin_layout Plain Layout


\backslash
epigraph{
\backslash
emph{Le faux se r
\backslash
'{e}pand vite, et le vrai surnage lentement.}}{George Sand, 
\backslash
emph{C
\backslash
'{e}sarine Dietrich}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chapter:detsdd"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
So far, our analysis has focused on the practical problem of estimating
 large, graph-based statistical models.
 We now consider the case of 
\emph on
extremely 
\emph default
large models with millions or billions of random variables.
 At such a scale, any operation has to be completed in a time that is at
 most linear with the structure of the problem.
 Any algorithm that takes longer will be hopelessly slow at such a scale.
 Such problems are not common (yet) in civil engineering applications, but
 they begin to crop up in other disciplines as we will see.
 We investigate how novel results in graph theory can inform the design
 of low-complexity algorithms for machine learning.
 In this chapter, we present some algorithms which present some appealing
 theoretical guarantees, but for which there is no known implementation.
 We present these results with the hope that they will motivate further
 research into practical algorithms.
\end_layout

\begin_layout Standard
One of the simplest models to represent interactions between random variables
 is the multivariate Gaussian model, also called Gaussian Markov Random
 Field, that we used in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chapter:kdd"

\end_inset

.
 Consider a Gaussian distribution 
\begin_inset Formula $X\sim\mathcal{N}\left(\mu,\Sigma\right)$
\end_inset

 where 
\begin_inset Formula $\mu$
\end_inset

 is the mean and 
\begin_inset Formula $\Sigma$
\end_inset

 is the covariance matrix.
 It is more convenient to consider the precision matrix 
\begin_inset Formula $S=\Sigma^{-1}$
\end_inset

.
 In most application, the precision matrix is sparse, the sparsity pattern
 representing condition dependencies between the variables 
\begin_inset CommandInset citation
LatexCommand cite
key "wainwright2008graphical"

\end_inset

.
 The log-likelihood of one observation 
\begin_inset Formula $x$
\end_inset

 is the sum of a log-determinant and a scalar product:
\begin_inset Formula 
\[
\log\pi\left(x\right)=\frac{1}{2}\log\det S-\frac{1}{2}\left(x-\mu\right)^{T}S\left(x-\mu\right)-\frac{n}{2}\log\left(2\pi\right)
\]

\end_inset

 Tuning the parameters of this model against a set of observations requires
 computing the determinant of the precision matrix, and this computation
 is in general the bottleneck.
 Instead of considering the general problem of computing the determinant
 of symmetric, positive-definite matrices, we focus on a more restricted
 case: computing the determinant of symmetric, diagonally dominant (SDD)
 matrices, i.e.
 real symmetric matrices 
\begin_inset Formula $A$
\end_inset

 for which: 
\begin_inset Formula 
\[
A_{ii}\geq\sum_{j\neq i}\left|A_{ij}\right|
\]

\end_inset

The set of all such matrices of size 
\begin_inset Formula $n\times n$
\end_inset

 is denoted 
\begin_inset Formula $SDD_{n}$
\end_inset

, and the set of all symmetric real matrices is called 
\begin_inset Formula $\mathcal{S}_{n}$
\end_inset

.
 Call 
\begin_inset Formula $m$
\end_inset

 the number of non-zero entries in 
\begin_inset Formula $A$
\end_inset

.
 We are interested in computing the determinant of sparse matrices, i.e.
 matrices for which 
\begin_inset Formula $m\ll n^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The best exact algorithm known for computing the determinant of general
 matrices, the Cholesky factorization, runs in a cubic complexity 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

.
 Computing the factorization can be sped up for a few specific patterns
 such as trees, but no algorithm has been shown to work in a generic way
 for 
\begin_inset Formula $SDD_{n}$
\end_inset

, let alone general symmetric matrices.
 We present an algorithm that returns an approximation of the logarithm
 of the determinant in time quasi-linear with the number of non-zero entries
 of 
\begin_inset Formula $A$
\end_inset

.
 More specifically, we show that our algorithm, 
\family typewriter
UltraLogDet
\family default
, computes an 
\begin_inset Formula $\epsilon$
\end_inset

-approximation of the logarithm of the determinant with high probability
 and in expected time
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We use the notation 
\begin_inset Formula $\tilde{O}$
\end_inset

 to hide a factor at most 
\begin_inset Formula $\left(\log\log n\right)^{8}$
\end_inset


\end_layout

\end_inset

: 
\begin_inset Formula 
\[
\tilde{O}\left(m\epsilon^{-2}\log^{3}n\log^{2}\left(\frac{n\kappa_{A}}{\epsilon}\right)\right)
\]

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
left(B
\backslash
right)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\kappa_{A}$
\end_inset

 is the condition number of 
\begin_inset Formula $A$
\end_inset

.
 This algorithm builds upon the work of Spielmann and Teng on 
\emph on
ultra-sparsifiers
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009a"

\end_inset

, and it critically exploits the recent improvements from Koutis, Miller
 and Peng 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

.
 This is to our knowledge the first algorithm that presents a nearly linear
 complexity which depends neither on the condition number of 
\begin_inset Formula $A$
\end_inset

 (except through a log-term) nor on a specific pattern for the non-zero
 coefficients of 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
The sophistication of the algorithm transpires through the large exponent
 of 
\begin_inset Formula $\log\log n$
\end_inset

.
 However, our algorithm will directly benefit from any improvement on ultra-spar
sifiers.
 Given the considerable practical importance of such preconditioners, we
 expect some fast improvements in this area.
 Also, the bulk of the work is performed in a Monte Carlo procedure that
 is straightforward to parallelize.
 Furthermore, we also present simpler, non-optimal algorithms that compute
 upper and lower bounds of the logarithm of the determinant, and that may
 be of more immediate practical interest.
\end_layout

\begin_layout Subsection
Background
\end_layout

\begin_layout Standard
There are two approaches in numerical linear algebra to approximately compute
 a determinant (or the log of the determinant): by performing a (partial)
 Cholesky factorization of 
\begin_inset Formula $A$
\end_inset

, or by considering the trace of some power series.
\end_layout

\begin_layout Standard
As mentioned above, the Cholesky factorization performs a decomposition
 of the form: 
\begin_inset Formula $A=PLDL^{T}P^{T}$
\end_inset

 with 
\begin_inset Formula $P$
\end_inset

 a permutation matrix, 
\begin_inset Formula $L$
\end_inset

 a low-triangular matrix with 
\begin_inset Formula $1$
\end_inset

 on the diagonal and 
\begin_inset Formula $D$
\end_inset

 a diagonal matrix of non-negative coefficients.
 Then the log-determinant of 
\begin_inset Formula $A$
\end_inset

 is simply
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We will use the 
\begin_inset Formula $\left|\cdot\right|$
\end_inset

 operator to denote the determinant, it will be clear from the context that
 it is different from the absolute value.
\end_layout

\end_inset

: 
\begin_inset Formula 
\[
\log\left|A\right|=\sum_{i}\log D_{ii}
\]

\end_inset

The complexity of dense Cholesky factorization for dense matrices is 
\begin_inset Formula $\mathcal{O}\left(n^{3}\right)$
\end_inset

.
 Unfortunately, Cholesky factorization usually does not gain much from the
 knowledge of the sparsity pattern due to the 
\emph on
fill-in problem
\emph default
 (see 
\begin_inset CommandInset citation
LatexCommand cite
key "meurant1999computer"

\end_inset

, section 3.2).
 There is one case, though, for which Cholesky factorization is efficient:
 if the sparsity pattern of 
\begin_inset Formula $A$
\end_inset

 is a tree, then performing Cholesky factorization takes 
\begin_inset Formula $\mathcal{O}\left(n\right)$
\end_inset

 time, and the matrix 
\begin_inset Formula $L$
\end_inset

 is a banded matrix 
\begin_inset CommandInset citation
LatexCommand cite
key "liu1990eliminationtrees"

\end_inset

.
 If the sparsity pattern of 
\begin_inset Formula $A$
\end_inset

 is not a tree, however, this advantageous decomposition does not hold anymore.
\end_layout

\begin_layout Standard
When the matrix 
\begin_inset Formula $A$
\end_inset

 is close to the identity, more precisely when the spectral radius of 
\begin_inset Formula $M=A-I$
\end_inset

 is less than
\begin_inset space ~
\end_inset


\begin_inset Formula $1$
\end_inset

, one can use the remarkable Martin expansion of the log-determinant 
\begin_inset CommandInset citation
LatexCommand cite
key "martin1992approximations"

\end_inset

: 
\begin_inset Formula 
\begin{equation}
\log\left|A\right|=\text{Tr}\left(\log A\right)\label{eq:martin-expansion}
\end{equation}

\end_inset

where 
\begin_inset Formula $\log A$
\end_inset

 is the matrix logarithm defined by the series expansion: 
\begin_inset Formula 
\begin{equation}
\log A=\sum_{i=0}^{\infty}\frac{\left(-1\right)^{i}}{i+1}M^{i}\label{eq:matrix-log}
\end{equation}

\end_inset

The determinant can then be computed by a sum of traces of the power of
 
\begin_inset Formula $M$
\end_inset

, and the rate of convergence of this series is driven by the spectral radius
 
\begin_inset Formula $M$
\end_inset

.
 This line of reasoning has led researchers to look for decompositions of
 
\begin_inset Formula $A$
\end_inset

 of the form 
\begin_inset Formula $A=U+V$
\end_inset

 with the determinant of 
\begin_inset Formula $U$
\end_inset

 being easier to compute and 
\begin_inset Formula $U^{-1}V$
\end_inset

 having a small spectral radius (less than 
\begin_inset Formula $1$
\end_inset

).
 Then 
\begin_inset Formula $\log\left|A\right|=\log\left|U\right|+\log\left|U^{-1}V+I\right|$
\end_inset

.
 The most common decomposition 
\begin_inset Formula $U,V$
\end_inset

 is in terms of block diagonal and off-diagonal terms, which can then use
 Hadamard inequalities on the determinant to bound the error 
\begin_inset CommandInset citation
LatexCommand cite
key "Ipsen2006"

\end_inset

.
 Diagonal blocks also have the advantage of having determinants easy to
 compute.
 However, this approach requires some strong assumptions on the condition
 number of 
\begin_inset Formula $A$
\end_inset

, which may not hold in practice.
\end_layout

\begin_layout Standard
The trace approach is driven by 
\emph on
spectral properties 
\emph default
(the condition number) while the Cholesky approach is driven by 
\emph on
graphical 
\emph default
properties
\emph on
 
\emph default
(the non-zero pattern)
\emph on
.
 
\emph default
We
\emph on
 
\emph default
propose to combine these two approaches by decomposing the problem with
 one component that is close to a tree (and is more amenable to Cholesky
 methods), and one component that has a bounded condition number.
 Our solution is to use a 
\emph on
spectral sparsifier
\emph default
 introduced by Spielman in 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2008"

\end_inset

.
\end_layout

\begin_layout Subsection
Applications
\end_layout

\begin_layout Standard
The problem of estimating determinants has important applications in spatial
 data analysis, statistical physics and statistics.
 In spatial statistics, it is often convenient to interpolate measurements
 in a 2-, 3- or 4-dimensional volume using a sparse Gaussian process, a
 technique known in the geospatial community as 
\emph on
kriging 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "zhang2010kriging,li2005analysis"

\end_inset


\emph on
.
 
\emph default
Computing the optimal parameters of this Gaussian process involves repeated
 evaluations of the partition function, which is a log-determinant.
 In this context, a diagonally dominant matrix for the Gram matrix of the
 process corresponds to distant interactions between points of measure (which
 is verified in some contexts, see 
\begin_inset CommandInset citation
LatexCommand cite
key "KelleyPace1997291"

\end_inset

).
 Determinants also play a crucial role in quantum physics and in theoretical
 physics.
 The wave function of a system of multiple fermion particles is an antisymmetric
 function which can be described as a determinant (Slater determinant, 
\begin_inset CommandInset citation
LatexCommand cite
key "atkins2011molecular,lowdin1955quantum"

\end_inset

).
 In the theory of quantum chromodynamics (QCD), the interaction between
 particles can be discretized on a lattice, and the energy level of particles
 is the determinant of some functional operators over this lattice 
\begin_inset CommandInset citation
LatexCommand cite
key "duncan1998efficient"

\end_inset

.
 It is itself a very complex problem because of the size of the matrices
 involved for any non-trivial problem, for which the number of variables
 is typically in the millions 
\begin_inset CommandInset citation
LatexCommand cite
key "bernardson1994monte"

\end_inset

.
 In this setting, the restriction to diagonally dominant matrices can be
 interpreted as an interaction between relatively massive particles 
\begin_inset CommandInset citation
LatexCommand cite
key "deForcrand1989516"

\end_inset

, or as a bound on the propagation of interactions between sites in the
 lattice 
\begin_inset CommandInset citation
LatexCommand cite
key "bernardson1994monte"

\end_inset

.
\end_layout

\begin_layout Standard
For these reasons, computing estimates of the log-determinant has been an
 active problem in physics and statistics.
 In particular, the Martin expansion presented in Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:martin-expansion"

\end_inset

 is extensively used in quantum physics 
\begin_inset CommandInset citation
LatexCommand cite
key "Ipsen2006"

\end_inset

, and it can be combined with sampling method to estimate the trace of a
 matrix series (
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang2008"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "McCourt2008"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang2007"

\end_inset

).
 Another different line of research has worked on bounds on the values of
 the determinant itself.
 This is deeply connected to simplifying statistical models using variational
 methods.
 Such a relaxation using a message-passing technique is presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Wainwright2006"

\end_inset

.
 Our method is close in spirit to Reuksen's work 
\begin_inset CommandInset citation
LatexCommand cite
key "Reusken2002"

\end_inset

 by the use of a preconditioner.
 However, Reuksen considers preconditioners based on a clever approximation
 of the Cholesky decomposition, and its interaction with the eigenvalues
 of the complete matrix is not well understood.
 Using simpler methods based on sampling, we are able to carefully control
 the spectrum of the remainder, which in turn leads to strong convergence
 guarantees.
\end_layout

\begin_layout Subsection
A note on scaling
\end_layout

\begin_layout Standard
Unlike other common characteristics of linear operators, the determinant
 and the log-determinant are very sensitive to dimensionality.
 We will follow the approach of Reuksen 
\begin_inset CommandInset citation
LatexCommand cite
key "Reusken2002"

\end_inset

 and consider the 
\emph on
regularized log-determinant
\emph default
 
\begin_inset Formula $f\left(A\right)=n^{-1}\log\left|A\right|$
\end_inset

 instead of the log-determinant.
 The regularized determinant has appealing properties with respect to dimensiona
lity.
 In particular, its sensitivity to perturbations does not increase with
 the dimensionality, but only depends on spectral properties of the operator
 
\begin_inset Formula $A$
\end_inset

.
 For example, calling 
\begin_inset Formula $\lambda_{\min}$
\end_inset

 and 
\begin_inset Formula $\lambda_{\max}$
\end_inset

 the minimum and maximum eigenvalues of 
\begin_inset Formula $A$
\end_inset

, respectively: 
\begin_inset Formula 
\[
\log\lambda_{\min}\leq f\left(A\right)\leq\log\lambda_{\max}
\]

\end_inset


\begin_inset Formula 
\[
\left|f\left(A+\epsilon I\right)-f\left(A\right)\right|\leq\epsilon\left\Vert A^{-1}\right\Vert _{2}+\mathcal{O}\left(\epsilon^{2}\right)
\]

\end_inset

The last inequality in particular shows that any perturbation to 
\begin_inset Formula $\log\left|A\right|$
\end_inset

 will be in the order 
\begin_inset Formula $\mathcal{O}\left(n\right)$
\end_inset

, and so that all the interesting log-determinants in practice will be dominated
 by some 
\begin_inset Formula $\mathcal{O}\left(n\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Main results
\end_layout

\begin_layout Standard
We first present some general results about the preconditioning of determinants.
 Consider 
\begin_inset Formula $A\in SDD_{n}$
\end_inset

 invertible, and some other matrix 
\begin_inset Formula $B\in SDD_{n}$
\end_inset

 that is close to 
\begin_inset Formula $A$
\end_inset

 in the spectral sense.
 All the results of this section stem from observing that: 
\begin_inset Formula 
\begin{eqnarray*}
\log\left|A\right| & = & \log\left|B\right|+\log\left|B^{-1}A\right|\\
 &  & \log\left|B\right|+\text{Tr}\left(\log\left(B^{-1}A\right)\right)
\end{eqnarray*}

\end_inset

The first section is concerned with estimating the remainder term 
\begin_inset Formula $\text{Tr}\left(\log\left(B^{-1}A\right)\right)$
\end_inset

 using the Martin expansion.
 The exact inverse 
\begin_inset Formula $B^{-1}$
\end_inset

 is usually not available, but we are given instead a linear operator 
\begin_inset Formula $C$
\end_inset

 that is an 
\begin_inset Formula $\epsilon-$
\end_inset

approximation of 
\begin_inset Formula $B^{-1}$
\end_inset

, for example using a conjugate gradient method.
 We show in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Preconditioned-log-determinants"

\end_inset

 that if the precision of this approximation is high enough, we can estimate
 the remainder with high probability and with a reasonable number of calls
 to the operator 
\begin_inset Formula $C$
\end_inset

 (this sentence will be made precise in the rather technical Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:preconditioning-approx"

\end_inset

).
 Using this general framework, the subsequent Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Making-the-problem"

\end_inset

 shows that spectral sparsifiers make excellent preconditioners that are
 close enough to 
\begin_inset Formula $A$
\end_inset

 and so that computing the Martin expansion is not too expansive.
 In particular, we build upon the recursive structure of Spielman-Teng ultra-spa
rsifiers to obtain our main result: 
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{theorem}
\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "thm:ultra_main"

\end_inset

On input 
\begin_inset Formula $A\in SDD_{n}$
\end_inset

 with 
\begin_inset Formula $m$
\end_inset

 non-zeros, 
\begin_inset Formula $\eta>0$
\end_inset

, the algorithm 
\family typewriter
UltraLogDet
\family default
 returns a scalar 
\begin_inset Formula $z$
\end_inset

 so that: 
\begin_inset Formula 
\[
\mathbb{P}\left[\left|z-n^{-1}\log\left|A\right|\right|>\epsilon\right]\leq\eta
\]

\end_inset

and this algorithm completes in expected time 
\begin_inset Formula $\tilde{O}\left(m\epsilon^{-2}\log^{3}n\log^{2}\left(\frac{\kappa_{A}}{\epsilon}\right)\log\left(\eta^{-1}\right)\right)$
\end_inset

.
 Moreover, if 
\begin_inset Formula $\epsilon>\Omega(n^{-1})$
\end_inset

, then the running time improves by a factor 
\begin_inset Formula $\epsilon$
\end_inset

.
 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
end{theorem}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The rest of the chapter is structured as follows.
 In the next section, we present some results about estimating the log-determina
nt from a truncated expansion.
 These results will justify the use of 
\emph on
preconditioners 
\emph default
to compute the determinant of a matrix.
 The techniques developed by Spielman et al.
 work on the Laplacians of weighted graphs 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2010"

\end_inset

.
 Section 3 introduces some new concepts to expand the notion of determinants
 to Laplacian matrices, and presents a few straightforward results in the
 relations between graph Laplacians and SDD matrices.
 Section 3.2 will use these new concepts to introduce a first family of precondit
ioners based on low-stretch spanning trees.
 Finally, Section 3.3 contains the proof of our main result, an algorithm
 to compute determinants in near-linear time.
 
\end_layout

\end_body
\end_document
